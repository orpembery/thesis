In this \lcnamecref{app:complexerror}, we recall the definition of some elementary properties of \emph{complex-valued} random variables, properties that are slightly different to their analogues for real-valued random variables. We then prove that the standard unbiased estimator of the variance (with Bessel's correction) of a complex-valued random variable  is indeed an unbiased estimator of the variance.

\bde[Complex-valued random variable {\cite[Equation (4.2)]{Pa:18}}]
If $\OFP$ is a probability space, then a \defn{complex-valued} random variable is a map $Y:\Omega\rightarrow \CC,$ such that $\Re Y$ and $\Im Y$ are real-valued random variables.
\ede

\bde[Mean and variance of a complex-valued random variable {\cite[Equations (5.8) and (5.27)]{Pa:18}}]
Let $Y$ be a complex-valued random variable. The \defn{expectation of $Y$} is
\beqs
\EXP{Y} \de \EXP{\Re{Y}} + i \EXP{\Im{Y}},
\eeqs
if it exists. The \defn{variance of $Y$} is
\beqs
\VAR{Y} \de \EXP{\abs{Y}^2} - \abs{\EXP{Y}}^2
\eeqs
if it exists.
\ede

\bde[$\sigma$-algebra generated by a random variable]
Let $Y$ be a complex-valued random variable. The \defn{$\sigma$-algebra generated by $Y$} is
\beqs
\sigalggen{Y} \de \set{Y^{-1}\mleft(E\mright) \st E \in \sigma\mleft(\CC\mright)},
\eeqs
where $Y^{-1}$ denotes the pullback.
\ede

\bde[Independent $\sigma$-algebras]
Two $\sigma$-algebras $\cFo$ and $\cFt$ on $\Omega$ are \defn{independent} if all their sets are independent, i.e.
\beqs
\PP\mleft(\Eo\mright) \cap \PP\mleft(\Et\mright) = \PP\mleft(\Eo\mright)\PP\mleft(\Et\mright)
\eeqs
for all $\Eo \in \cFo$ and $\Et \in \cFt.$
\ede

\bde[Independent random variables]
Two complex-valued random variables $\Yo$ and $\Yt$ are \defn{independent} if their respective generated $\sigma$-algebras are independent.
\ede

\ble[Independent implies uncorrelated]\label{lem:complexindep}
If $\Yo$ and $\Yt$ are independent complex-valued random variables, then
\beqs
\EXP{\Yo\Ytbar} = \EXP{\Yo}\EXP{\Ytbar}.
\eeqs
\ele

The proof of \cref{lem:complexindep} is identical to the real case.

\bde[Monte-Carlo estimator for $\EXP{Y}.$]
Let $Y$ be a complex-valued random variable, and $\Yo,\ldots,\YN$ be independent and identically distributed to $Y$. The \defn{Monte-Carlo estimator of $\EXP{Y}$} is
\beqs
\Yhat \de\frac1N \sum_{l=1}^N \Yl.
\eeqs
\ede

\bde[Unbiased estimator of the variance of the Monte-Carlo estimator]
Let $Y$ be a complex-valued random variable, and $\Yhat$ the Monte-Carlo estimator of $\EXP{Y}$. The estimator $\unbiased{\Yhat}{N}$ of $\VAR{\Yhat}$ is
\beq\label{eq:unbiased}
\unbiased{\Yhat}{N} \de \frac1{N(N-1)} \sum_{j=1}^N \abs{\Yj - \Yhat}^2.
\eeq
\ede

Observe that \cref{eq:unbiased} defines an estimator for the variance of \emph{the Monte-Carlo estimator $\Yhat$}; this is in contrast to more standard statistical settings, where one constructs an estimator of the variance of $Y$. The factor $1/(N-1)$ in \cref{eq:unbiased} is known as \defn{Bessel's correction}, and ensures the estimator is unbiased, as we now prove.

\ble[The unbiased estimator is unbiased]\label{lem:unbiased}
Let $Y$ be a complex-valued random variable and  $\Yhat$ the Monte-Carlo estimator of $\EXP{Y}$. Then $\unbiased{\Yhat}{N}$ is unbiased, i.e.,
\beqs
\EXP{\unbiased{\Yhat}{N}} = \VAR{\Yhat}.
\eeqs
\ele

The proof of \cref{lem:unbiased} is nearly identical to the proof for an unbiased estimator for $\VAR{Y}$ in the real-valued case. Nevertheless, we write the proof out in full, as we have not been able to find this exact result anywhere in the literature.

\bpf[Proof of \cref{lem:unbiased}]
%% We introduce the notation
%% \beqs
%% \mu \de \abs{\EXP{Y}}\quad\tand\quad\sigma \de \VAR{Y}
%% \eeqs
%% and we observe that
%% \beq\label{eq:varrelation}
%% \EXP{\abs{Y}^2} = \sigma^2 + \mu^2.
%% \eeq
Firstly, note that
\beqs
\VAR{\Yhat} = \VAR{\frac1N \sum_{l=1}^N \Yl} = \frac1{N^2} \VAR{\sum_{l=1}^N \Yl} = \frac1N \VAR{Y}.
\eeqs
Therefore, it is sufficient to show that $\unbiased{\Yhat}{N}$ is an unbiased estimator for $\VAR{Y}/N,$ or equivalently, $N\unbiased{\Yhat}{N}$ is an unbiased estimator for $\VAR{Y}$ (i.e. $\EXP{N\unbiased{\Yhat}{N}} = \VAR{Y}$). We show the latter by direct computation. Observe that
\beqs
\EXP{N\unbiased{\Yhat}{N}} = \frac1{N-1} \EXP{\sum_{j=1}^N \abs{\Yj - \Yhat}^2}= \frac1{N-1} \sum_{j=1}^N\EXP{ \abs{\Yj - \Yhat}^2},
\eeqs
therefore, it is sufficient for us to show
\beq\label{eq:comperror1}
\EXP{\abs{\Yj-\Yhat}^2} = \frac{N-1}N\VAR{Y},
\eeq
as the $\Yj$s are all independently and identically distributed. We show \cref{eq:comperror1} by direct computation.

\begin{align}
\EXP{\abs{\Yj-\Yhat}^2} &= \EXP{\abs{\Yj}^2 - \Yj \Yhatbar - \Yjbar \Yhat + \abs{\Yhat}^2}\nonumber\\
&= \EXP{\abs{\Yj}^2} - \frac1N\sum_{l=1}^N \mleft(\EXP{\Yj\Ylbar + \Yjbar \Yl}\mright) + \EXP{\abs{\Yhat}^2}\nonumber\\
&= \EXP{\abs{\Yj}^2} - \frac2N \EXP{\abs{\Yj}^2} - \frac1N \sum_{l \neq j} \mleft(\EXP{\Yj}\EXP{\Ylbar} + \EXP{\Yjbar}\EXP{\Yl}\mright) + \EXP{\abs{\Yhat}^2}\nonumber\\
&= \frac{N-2}N \EXP{\abs{Y}^2} - \frac{2(N-1)}N \abs{\EXP{Y}}^2 + \EXP{\abs{\Yhat}^2},\label{eq:comperror2}
  \end{align}
since the $\Yl$s have has the same distribution as $Y$. We now turn our attention to simplifying $\EXP{\abs{\Yhat}^2}.$ We have
\begin{align}
\EXP{\abs{\Yhat}^2} &= \EXP{\mleft(\frac1N \sum_{l=1}^N \Yl\mright)\overline{\mleft(\frac1N \sum_{m=1}^N \Ym\mright)}}\nonumber\\
&= \frac1{N^2}\EXP{ \sum_{l=1}^N \abs{\Yl}^2 + \sum_{l=1}^N\sum_{l \neq m} \Yl\Ymbar}\nonumber\\
&= \frac1N \EXP{\abs{Y}^2} + \frac{N(N-1)}{N^2}\abs{\EXP{Y}}^2,\label{eq:comperror3}
\end{align}
since the  $\Yl$ are i.i.d. Therefore combining \cref{eq:comperror2,eq:comperror3}, we obtain

\begin{align*}
\EXP{\abs{\Yj-\Yhat}^2} &= \frac{N-2}N \EXP{\abs{Y}^2} - \frac{2(N-1)}N \abs{\EXP{Y}}^2 + \frac1N \EXP{\abs{Y}^2} + \frac{N-1}{N}\abs{\EXP{Y}}^2\\
&= \frac{N-1}N \EXP{\abs{Y}^2} - \frac{N-1}N \abs{\EXP{Y}}^2\\
&= \frac{N-1}N \mleft(\VAR{Y}+\abs{\EXP{Y}}^2\mright) - \frac{N-1}N \abs{\EXP{Y}}^2,
\end{align*}
which gives us \cref{eq:comperror1}, as required.
\epf

