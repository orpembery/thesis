\section{Introduction}

Having considered how to speed up solving the individual linear systems UQ algorithms for the Helmholtz equation via nearby preconditioning in \cref{sec:nbpcqmc}, we now consider how one can reduce the total number of linear systems we must solve via a Multi-Level Monte-Carlo (MLMC) method. In particular, we prove bounds on the computational effort needed for Monte Carlo (MC) and Multi-Level Monte-Carlo (MLMC) methods for the stochastic Helmholtz equation. We compare and constrast the behaviour of these methods for different wavenumbers and tolerances and we show that Multi-Level Monte-Carlo methods asymptotically require less work than Monte-Carlo methods.

We note that our analysis of Monte-Carlo and Multi-Level Monte Carlo methods is in contrast to the the analysis of QMC methods in \cref{sec:nbpcqmcnumerics}. In our analysis of a QMC method in \cref{sec:nbpcqmcnumerics} we focused on showing numerically how the number of QMC points must be adapted for increasing $k$ to ensure the statistical error remains bounded. In contrast, our analysis of Monte-Carlo and Multi-Level Monte-Carlo methods below proves mathematically methods must be adapted for increasing $k$ to ensure the overall error (both numerical and statistical) remains bounded. In addition, we prove bounds on the expected computational cost of both the Monte-Carlo and Multi-Level Monte-Carlo methods.

We now provide a brief overview of this \lcnamecref{chap:mlmc}. In \cref{sec:overview} we give a brief introduction to Monte-Carlo and Multi-Level Monte-Carlo methods, and discuss some of the challenges in applying them to the Helmholtz equation. We then review literature on Multi-Level Monte-Carlo methods, focussing only on those works that are relevant for our study of Multi-Level Monte Carlo methods applied to the stochastic Helmholtz equation. In \cref{sec:mlmcsetup} we give an abstract setting for a $k$-dependent analysis of Multi-Level Monte Carlo methods. In \cref{sec:mc} we prove a bound on the computational work for the Monte-Carlo method in this abstract setting; in \cref{sec:mlmcan} we prove an analogous result for the Multi-Level Monte-Carlo method. Finally, in \cref{sec:mlmcapplying} we show that the stochastic Helmholtz equation fits into this abstract setting, and then compare and contrast the behaviour of Monte-Carlo and Multi-Level Monte-Carlo methods for the stochastic Helmholtz equation.

\section{Background on Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation}\label{sec:overview}
\subsection{The ideas of Monte-Carlo and Multi-Level Monte-Carlo methods}\label{sec:mlmcideasoverview}
Throughout this \lcnamecref{sec:mlmcideasoverview} we assume our goal is to compute an approximation of $\EXP{Q},$ where $Q:\Omega\rightarrow\RR$ is a random variable. We also assume we have access to a family of random variables $\Qh:\Omega\rightarrow\RR$, indexed by $h>0.$ We assume we can compute samples of $\Qh$, for any $h>0$. When we consider quantities of interest corresponding to the solution of a stochastic PDE, $Q$ will be a function of the true solution $u,$ and $\Qh$ will be a function of the finite-element approximation $\uh$ of $u.$ However, to explain the ideas behind Monte-Carlo and Multi-Level Monte-Carlo methods we will only occasionally need to mention $u$ and $\uh$. Therefore, for most of this chapter we will instead ork with $Q$ and $\Qh.$

\subsubsection{Monte-Carlo Estimators}

The \defn{Monte-Carlo estimator $\QhatMC$} of $Q$ is the simplest possible estimator of $\EXP{Q}.$ The estimator is given by
\beqs
\QhatMC \de \frac1{\NMC} \sum_{j=1}^{\NMC} \Qh\mleft(\omegasj\mright),
\eeqs
where the $\omegasj$ are independent and identically distributed samples from the probability space $\Omega$.

One would expect that reducing $h$ and increasing $\NMC$ would give a more accurate approximation of $\EXP{Q}.$ Therefore our analysis of $\QhatMC$ seeks to answer the question `How should we choose $h$ and $\NMC$ to ensure the error is less than $\eps$ (with minimal computational work)?' (for some pre-chosen tolerance $\eps > 0$). Our error analysis in \cref{thm:hhmc} below shows that one should take $\NMC \sim \eps^{-2}$ (the standard relationship between $\NMC$ and $\eps,$ see, e.g., \cite[Text after equation (3)]{ClGiScTe:11}) and that the size of $h$ should be dictated by the rate of convergence of $\uh$ to $u$ (this rate depends on $k$).

\subsubsection{Multi-Level Monte-Carlo Estimators}

In contrast to the Monte-Carlo estimator, where all of the approximations $\Qh(\omegasj)$ are performed for a single specified mesh size\footnote{However, for technical reasons due to the randomness of the coefficients, some of these meshes may be refined on a sample-by-sample basis, see \cref{sec:mlmcsetup} below. We ignore this technicality in the current discussion, but it will be fully addressed in \cref{sec:mlmcsetup} below.} $h$, the Multi-Level Monte-Carlo estimator computes approximations for a hierarchy of mesh sizes $\hz \geq \ho \geq \cdots \geq \hL.$ The rationale for this computation is the observation that the telescoping sum identity
\beq\label{eq:mlmctelescope}
\EXP{\uhL} = \EXP{\uhz} + \sum_{l=1}^L \EXP{\uhl-\uhlmo}
\eeq
holds and therefore, if one computes estimators $\Yhatz$ for $\EXP{\Qhz}$ and $\Yhatl$ for $\EXP{\Qhl - \Qhlmo}$, then one can construct an estimator for $\EXP{\QhL},$ 
\beqs
\QhatMLhL \de \Yzhat + \sum_{l=1}^L \Ylhat.
\eeqs
In this \lcnamecref{chap:mlmc}, the estimators $\Ylhat$ we be Monte-Carlo estimators using $\Nz$ samples of $\Qhz$ (for $\Yhatz$) and $\Nl$ samples of $\Qhl-\Qhlmo$ (for $\Yhatl,$ $l \geq 1$).

The reason one expects the Multi-Level Monte-Carlo estimator to require less computational effort than the Monte-Carlo estimator is that one expects the variance $\VAR{\Qhl-\Qhlmo}$ to decrease as $l$ increases. One expects this decrease because the quantities of interest $\Qhl$ and $\Qhlmo$ are obtained from finite-element approximations $\uhl$ and $\uhlmo$, and one expects these approximations to get closer together as $l$ increases. A basic finite-element calculation confirms this. Provided the solution $u$ is sufficiently smooth, and $\hl \sim \hlmo$ uniformly in $l,$ then
\beqs
\NHo{\uhl - \uhlmo} \leq \NHo{\uhl - u} + \NHo{u-\uhlmo} \lesssim \hl + \hlmo \sim \hlmo \rightarrow 0 \text{ as } l \rightarrow L.
\eeqs
Therefore $\uhl$ and $\uhlmo$ get closer together as $l$ increases, and one expects analogous behaviour for $\Qhl$ and $\Qhlmo.$ Since one takes the number of samples in a Monte-Carlo estimator to be proportional to the variance on the sampled quantity (i.e., $\Qhz$ or $\Qhl-\Qhlmo$ in this case), see \cref{thm:hhmc} below, the fact that $\VAR{\Qhl-\Qhlmo}$ gets smaller as $l$ increases should mean the number of samples of $\Qhl-\Qhlmo$ can decreases as $l$ increases. As the computational cost of performing numerical solves is higher for finer meshes (i.e., the cost of computing $\Qhl-\Qhlmo$ increases as $l$ increases), we expect that the Multi-Level Monte-Carlo estimator allows us to perform a large number of (cheap) solves on the coarser meshes, and a small number of (expensive) solves on the fine meshes, i.e. $\Nz \geq \No \geq \cdots \geq \NL$. Replacing solves on finer meshes with solvers on coarser meshes in this way should result in computational savings.

Our analysis of $\QhatMLhL$ then seeks to answer the question `How should we choose $\hL$ and $\Nz,\No,\ldots,\NL$ to ensure the error is less than $\eps$? (with minimal computational work)' (for some pre-chosen tolerance $\eps > 0$). The analysis for Multi-Level Monte-Carlo methods is more involved than that for Monte-Carlo methods, and so we refer to \cref{thm:mlmccomp} below for the details.

\subsection{Challenges in Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation}\label{sec:mlmcchallenges}

Analysing Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation has two main challenges that are not present in the analysis of these methods for, e.g., the stationary diffusion equation. We discuss each of these challenges in turn.

Firstly, the behaviour of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation will be $k$-dependent, because the behaviour of the finite-element method for the Helmholtz equation is $k$-dependent, see \cref{sec:helmfe}. Because of this $k$-dependent behaviour, we would like our analysis of these methods to be completely $k$-explicit. In particular, since we have access to $k$-explicit finite-element-error estimates for the Helmholtz equation in \cref{sec:fem} above, we are able to make our analysis of Monte-Carlo and Multi-Level Monte Carlo methods $k$-explicit.  As the standard proofs of computational complexity for Monte-Carlo and Multi-Level Monte-Carlo methods do not include any $k$-dependence (which is understandable, as in most other cases one is not interested in the dependence of Monte-Carlo and Multi-Level Monte-Carlo methods on additional parameters), we will need to re-prove some of these standard results with the $k$-dependence incorporated explicitly.

Secondly, the finite-element approximation $\uh$ of the solution $u$ of the stochastic Helmholtz equation may not exist for all $h > 0,$ and the criteria to prove its existence and uniqueness may be dependent on the coefficients $A$ and $n.$ I.e., $\uh(\omegaso)$ may exist and be unique, but $\uh(\omegast)$ may not, for $\omegaso\neq\omegast\in\Omega$. To see why this is the case, recall from the definitions of $\hk{a}{b}$-accuracy and -data-accuracy for the finite-element solution of the Helmholtz equation (\cref{def:hkacc,def:hkdataacc}) that the finite-element approximation $\uh$ only exists for $h$ sufficiently small (with the definitions of $\hk{a}{n}$-accuracy and -data-accuracy defining `sufficiently small' in terms of $k$ and other quantities). Moreover, the criteria for `sufficiently small' also depend on the coefficients $A$ and $n$ (see \cref{rem:accuracyhetero}). Therefore, when $A$ and $n$ are stochastic, the existence and uniqueness of $\uh(\omega)$ is not only $h$-dependent but also $\omega$-dependent, as outlined above. Putting the above challenge into the language of the random variables $\Qh,$ the random variable $\Qh$ may not exist or be unique for all $h>0,$ and moreover, its existence and uniqueness may be sample-dependent. I.e., $\Qh(\omegaso)$ may exist and be unique, but $\Qh(\omegast)$ may not, for $\omegaso\neq\omegast.$

This sample-dependence poses an issue for Monte-Carlo and Multi-Level Monte-Carlo methods. The method may require us to compute $\Qh(\omegasj)$, but there is no guarantee that $\Qh(\omegasj)$ exists. Therefore, we need to modify our methods to deal with this sample-dependence. Such a modification to Monte-Carlo and Multi-Level Monte-Carlo methods for sample-dependent existence and uniqueness criteria was given by Graham, Parkinson, and Scheichl in \cite{GrPaSc:19} (and in Parkinson's PhD thesis \cite{Pa:18}), in the context of the Radiative Transport Equation (RTE). The RTE is an integro-differential equation whose numerical approximations have similar sample-dependent existence and uniqueness criteria to the Helmholtz equation. We adopt their approach for dealing with the sample-dependence, this approach is discussed in \cref{sec:mlmcsetup} below.

\subsection{Literature Review of Multi-Level Monte-Carlo methods}
\optodo{Scarabosio! Need to highlight, but work in a small frequency setting, so everything is $k$-independent}
We focus our literature review on (i) foundational works in Multi-Level Monte-Carlo methods, to provide a little context for our work on the Helmholtz equation, and (ii) applications of Multi-Level Monte-Carlo methods to problems sharing the challenges outlined in \cref{sec:challenges} above. As far as we are aware, there is no prior work on Multi-Level Monte-Carlo methods explicitly incorporating the dependence on an additional parameter, and so we just mention works dealing with sample-dependent criteria for the numerical approximation. For a wider-ranging overview of the literature, we refer the reader to the review article \cite{Gi:15} and the webpage \cite{Gi}, the latter of which is kept up-to-date with a range of recent work on Multi-Level Monte-Carlo methods.

Multi-level Monte Carlo methods for stochastic differential equations were first introduced by Giles \cite{Gi:08} for time-dependent SDEs, with applications mostly arising in finance, although the ideas were present in earlier work by Heinrich \cite{He:98,He:01} on multilevel methods for parametric integration. Multi-Level Monte-Carlo methods were first applied to elliptic (i.e., non-time-dependent) PDEs by Cliffe, Giles, Scheichl, and Teckentrup in \cite{ClGiScTe:11} for the stationary diffusion equation, with an application in porous media flow. In particular, the statement of the Multi-Level Monte-Carlo complexity theorem in \cite[Theorem 1]{ClGiScTe:11} is the basis for our statement of a Multi-Level Monte-Carlo complexity theorem for the Helmholtz equation in \cref{thm:mlmccomp} below. We highlight that a key result of \cite[Theorem 1]{ClGiScTe:11} is that Multi-Level Monte-Carlo methods \emph{always} outperform Monte-Carlo methods, at least in the setting given in \cite{ClGiScTe:11}.

We now highlight two bodies of work on Multi-Level Monte-Carlo methods with sample-dependent criteria; the work of Mishra, Schwab, and \v{s}ukys on Monte-Carlo and Multi-Level Monte-Carlo methods for time-domain wave propagation and the work of Graham, Parkinson, and Scheichl on Monte-Carlo and Multi-Level Monte-Carlo methods for the Radiative Transport Equation.

The work of Mishra, Schwab, and \v{S}ukys covers Monte-Carlo and Multi-Level Monte-Carlo methods for a range of linear and nonlinear hyperbolic problems, see, e.g., \cite{Su:14}. However, we focus just on their results for linear problems, as then the PDE involved is the time-domain wave equation with random coefficients and random initial data, whose Fourier transform in time is the Helmholtz equation (recall the discussion in \cref{sec:motivapp}). This work on linear wave propagation is contained in the papers \cite{SuMiSc:13,MiScSu:16} and in \v{S}ukys' PhD thesis \cite{Su:14}. They discretised the individual realisations of the wave problems using a finite-volume method in space and specialised time-stepping algorithms in time (see, e.g., \cite[Section 3.1]{MiScSu:16}). Because the PDEs in these works have random coefficients, the CFL condition for the numerical method (this condition depends on the coefficients) is also random, meaning the number of time steps used in the time-stepping algorithm is random. (The spatial discretisation is fixed across all realisations.) In \cite{SuMiSc:13} the authors analyse the error against the expected work (analagous to our analysis in \cref{sec:mc,sec:mlmcan} below. In \cite{MiScSu:16} the authors present more realistic test cases, and a load-balancing algorithm for applying the Multi-Level Monte-Carlo method on high-performance computers. The load-balancing algorithms is needed because the different individual solves have different computational requirements, because of the random number of timesteps mentioned above. They see that the Multi-Level Monte-Carlo method consistently outperforms the Monte-Carlo method.

Another collection of relevant work is that of Graham, Scheichl, and Parkinson \cite{GrPaSc:18,Pa:18,GrPaSc:19} on UQ methods (including Multi-Level Monte-Carlo methods) for the Radiative Transport Equation, as mentioned above. The main relevance of this work for our study of the Helmholtz equation is that, as mentioned above, proving the numerical approximation of the solution of the RTE exists and is unique requires a coefficient-dependent discretisation condition (se \cite[Theorem 4.12]{GrPaSc:19}). (This condition is analogous to a mesh constraint, except the RTE is not discretised with a traditional mesh, as it is defined on both spatial and angular variables.) When this discretisation constraint is carried over into a UQ setting, the RTE has a sample-dependent discretisation condition. Therefore, for some samples a given discretisation may be too coarse to guarantee existence and uniqueness. This sample-dependence is very similar to the situation we encounter for the Helmholtz equation, where the condition to ensure data-accuracy is $A$- and $n$-dependent (see \cref{cor:dataacc} above), and therefore will be a sample-dependent condition in the UQ setting.

The remedy proposed for this sample-dependence by Graham, Parkinson, and Scheichl is to \emph{selectively} refine the dsicretisation \emph{only} for those samples that require a finer discretisation. We adopt this strategy for the Helmholtz equation, as outlined in \cref{sec:mlmcsetup} below. Moreover, Graham, Parkinson, and Scheichl show that (under suitable assumptions on the randomness, that are satisfied for a range of realistic random field models) this sample-wise refinement does not affect the asymptotics of the expected cost of the algorithm, see \cite[Lemma 5.8]{GrPaSc:19}. We obtain similar results for the Helmholtz equation in \cref{lem:c} below.

%% In this \lcnamecref{sec:comp} we state and prove an abstract result on the convergence of multi-level Monte Carlo methods, laregly following the proof of \cite[Theorem 1]{ClGiScTe:11}. Our result is a generalisation of \cite[Theorem 1]{ClGiScTe:11} in the following three ways:
%% \ben
%% \item In \cite{ClGiScTe:11} it is assumed that the convergence of the approximate QoIs $\Qhl$, and the cost of producing samples of these QoIs, only depends on the parameter $\hl$ (where, in stochastic PDE applications, $\hl$ is the mesh size for the finite-element discretisation). However, in this work, we assume that the convergence and cost also depend on another parameter $k,$ and we make the dependence of the final computational cost of the MLMC method explicit in $k.$ In our application to the Helmholtz equation, $k$ will be the wavenumber of the problem.
%% \item In \cite{ClGiScTe:11} it is assumed that the approximating QoIS $\Qhl$ exist for all levels $l$. This corresponds to the finite-element solution of the PDE under investigation existing for all mesh sizes $h.$ Whilst this assumption is true for the stationary diffusion equation studied in \cite{ClGiScTe:11}, it is \emph{not} true for the Helmholtz equation that we study here. Therefore we make the additional assumption (\cref{ass:qoie} below) that $\Qhl$ only exists for sufficiently small $\hl.$
%% \item In \cite{ClGiScTe:11} the error $\eps$ incurred in the MLMC method is equally divided between the bias and the variance of the MLMC method (see the Proof of \cref{thm:mlmccomp3}). However, in this work we assume that there is a quantity $\splitting \in (0,1)$ (see \cref{ass:splittingbounds}), possibly dependent on $k$ that allows a vairable `split' of the error between the bias and the variance. Our main use of this is in\optodo{Insert refs once it's done}, where we use this variable splitting to compensate for the fact that to bound the (squared) bias error by $\eps^2/2$ would mean we take $\hL \lesssim k^{-1},$ but to ensure the finite-element solution $\uh$ exists, we must take $\hL \lesssim k^{-3/2}.$
%% \een
%% We now proceed to prove our abstract MLMC convergence result, comtaining the generalisations metioned above.

\section{Set up}\label{sec:mlmcsetup}

We work in the framework of \cref{chap:stochastic}. For $h>0,$ define the random field $\uh:\Omega \rightarrow \Vhp$ by letting $\uh(\omega)$ solve \cref{prob:fevgen} (with $\T = \TR$) with coefficients $A(\omega)$ and $n(\omega)$ (if it exists). For simplicity throughout this \cref{chap:mlmc} we assume $\uh$ is measurable. Let $Q:\HozDDR\rightarrow\RR$ be a (measurable) \defn{quantity of interest} (QoI) of the solution $u$ (so that $Q(u)$ is a random variable). As an abuse of notation, we also use $Q$ to denote $Q \circ u,$ where the context means this notation is unambiguous. If $\uh(\omega)$ exists, we let $\Qh$ denote $Q \circ \uh.$

As hnited at in the preceeding paragraph, the finite-element solution $\uh(\omega)$ may not always exist. Moreover, the existence (or not) of $\uh(\omega)$ is due to the fact that the constants involved in the definitions of accuracy and data-accuracy of a finite-element method (\cref{def:hkacc,def:hkdataacc} above) are dependent on the coefficients $A$ and $n$. Therefore, when $A$ and $n$ are random fields, the associated existence and uniqueness criterion (and a priori bounds) are all path-dependent. To define this path-dependence precisely, we use the following \lcnamecref{def:probdataacc}.

\bas[Probabilistic version of data-accuracy]\label{def:probdataacc}
There exist random variables $\Co$ and $\cotilde$ such that if
\beq\label{eq:probdataacc}
hk^a < \Co(\omega),
\eeq
then $\uh(\omega)$ exists, is unique, and $Q$ and $\Qh$ satisfy
\beqs
\abs{Q(\omega)-\Qh(\omega)} \leq \cotilde(\omega) h^\alpha k^\sigma \Cfg.
\eeqs
\eas

In order to define a finite-element approximation of $u$ (and therefore a random variable $\Qh$) that exists almost surely, we borrow a technique from \cite{GrPaSc:19}. Informally, for a given $h>0,$ for any realisations $\omega \in \Omega$ such that \cref{eq:probdataacc} is \emph{not} satisfied, we compute on a finer mesh (that does satisfy \cref{eq:probdataacc}). For fixed $h>0$ we define
\beq\label{eq:hmaxomega}
\hmaxomega \de \Co(\omega)k^{-a}.
\eeq
We then define
\beq\label{eq:homega}
\homega \de \min\set{h,\hmaxomega}.
\eeq
Observe that $\homega$ \emph{always} satisfies \cref{eq:probdataacc}. We then define
\beqs
\uhtilde(\omega) = u_{\homega}(\omega),
\eeqs
i.e., $\uhtilde$ is the finite-element approximation of $u(\omega)$ on the mesh with mesh size $\homega.$ We then define
\beq\label{eq:Qhtilde}
\Qhtilde = Q \circ \uhtilde.
\eeq

Given $\uhtilde$ has a random mesh size, the cost of computing $\uhtilde$ (or $\Qhtilde$) will be a random variable. Therefore we make the following \lcnamecref{ass:costone} on the cost of computing samples.

\bas[Cost of one sample]\label{ass:costone}
There exist $\cthtilde, \gamma > 0$ such that $\cthtilde$ is independent of $h$ and $k$, and if $\Qhtilde(\omega)$ exists, then
\beqs
\Cost{\Qhtilde(\omega)} \leq \cthtilde(\omega) \homega^{-\gamma},
\eeqs
\eas

The following \lcnamecref{lem:c} shows how, provided the set $\Omegabad$\optodo{Chat about this earlier/here} has small probability, the expected cost of computing a sample of $\Qhtilde$ is governed solely by the specified mesh size $h$ (and not by any over-refinement). The assumption \cref{eq:cass} is the technical version of the statement `$\Omegabad$ has small probability'.

\bas[Assumptions on $\Omegabad$]\label{ass:omegabad}
Assume:
\beq\label{eq:cass}
\cth \de \EXP{\cthtilde}+ \EXP{\cthtilde\Co^{-\gamma}} < \infty
\eeq
\eas

\ble[Expected cost of one sample]\label{lem:c}
If \cref{ass:omegabad} holds, then 
\beq\label{eq:singlecost}
\EXP{\Cost{\Qh}} \leq \cth \mleft(h^{-\gamma}+k^{a\gamma}\mright).
\eeq
\ele

\bpf[Proof of \cref{lem:c}]
The proof follows closely that in \cite[Lemma 5.8]{GrPaSc:19}.
We have
\beq\label{eq:costpf1}
\Cost{\uhtilde(\omega)} \leq \cthtilde(\omega)\homega^{-\gamma} \leq \cthtilde(\omega) \mleft(h^{-\gamma} + \mleft(\hmaxomega\mright)^{-\gamma}\mright)
\eeq
by \cref{ass:costone,eq:homega}. Then using \cref{eq:hmaxomega}, the definition of $\hmaxomega,$ \cref{eq:costpf1} is bounded by
\beq\label{eq:costpf2}
\cthtilde(\omega)h^{-\gamma} + \mleft(\cthtilde\Co\mright)(\omega) k^{a\gamma},
\eeq
and therefore as \cref{eq:cass} holds, we obtain \cref{eq:singlecost}.
\epf

\ble[Convergence of numerical method]\label{ass:a}
Under \cref{def:probdataacc}, there exist constant $\co, \alpha, \sigma> 0$, such that $\co$ is independent of $h$ and $k$, and
\beqs
\abs{\EXP{\Qhtilde-Q}} \leq \co k^\sigma h^{\alpha}.
\eeqs
\ele

\bpf[Proof of \cref{ass:a}]
The proof of \cref{ass:a} is immediate (with $\co = \EXP{\cotilde}$) from the definition of $\Qhtilde$ \cref{eq:Qhtilde}, \cref{def:probdataacc} and the fact that $\homega \leq h$ (by \cref{eq:homega}.
\epf

\bde[Root-mean-squared error]\label{def:rmse}
Given a random variable $Q$ and an estimator $\Qhat$ of Q, the \defn{root-mean-squared error} of $\Qhat$ is
\beqs
\err{\Qhat} \de \mleft(\EXP{\mleft(\Qhat-Q\mright)^2}\mright)^{\half}.
\eeqs
\ede

\section{Monte-Carlo methods}\label{sec:mc}

We now define the Monte-Carlo estimator of $Q$,
\beqs
\QhatMC \de \frac1{\NMC} \sum_{j=1}^{\NMC} \Qhtildesj,
\eeqs
where the $\Qhtildesj$ are independently and identically distributed samples of $\Qhtilde.$

We can prove the following \lcnamecref{thm:hhmc} on the computational complexity of the Monte-Carlo estimator $\QhatMC$

\bth[Computational complexity of Monte-Carlo]\label{thm:hhmc}
Let the assumptions of \cref{lem:c,ass:a} hold. Given $\eps \in (0,1),$ if
\beq\label{eq:NMC}
\NMC  \sim 2\VAR{\Qhtilde}\eps^{-2}
\eeq
and
\beq\label{eq:hMC}
h \sim \mleft(\sqrt{2}\co\mright)^{-\frac1{\alpha}}k^{-\frac\sigma\alpha}\eps^{\frac1{\alpha}},
\eeq
then $\err{\QhatMC} \leq \eps$ and the computational complexity of $\QhatMC$ satisfies
\beqs
\EXP{\CMC} \sim \VAR{\Qhtilde}\mleft(\eps^{-2-\frac{\gamma}{\alpha}}k^{\frac{\gamma\sigma}\alpha} + \eps^{-2}k^{a\gamma}\mright).
\eeqs
\enth

\bpf[Proof of \cref{thm:hhmc}]
The proof is standard, see, e.g., \cite[Section 2.1]{ClGiScTe:11}. We have
\begin{align}
\err{\QhatMC}^2 &= \EXP{\mleft(\QhatMC - \EXP{\QhatMC} + \EXP{\QhatMC} - \EXP{Q}\mright)^2}\nonumber\\
&= \EXP{\mleft(\QhatMC - \EXP{\QhatMC}\mright)^2} + \mleft(\EXP{\QhatMC} - \EXP{Q}\mright)^2\nonumber\\
&= \VAR{\QhatMC} + \mleft(\EXP{\QhatMC} - \EXP{Q}\mright)^2,\label{eq:mccomp1}
\end{align}
where the second line follows from the fact that $\EXP{\QhatMC - \EXP{\QhatMC}} = 0$, and the third line follows from the fact that $\QhatMC$ is an unbiased estimator.

By definition of $\QhatMC$, and the fact that the samples $\Qhtildesj$ are independent, we have
\beq\label{eq:mccomp2}
\VAR{\QhatMC} = \frac1{\NMC^2}\sum_{j=1}^{\NMC}\VAR{\Qhtildesj} = \frac1{\NMC} \VAR{\Qhtilde}.
\eeq
Therefore we can conclude from \cref{eq:mccomp1,eq:mccomp2} that the root-mean-squared-error satisfies
\beq\label{eq:mccomp3}
\err{\QhatMC}^2 = \frac1{\NMC}\VAR{\Qhtilde} + \NLoO{\Qhtilde-Q}^2.
\eeq
By \cref{eq:NMC,eq:hMC}, each of the terms in \cref{eq:mccomp3} is bounded by  $\eps^2/2,$ and therefore $\err{\QhatMC} \leq \eps.$

All that remains is to estimate the (expected) computational complexity. We have
\beqs
\EXP{\Cost{\QhatMC}} = \NMC \EXP{\Cost{\uhtilde}} \leq \NMC \cth h^{-\gamma} \sim 2\VAR{\Qhtilde}\eps^{-2} \mleft(\cth \mleft(\sqrt{2} \co\mright)^{\frac\gamma\alpha}k^{\frac{\gamma\sigma}{\alpha}}\eps^{-\frac\gamma\alpha} + k^{a\gamma}\mright)
\eeqs
as required.
\epf

\section{Multi-level Monte-Carlo}\label{sec:mlmcan}

We define a set of levels$\set{\hl}_{l=0}^L$ ($L$ to be chosen) such that $\hl =\frac{\hlmo}s$ for $l \geq 1$. We then define the correction operators between the levels by $\Yl \de \Qhltilde - \Qhlmotilde, l \geq 1,$ $\Yz = \Qhztilde.$ We let $\Ylhat$ be the Monte-Carlo estimator of $\Yl$,
 \beqs
\Ylhat \de \frac1{\Nl}\sum_{i=1}^{\Nl} \Yli,
 \eeqs
 with $\Nl$ to be chosen, where $\Yli$ denotes independent samples of $\Yl$. Finally we are able to define the \defn{multi-level Monte Carlo estimator}
 \beqs
 \QhatMLhL \de \sum_{l=1}^L \Ylhat,
 \eeqs
 where the $\Ylhat$ are independent.

  %% The following assumptions
  %% % \lcnamecrefs{ass:coarse}
  %%  will form the backbone of our analysis. They are a generalisation of the assumptions contained in \cite{ClGiScTe:11,ChScTe:13} for the MLMC method, the generalisation being that we assume that the quantities below depend not only on the levels $\hl$ but also on some additional parameter $k>1.$ When this theory is applied to the Helmholtz equation, $k$ will be the wavenumber of the Helmholtz equation.

%% The following assumption (which will be realised in a more concrete setting for the Helmholtz equation) concerns the existence of the approximating QoIs $\Qhl.$

%% \bas[Existence of $\Qhl$]\label{ass:qoie}
%% There exist $\Ccoarse,\coarseexp > 0$ with $\Ccoarse$ independent of $k$ such that if
%% \beqs
%% \hl \leq \Ccoarse k^{-\coarseexp},
%% \eeqs
%% then the QoI $\Qhl$ exists.
%% \eas

\bas[Variance of correction operators]\label{ass:b}
There exist $\ct, \beta, \tau > 0$, such that $\ct$ is independent of $h$ and $k,$ and
\beq\label{eq:mlmcassb}
\Vl \de \VAR{\Yl} \leq \ct k^\tau\hl^{\beta},
\eeq
where $\VAR{\cdot}$ denotes variance.
\eas

In order to obtain a nice expression for the cost of computing one sample of $\Qh,$ we require the following assumption on the coarse space:

\bas[Dependence of coarse space on $k$]\label{ass:coarse}
We let
\beqs
\hz = \Ccoarse k^{-a}.
\eeqs
for some chosen constant $\Ccoarse > 0.$
\eas

 
% We write $\Vl$ for $\VAR{\Yl}.$
 
 We want to determine the choices of $L$ and $\Nl, l = 0,\ldots,L,$ such that the root-mean-squared eror (RMSE)
 \beqs
 \err{\QhatMLhL} \de \mleft(\EXP{\mleft(\QhatMLhL - \EXP{Q}\mright)^2}\mright)^{\half}
 \eeqs
 satisfies $\err{\QhatMLhL} \leq \eps,$ for some pre-defined $\eps > 0.$



%\input{nice}



%% \subsection{Lemma}

%% The proof of the main \lcnamecref{thm:mlmccomp} will require the following \lcnamecref{lem:sumbound}.

%% \ble\label{lem:sumbound}
%% If $L$ is given by

%% then, for $s>1$ and $\delta \in \RR,$ we have the bound
%% \beq\label{eq:sumbound}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L+1 & \tif \delta = 0,\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}s^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha} &\tif \delta >0\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}&\tif \delta < 0
%% \end{cases}
%% \eeq
%% \ele

%% \bpf[Proof of \cref{lem:sumbound}]
%% The proof follows that in \cite{ClGiScTe:11}. We first observe that, since $L$ is given by \eqref{eq:Ldef}, it follows that
%% \beq\label{eq:Lbounds}
%% \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) \leq L < \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) + 1.
%% \eeq
%% Rearranging \eqref{eq:Lbounds}, we obtain the bounds
%% \beq\label{eq:saLbounds}
%% \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1} \leq s^{\alpha L} < \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1}s^\alpha.
%% \eeq
%% If $\delta > 0,$ then we use the right-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLpos}
%% s^{\delta L} < \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}s^{\delta},
%% \eeq
%% and if $\delta < 0,$ we use the left-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLneg}
%% s^{\delta L} \leq \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}.
%% \eeq
%% We now observe that, for $\delta \neq 0,$
%% \begin{align}
%% \sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
%% &= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
%% &\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumbound}
%% \end{align}
%% since $s^{-\delta} > 0,$ as $s >0.$ Combining \eqref{eq:ssumbound} with \eqref{eq:sdLpos} and \eqref{eq:sdLneg}, we obtain \eqref{eq:sumbound} in the cases $\delta \neq 0.$ The case $\delta=0$ is straightforward.
%% \epf


The following theorem describes the computational effort needed to obtain RMSE $\leq \eps$. It is exactly the same as \cite[Theorem 1]{ClGiScTe:11}, but with the dependence on all the parameters explicit.%, and with some additional cases enumerated. %\Cref{thm:mlmccomp} contains more cases than in \cite[Theorem 1]{ClGiScTe:11} because \cite[Theorem 1]{ClGiScTe:11} makes the assumption throughout that $\alpha \geq 1/2\min\set{\beta,\gamma}.$ This assumption does not always hold for the Helmholtz equation (see the cases of a direct solver in 3-D below), however, examining the proof of \cite[Theorem 1]{ClGiScTe:11}  shows that in any given case, one only needs the assumption $\alpha \geq \beta/2$ or the assumption $\alpha \geq \gamma2$, never both at the same time. Therefore, for convenience, we explicitly state when these conditions are needed, and for completeness, we give the results when these conditions are violated. 

%% The following \lcnamecref{ass:constants3} will ensure that \cref{ass:qoie} is satisfied.

%%  \bas[$\eps$ sufficiently small]\label{ass:constants3}
%%  Assume
%%  \beqs
%% \eps \leq \sqrt{2} \co \Ccoarse^{\alpha} k^{\sigma-\coarseexp\alpha}.
%%  \eeqs
%%  \eas


%% \bas[Assumptions on $\eps$ and $k$ to simplify expressions in the case $\beta=\gamma$]\label{ass:epsk}
%% \beqs
%% \eps \leq \min\set{\frac{\sqrt{2}\co\Ccoarse^\alpha}{s^{2\alpha}},\frac1{\sqrt{2}\co\Ccoarse^\alpha}},
%% \eeqs
%% and
%% \beqs
%% k^{\sigma-a\alpha} \geq 1.
%% \eeqs
%% \eas
\bth[Multi-Level Monte-Carlo Complexity Theorem]\label{thm:mlmccomp}
The number of levels $L$ is given by
\beq\label{eq:Ldef}
L = \max\set{\ceil{\frac1\alpha\log_{s}\mleft(\sqrt{2}\co  \Ccoarse^\alpha k^{\sigma-a\alpha} \eps^{-1}\mright)},0},
\eeq
that is,
\beq\label{eq:hLcond}
\hL \leq \min\set{\mleft(\frac\eps{\sqrt{2}\co k^{\sigma}}\mright)^{\frac1\alpha},\hz},
\eeq
and the number of samples on each computational level is given by
\beq\label{eq:Nl}
\Nl = \ceil{\frac2{\eps^{2}} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^{L} \mleft(\Vj\Cj\mright)^{\half}},
\eeq
where $\Cl \de \cth\hl^{-\gamma}$. In this case, if $L=0$, then computational effort $\CMLhL(\eps)$ required to obtain $\err{\QhatMLhL} \leq \eps$ is given by \cref{thm:hhmc}. Otherwise, $\CMLhL(\eps)$ satisfies the bounds
 
 \begin{numcases}{ \CMLhL(\eps) \lesssim}
k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^{\sigma-a\alpha}}\eps\mright)+2\mright)^2  & if $\beta = \gamma$,\label{eq:mlmchheq}\\ 
k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}} + k^{\frac{\gamma\sigma}{\alpha}}\eps^{-\frac\gamma\alpha} & otherwise.\label{eq:mlmchhoth}
\end{numcases}
 \enth

For an explanation of why one must include the maximum in \cref{eq:Ldef}, see the proof of \cref{thm:mlmchelmholtz} below.
 
 \bpf[Proof of \cref{thm:mlmccomp}]
Throughout the proof, we assume $L>0,$ as the case $L=0$ is given by \cref{thm:hhmc}. We recall the decomposition of the (squared) mean-squared error into the bias error and the sampling error analagous to \cref{eq:mccomp1}
\beqs\label{eq:mlmcdecomp}
\errQhatMLhL^2 = \VAR{\QhatMLhL} + \mleft(\EXP{\QhatMLhL - Q}\mright)^2.
\eeqs
We now proceed to choose the parameters $L$ and $\Nl, l = 0,\ldots,L$ such that we can bound both the bias and the variance by $\eps^2/2.$

We first bound the bias, to do this, we only need to choose $L.$ One can show that the bias is equal to $\abs{\EXP{\QhL - Q}}^2.$ Therefore by \cref{ass:a} a sufficient condition for the bias to be $\leq \eps^2/2$ is (by \cref{ass:a})
\beqs
\co k^\sigma \hL^\alpha \leq \frac{\eps}{\sqrt{2}},
\eeqs
that is, \eqref{eq:hLcond}. As $\hL = \hz s^{-L},$ it follows from \eqref{eq:hLcond} that a sufficient condition for the bias to be $\leq \eps^2/2$ is
\beq\label{eq:Lcondpart}
L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co k^\sigma \hz^\alpha \eps^{-1}\mright)}.
\eeq
As $\hz = \Ccoarse k^{-a},$ we can simplify \eqref{eq:Lcondpart} to obtain \eqref{eq:Ldef}.
% \beqs
% L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma-\coarseexp\alpha} \eps^{-1}\mright)}.
% \eeqs

We now seek to bound the variance. One can show similarly to \cref{eq:mccomp2} that the variance $\VAR{\QhatMLhL} = \sum_{l=0}^L \Nl^{-1} \Vl,$ and the cost is: (following \cite{GrPaSc:19})
\begin{align}
\EXP{\Cost{\QhatMLhL}}&\leq \sum_{l=0}^L \EXP{\Cost{\Ylhat}}\nonumber\\
&= \sum_{l=0}^L \sum_{i=1}^{\Nl} \EXP{\Cost{\Yli}}\nonumber\\
&\leq \sum_{l=0}^L \sum_{i=0}^{\Nl} \mleft(\EXP{\Cost{\Qhltilde}} + \EXP{\Cost{\Qhlmotilde}}\mright)\nonumber\\
%% &\leq \sum_{l=0}^L \Nl \mleft(\cth \hl^{-\gamma} + \cth \hlmo^{-\gamma}\mright)\nonumber\\
&=\sum_{l=0}^L \Nl\mleft(1+s^{-\gamma}\mright) \cth \hl^{-\gamma}\nonumber\\
&= \mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \Nl\Cl\label{eq:Cboundformin}
\end{align}

To find the optimal number of samples per level (the values of $\Nl, l=0,\ldots,L$) we formulate this as an optimisation problem to find the numbers $\Nl$ that minimise \eqref{eq:Cboundformin}, subject to $\VAR{\QhatMLhL}=\eps/2.$ This can be solved using a Lagrange multiplier as in \cite{Gi:15}, and we obtain \cref{eq:Nl}. We now need to infer the computational complexity for Multi-Level Monte-Carlo with $L$ given by \eqref{eq:Ldef} and the $\Nl$ given by \eqref{eq:Nl}.

From \cref{eq:Cboundformin}
\begin{align}
\EXP{\Cost{\QhatMLhL}} &\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^{L} \Cl \Nl\nonumber\\
&\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl \mleft(\frac2{\eps^{2}} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^L \mleft(\Vj\Cj\mright)^{\half} + 1\mright) \text{ (by \eqref{eq:Nl})}\nonumber\\
&= 2\eps^{-2}\mleft(1+s^{-\gamma}\mright)\mleft(\sum_{l=0}^L\mleft(\Vl\Cl\mright)^{\half}\mright)^2 + \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl\nonumber\\
&= 2 \ct \cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2} \mleft(\sum_{l=0}^L \hl^{\frac{\beta-\gamma}2}\mright)^2 + \cth \mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \hl^{-\gamma} \text{ (by \cref{ass:b,ass:costone})}\nonumber\\
&= 2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\hz^{\beta-\gamma}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 + \cth\mleft(1+s^{-\gamma}\mright) \hz^{-\gamma} \sum_{l=0}^L s^{\gamma l} \text{ (by definition of } \hl\text{ )}\label{eq:complexitymidway}\\
%% &=2 \ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 + \cth\Cppw^{-\gamma} k^{\rho + \gamma\coarseexp}  \sum_{l=0}^L s^{\gamma l} \text{ (by definition of } \hz\text{ )}\nonumber\\
%% &\leq2\ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 +  \frac{\mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha}\cth s^{\gamma}}{1-s^{-\gamma}}k^{\rho +  \frac{\gamma\sigma}\alpha}\eps^{-\frac\gamma\alpha} \text{ (since }\gamma>0,\text{ by \cref{lem:sumbound})}.\label{eq:complexitymidway}
\end{align}

Using \cref{lem:sumboundnew} with $\Lconst = 1/\alpha,$ $\func = \sqrt{2}\co\Ccoarse^\alpha k^{\sigma - a\alpha}$, and $\delta = \gamma$, the second term in \eqref{eq:complexitymidway} can be bounded (as $\gamma > 0$) by %(letting \csumdelta \de \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha} \Ccoarse^\delta / \mleft(1-s^{-\delta}\mright)$)
\beq\label{eq:firstterm}
\cth\frac{\mleft(1+s^{-\gamma}\mright)  \hz^{-\gamma} s^\gamma \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} \Ccoarse^\gamma}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha-a\gamma} \eps^{-\frac\gamma\alpha}
= \frac{\mleft(1+s^{-\gamma}\mright)\cth \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} s^\gamma}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha} \eps^{-\frac\gamma\alpha}
\eeq

To bound the sum in the first part of \eqref{eq:complexitymidway}, we must distinguish three cases based on $\gamma - \beta.$


If $\gamma=\beta,$ then the first part of \eqref{eq:complexitymidway} becomes (using \cref{lem:sumboundnew} with $\Lconst$ and $\func$ as above, and $\delta = 0$ and \cref{eq:Ldef})
\beq
2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(L+1\mright)^2 \leq 2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^{\sigma-a\alpha}}\eps\mright)+2\mright)^2,
\label{eq:gammaequal}
\eeq
which gives \cref{eq:mlmchheq}.
%We wish to simplify \eqref{eq:gammaequal}, so that it is of the form Constant $\times$ `Terms involving $\eps$ and $k$'. To achieve this simplification, we use \cref{ass:epsk}. As $k^{\sigma-a\alpha} \geq 1$ and $\eps \leq \mleft(\sqrt{2} \co \Ccoarse^{\alpha}\mright)/s^{2\alpha},$ it follows that
%% \beqs
%% 2 \leq \frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright),
%% \eeqs
%% and thus \eqref{eq:gammaequal} can be bounded by
%% \beq\label{eq:gammaequalpart1}
%% 8 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright)\mright)^2.
%% \eeq
%% As $k^\sigma k^{-a\alpha} \geq 1$ and $\eps \leq 1/\mleft(\sqrt{2}\co\Ccoarse^\alpha\mright),$ we can bound \eqref{eq:gammaequalpart1} by (including a change of base in the logarithm)
%% \beq\label{eq:gammaequalfinal}
%% \frac{32 \ct\cth \mleft(1+s^{-\gamma}\mright)}{\alpha^2 \mleft(\loge(s)\mright)^2} k^\tau \mleft(\loge\mleft(\frac{k^{\sigma-a\alpha}}\eps\mright)\mright)^2
%% \eeq
%% and obtain \cref{eq:mlmchheq}.

For simplicity in the proof of \cref{eq:mlmchhoth}, we define
\beqs
\csumdelta \de \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}.
\eeqs

If $\gamma > \beta$ then using \cref{lem:sumboundnew} as above, but with $\delta = (\gamma-\beta)/2$, the first term in \eqref{eq:complexitymidway} becomes
\beq
\eps^{-2}2\ct\cth \mleft(1+s^{-\gamma}\mright) k^\tau \hz^{\beta-\gamma}\mleft(\csumgammambetat s^{\frac{\gamma-\beta}2} k^{\frac{\gamma-\beta}2\frac\sigma\alpha} k^{-a\frac{\gamma-\beta}2} \eps^{-\frac{\gamma-\beta}{2\alpha}}\mright)^2 = \Cgammagtrbeta k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}},\label{eq:gammagtr}
\eeq
where
\beqs
\Cgammagtrbeta \de 2\ct\cth\mleft(1+s^{-\gamma}\mright)\csumgammambetat^2 s^{\gamma-\beta} \Ccoarse^{\beta-\gamma},
\eeqs
that is, an expression of the form \cref{eq:mlmchhoth}. The second equality in \cref{eq:gammagtr} follows from the definition of $\hz$ in \cref{ass:coarse}.
If $\gamma < \beta,$ then analagously the first term in \eqref{eq:complexitymidway} is
\beqs
\Cgammalessbeta k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}},
\eeqs
where
\beq\label{eq:gammaless}
\Cgammalessbeta \de \frac{\Cgammagtrbeta}{s^{\gamma-\beta}}.
\eeq

We now combine \eqref{eq:firstterm}, \eqref{eq:gammaequalfinal}, \eqref{eq:gammagtr}, and \eqref{eq:gammaless} and supress all the constants to obtain the result.
%Removing all the terms that are not of interest from \eqref{eq:gammaequal}, \eqref{eq:gammagtr}, and \eqref{eq:gammaless}, we obtain \eqref{eq:mlmchheq} and \eqref{eq:mlmchhoth}.
\epf


%\input{nasty}

\subsection{Technical lemma}

\ble\label{lem:sumboundnew}
If $L$ is given by
\beq\label{eq:Ldefgen}
L = \ceil{\Lconst\log_{s}\mleft( \func \eps^{-1}\mright)},
\eeq
for some $\Lconst, \func > 0,$ then, for $s>1$ and $\delta \in \RR,$ we have the bound
\beq\label{eq:sumboundgen}
\sum_{l=0}^{L} s^{\delta l} \leq
\begin{cases}
L+1 & \tif \delta = 0,\\
\frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0\\
\frac{1}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst}&\tif \delta < 0
\end{cases}
\eeq
%% \beq\label{eq:sumboundLmo}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L & \tif \delta = 0,\\
%% \frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0\\
%% \frac{1}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst}&\tif \delta < 0
%% \end{cases}
%% \eeq
%\opctodo{Tidy}
\ele

\bpf[Proof of \cref{lem:sumboundnew}]
The proof follows that in \cite{ClGiScTe:11}. We first observe that, since $L$ is given by \eqref{eq:Ldefgen}, it follows that
\beq\label{eq:Lboundsgen}
\Lconst\log_s\mleft(\func \eps^{-1}\mright) \leq L < \Lconst\log_s\mleft(\func \eps^{-1}\mright) + 1.
\eeq
Rearranging \eqref{eq:Lboundsgen}, we obtain the bounds
\beq\label{eq:saLboundsgen}
\mleft( \func\eps^{-1}\mright)^{\alpha \Lconst} \leq s^{\alpha L} < \mleft( \func\eps^{-1}\mright)^{\alpha \Lconst}s^\alpha.
\eeq
If $\delta > 0,$ then we use the right-hand bound in \eqref{eq:saLboundsgen} to obtain
\beq\label{eq:sdLposgen}
s^{\delta L} < \func^{\delta\Lconst}\eps^{-\delta\Lconst}s^{\delta},
\eeq
and if $\delta < 0,$ we use the left-hand bound in \eqref{eq:saLboundsgen} to obtain
\beq\label{eq:sdLneggen}
s^{\delta L} \leq \func^{\delta\Lconst}\eps^{-\delta\Lconst}.
\eeq
We now observe that, for $\delta \neq 0,$
\begin{align}
\sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
&= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
&\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumboundgen}
\end{align}
since $s^{-\delta} > 0,$ as $s >0.$ Combining \eqref{eq:ssumboundgen} with \eqref{eq:sdLposgen} and \eqref{eq:sdLneggen}, we obtain \eqref{eq:sumboundgen} in the cases $\delta \neq 0.$ The case $\delta=0$ is straightforward.
\epf

\section{Application}\label{sec:mlmcapp}

We now apply the above results on the computational complexity of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation to two model QoIs, $\NLtD{u}$ and $\NHokD{u}$, where $u$ is the solution of the TEDP-analogue of \cref{prob:msedp} in \cref{chap:stochastic} (see \cref{rem:tedp}). We choose these two representative QoIs as
\bit
\item The errors in their approximation have different $k$-dependencies (see \cref{lem:mlho,lem:mllt} below), and
  \item We expect other QoIs to have a similar $k$-dependence to $\NHokD{u}$ or $\NLtD{u}$, depending on whether the QoI depends on $u$ and $\grad u$, or just on $u$, respectively.
    \eit

    All that is left for us to determine are the values of $\alpha, \sigma, \beta, \tau, \gamma,$ and $a$ for each of these QoIs. The values of $\alpha$ and $\sigma$ are given by the convergence results for finite-element approximations of $u$; see \cref{thm:fembound} above and \cref{lem:mlho,lem:mllt} below. As we see in \cref{lem:mlho,lem:mllt} below, $\beta = 2\alpha$, and $\tau = 2\sigma$. In our calculations in this \lcnamecref{sec:mlmcapp} we assume $\gamma = d;$ i.e., we assume that we have a Helmholtz solver that scales optimally in the number of degrees of freedom; obtaining such a solver is the subject of much current research, and we refer to, e.g., the recent works \cite{GrSpVa:17,ZeScHeDe:19,TaZeHeDe:19} for a selection of modern solvers achieving close to this optimal scaling.

We use two different values of $a$ below, $a=-(2p+1)/2p$ (where $p$ is the polynomial degree of the finite-elements) and $a=-1$. Based on the finite-element result \cref{thm:fembound}, we should take $a=(2p+1)/2p = 1 + 1/2p;$ such a $k$-dependence is required in \cref{thm:fembound} for the finite-element solution to exist and be unique. However, such a choice of $a$ does not allow the number of levels $L$ to grow with $k$ for $Q = \NHokD{u}$: in \cref{eq:Ldef} above, $L$ depends on $k^{\sigma-a\alpha};$ if $a=(2p+1)/2p,$ $\alpha = 2p$, and $\sigma = 2p+1$ (see \cref{lem:mlho} below for details of these values for $\alpha$ and $\sigma$) then $k^{\sigma-a\alpha} = 1$, and therefore $L$ is $k$-independent. However, if $a = 1,$ then $k^{\sigma-a\alpha} = k,$ and $L$ grows (log-linearly) with $k$. We observe that such a choice for $a$ (keeping the same values for $\alpha$ and $\sigma$ can be heuristically justified; in \cite[Theorem 3.2]{Ai:04} Ainsworth shows for grids with cube elements that if $h \ll 1/k,$ then the phase error (i.e., $k-\kh,$ where $\kh$ is the discrete wavenumber associated with the finite-element solution) is of the order $h^{2p}k^{2p+1}$. We therefore also investigate the beahviour of Monte-Carlo and Multi-Level Monte-Carlo under the assumption $a=-1$ and the finite-element errors are given by terms of the same leading order as \cref{eq:femltbound,eq:femhobound}.

To summarise the results that we see in \cref{thm:mlmchelmholtz} below, in terms of $k$-dependence, the results for Monte-Carlo- and Multi-Level Monte-Carlo-methods are identical. We also see that, in terms of $\eps$-dependence, Multi-Level Monte-Carlo methods are consistently better than Monte-Carlo methods, unless the condition for existence and uniqueness is more restrictive than the condition to keep the error bounded\footnote{In the cases we consider, this only occurs when we take $a = (2p+1)/2p$ and $Q(u) = \NLtD{u},$ so $\alpha = \sigma = 2p.$}. In such a case, for $\eps$ small and/or $k$ small, Multi-Level Monte-Carlo out-performs Monte-Carlo, but if $\eps$ and/or $k$ are large, then Multi-Level Monte-Carlo is identical to Monte-Carlo (because there are no additional levels).
\begin{table}
  \centering
\begin{tabular}{Sc Sc Sc Sc}
  \toprule
  $Q(u)$ & $a$ & Monte-Carlo & Multi-Level Monte-Carlo\\
  \midrule
      $\NHokD{u}$ & $\displaystyle \frac{2p+1}{2p}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-\frac{d}{2p}}$ \\
  $\NLtD{u}$ & $\displaystyle \frac{2p+1}{2p}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & \makecell{$\displaystyle k^{d} \eps^{-\frac{d}{2p}}$ if $k\eps$ small,\\otherwise $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$} \\
    $\NHokD{u}$ & 1 &$\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-\frac{d}{2p}}$ \\
      $\NLtD{u}$ & 1 & $\displaystyle k^d \eps^{-2-\frac{d}{2p}}$ &$\displaystyle k^d \eps^{-\frac{d}{2p}}$\\
  \bottomrule
\end{tabular}
\caption{Computational complexity of Monte-Carlo and Multi-Level Monte-Carlo algorithms\label{tab:mcresults}}
\end{table}
%% If $Q(\cdot) = \NHokDR{\cdot},$ then $\alpha = 2p,$ $\sigma = 2p+1$. If $Q(\cdot) = \NLtDR{\cdot},$ then $\alpha = 2p,$ $\sigma = 2p$. In both cases, $\beta = 2\alpha,$ $\tau = 2\sigma.$ Assume $\gamma = d$---optimal solver.

%% FINISH TOMORROW.

\section{Verifying the assumptions in \cref{sec:mlmcsetup}, and applying the Monte-Carlo and Multi-Level Monte-Carlo theory}\label{sec:mlmcapplying}

We now let $u:\Omega\rightarrow\HokD$ solve the TEDP-analogue of \cref{prob:msedp}, and $\uhtilde:\Omega\rightarrow\Vhp$ solve the stochastic analogue of \cref{prob:fevtedp} (i.e., $\uhtilde$ solves \cref{prob:fevtedp} with coefficients $A(\omega)$ and $n(\omega)$, $\T = ik$, and meshsize $\homega$ pathwise). We assume $\uhtilde$ is measurable.


\ble[Verifying assumptions for $Q(u) = \NHokD{u}$]\label{lem:mlho}
If \cref{ass:coarse} holds with $\Ccoarse$ bounded by $\Chcond \Condn(n) \CAnk^{-\frac1{2p}}$ (as defined in \cref{thm:fembound}) and $a = (2p+1)/2p$, and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p+1,$ $\beta = 4p$, and $\tau = 4p+2$.
\ele

\bpf[Proof of \cref{lem:mlho}]
By the assumptions of the \lcnamecref{lem:mlho}, it is immediate from \cref{eq:femhobound} that \cref{def:probdataacc} holds with $\alpha = 2p$ and $\sigma = 2p+1$. (See \cref{rem:higherorder} for why we can neglect the lower-order terms in \cref{eq:femhobound}.) To show \cref{ass:b}, we follow \cite[Proof of Proposition 4.2]{ChScTe:13} and use the triangle inequality and \cref{def:probdataacc} to show
\beq\label{eq:Ylhatbound}
\Ylhat(\omega) \leq \mleft(\Qhl - Q\mright)(\omega) + \mleft(Q- \Qhlmo\mright)(\omega) \leq \cotilde(\omega)\mleft(\hl^\alpha + \hlmo^\alpha\mright)k^\sigma \Cfg.
\eeq
We then use the fact that $\VAR{\Ylhat} = \EXP{\Ylhat^2} - \EXP{\Ylhat}^2 \leq \EXP{\Ylhat^2}$ and \cref{eq:Ylhatbound} to show \cref{eq:mlmcassb}, with $\ct = \EXP{\cotilde^2}\mleft(1+s^\alpha\mright)^2.$
\epf

\ble[Verifying assumptions for $Q(u) = \NLtD{u}$]\label{lem:mllt}
If \cref{ass:coarse} holds with $\Ccoarse$ bounded by $\Chcond \Condn(n) \CAnk^{-\frac1{2p}}$ (as defined in \cref{thm:fembound}) and $a = (2p+1)/2p$, and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p,$ $\beta = 4p$, and $\tau = 4p$.
\ele

\bpf[Proof of \cref{lem:mllt}]
The proof is exactly analagous to the proof of \cref{lem:mlho}, except we use \cref{eq:femltbound} instead of \cref{eq:femhobound}.
\epf

As stated in \cref{sec:mlmcapp}, whilst we cannot prove analogues of \cref{lem:mlho,lem:mllt} with $a=1$, we wish to study the behaviour of Multi-Level Monte-Carlo when $a=1.$ Therefore, we now state analogues of \cref{lem:mlho,lem:mllt} in the case $a=1.$

\bas[Assumptions for $Q(u) = \NHokD{u}$ with $a=1$]\label{ass:mlho}
There exists $\Ccoarse>0$ such that if \cref{ass:coarse} holds with $a = 1$ and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p+1,$ $\beta = 4p$, and $\tau = 4p+2$.
\eas

\bas[Assumptions for $Q(u) = \NLtD{u}$ with $a=1$]\label{ass:mllt}
There exists $\Ccoarse>0$ such that if \cref{ass:coarse} holds with $a = 1$ and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p,$ $\beta = 4p$, and $\tau = 4p$.
\eas

We are now in a position to state our main \lcnamecref{thm:mcmlmchelmholtz} on the behaviour of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation.

To prove a bound on the complexity of the Monte-Carlo method, we require the following \lcnamecref{ass:variance}.

\bas[Variance of $\Qhtilde$ constant]\label{ass:variance}
The variance $\VAR{\Qhtilde}$ is constant with respect to $h$.
\eas

\bth[Computational complexity of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation]\label{thm:mcmlmchelmholtz}
Under \cref{ass:variance,ass:costone,ass:omegabad}, if the assumptions of \cref{lem:mllt,lem:mlho} hold, then the computational complexity of the Monte-Carlo and Multi-Level Monte Carlo methods for estimating $\EXP{Q(u)}$ is given by the first two lines of \cref{tab:mcresults}.

If \cref{ass:mlho,ass:mllt} hold instead of the assumptions of \cref{lem:mllt,lem:mlho}, then then the computational complexity of the Monte-Carlo and Multi-Level Monte Carlo methods for estimating $\EXP{Q(u)}$ is given by the last two lines of \cref{tab:mcresults}, where `$k\eps$ small' means
\beq\label{eq:kepscond}
k\eps < \sqrt{2}\co \Ccoarse^{2p}.
\eeq
\enth

\bpf[Proof of \cref{thm:mcmlmchelmholtz}]
The proof follows immediately from \cref{thm:mlmccomp}, by substituting in the appropriate values of $\alpha,$ $\beta$, etc.. The only case that requires explaining is when $a=(2p+1)/2p$ and $Q(u) = \NLtD{u}$ (so $\alpha = \sigma = 2p.$) In this case, the expression for $L$ in \cref{eq:Ldef} evaluates as
\beq\label{eq:specialL}
L = \max\set{\ceil{\frac1{2p}\log_s\mleft(\sqrt{2}co\Ccoarse^{2p}k^{-1}\eps^{-1}\mright)},0}
\eeq
Observe that for $k$ (or $\eps$) sufficiently large, the first term in the right-hand side of \cref{eq:specialL} may be negative (i.e. if $k^{-1}$ is suffciently close to 0). In such a case, the maximum of the two quantities on the right-hand side of \cref{eq:specialL} will be 0, and in such a case the Multi-Level Monte-Carlo algorithm reverts to the Monte-Carlo algorithm. The criterion for the first term to be positive (and so for Multi-Level Monte-Carlo to be distinct from Monte-Carlo) is
\beqs
\sqrt{2}co\Ccoarse^{2p}k^{-1}\eps^{-1} > 1,
\eeqs
which is equivalent to the condition \cref{eq:kepscond}, as required.
\epf

\bre[No coarse-space dependence in Multi-Level Monte-Carlo cost bounds]
Observe that there is no dependence on the coarse space (with mesh size $\hz$) in the Multi-Level Monte-Carlo results in \cref{thm:mcmlmchelmholtz}, i.e., the quantity $a$ does not appear when $\beta \neq \gamma$. (Recall from \cref{ass:coarse} that the coarse space is chosen to be proportional to $k^{-a}$.) This lack of $a$-dependence is surprising at first glance. If $a < \sigma/\alpha,$ then the number of levels $L$ grows with $k$ (as the term $k^{\sigma - a\alpha}$ in \cref{eq:Ldef} will grow with $k$), and one would expect to see this growing number of levels affect the overall computational complexity. However, it seems that whilst the number of levels grows with $k$, reducing the computational cost, on the other hand the growing number of samples (growing because there are more levels on which to compute) offsets the gains from extra levels.

One can see this offsetting in the proof; the terms $\hz^{\beta-\gamma}$ and $\hz^{-\gamma}$ in \cref{eq:complexitymidway} are exactly cancelled by the appearence of $\hz$ in the application of \cref{lem:sumboundnew}. Observe that in \cref{eq:firstterm} the quantity $\hz^{-\gamma}$ ancels with the term $\Ccoarse^{\gamma}k^{-a\gamma}$ arising from \cref{lem:sumboundnew}. Similarly the $k$-dependence of the term $\hz^{\beta-\gamma}$ in \cref{eq:gammagtr} cancels with the term $\mleft(k^{-a(\gamma-\beta)/2}\mright)^2$ arising from \ref{lem:sumbound}.

It remains to be seen whether this coarse-mesh independence is borne out in numerical computations. Such an investigation could be the subject of future work on Multi-Level Monte-Carlo methods for the Helmholtz equation.
\ere

\bre[Proving probabilistic bounds on the cost]
In \cite{GrPaSc:19}, the authors extend their bounds on the expectation of the computational cost for Monte-Carlo and Multi-Level Monte-Carlo methods to bounds on the \emph{exceedance probabilities} of the computational cost. I.e., they prove bounds of the form
\beq\label{eq:mcprobbound}
\PP\mleft(\Cost{\Qhat} < M(\eps,\delta,\Qhat)\mright) > 1-\delta^2,
\eeq
for some function $M$, where $\Qhat$ is the Monte-Carlo or Multi-Level Monte-Carlo estimator (see \cite[Theorems 5.12 and 5.13]{GrPaSc:19}). They make only mild additional assumptions on the randomness to prove bounds of the form \cref{eq:mcprobbound}; these assumptions mean they can bound $\VAR{\Qhtilde}$ and hence $\VAR{\Cost{\Qhat}}.$ The probabilistic bounds \cref{eq:mcprobbound} then follow from bounds on $\VAR{\Cost{\Qhat}}$ using Chebyshev's inequality.

We could apply these proof techniques to prove a probabilistic bound of the form \cref{eq:mcprobbound} for Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation. However, the calculations for the Helmholtz equation would be conceptually similar to those in \cite{GrPaSc:19}, albeit more involved, as we would need to keep track of the $k$-dependence. Given we expect the results we obtained would be similar to those in \cite{GrPaSc:19}, we elect not to pursue them.
\ere
\optodo{Make sure bibfile displays DOIs}
