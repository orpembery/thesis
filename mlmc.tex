\section{Introduction}

Having considered how to speed up the individual solves in UQ algorithms for the Helmholtz equation via nearby preconditioning, we now consider how one can reduce the total number of solves via a Multi-Level Monte-Carlo (MLMC) technique. For more details, see the discussion and overview in \cref{sec:mlmcideasoverview} below, but informally, in MLMC one performs solves on a hierarchy of meshes, and obtains computational gains because of the reduced (statistical) variance between solutions on different meshes. Since the variance is reduced, fewer solves are needed to control the overall variance of the estimator. We rigorously quantify the computational effort required by MLMC (and a Monte-Carlo estimator), and compare and constrast their behaviour for different wavenumbers and tolerances; this analysis is in contrast to the the analysis of QMC methods in \cref{sec:nbpcqmcnumerics}, where the analysis was only based on numerical experiments, and the overall cost of the estimator was not directly addressed.


In \cref{sec:overview} we give an overview of MC and MLMC methods, and discuss some of the challenges in applying them to the Helmholtz equation. We then give a brief overview of the relevant literature on MLMC methods for the Helmholtz equation. In \cref{sec:mlmcsetup} we state the stochastic Helmholtz problems that we wish to solve, and give the assumptions we use to prove properties of the MC and MLMC methods. In \cref{sec:mc} we prove a bound on the computational work for a MC method for the stochastic Helmholtz equation; in \cref{sec:mlmcan} we prove an analogous result for an MLMC method. Finally, in \cref{sec:mlmcapplying} we compare and contrast the results for MC and MLMC methods.

\section{Overview and literature review}\label{sec:overview}
\subsection{Overview of the ideas of Monte-Carlo and Multi-Level Monte-Carlo}\label{sec:mlmcideasoverview}
We now describe Monte-Carlo and Multi-Level Monte-Carlo estimators for the expectation of a quantity of interest $Q$ of the solution $u$ of a stochastic Helmholtz equation, that is, estimators for $\EXP{Q(u)}.$ Throughout this \cref{chap:mlmc} we let $\OFP$ be a complete probability space.

The \defn{Monte-Carlo estimator $\QhatMC$} is the simplest possible estimator of $\EXP{Q(u)},$ given by
\beqs
\QhatMC \de \frac1{\NMC} \sum_{j=1}^{\NMC} Q\mleft(\uh\mleft(\omegasj\mright))\mright),
\eeqs
where the $\omegasj$ are independent and identically distributed samples from $\Omega$ according to $\PP.$ We measure the error in $\QhatMC$ (and the MLMC estimator) by the \defn{root-mean-squared error}
\beqs
\EXP{\mleft(Q-\QhatMC\mright)^2}^{\half}.
\eeqs

The error analysis for $\QhatMC$ then seeks to answer the question `How should we choose $h$ and $\NMC$ to ensure the root-mean-squared error is less than $\eps$ (with minimal computational work)?' (for some pre-chosen tolerance $\eps > 0$). We give this error analysis (and an expression for the resulting expected computational effort required) in \cref{sec:mc} below.

In contrast to the Monte-Carlo estimator, where all of the numerical solutions $\uh(\omegasj)$ are performed for a single specified mesh size\footnote{However, for technical reasons due to the randomness of the coefficients, some of these meshes may be refined on a sample-by-sample basis, see \cref{sec:mlmcsetup} below.} $h$, the MLMC estimator computes the solutions for a hierarchy of specified mesh size $\hz \geq \ho \geq \cdots \geq \hL.$ The rationale for such a computation is an observation that the telescoping sum identity
\beq\label{eq:mlmctelescope}
\EXP{\uhL} = \EXP{\uhz} + \sum_{l=1}^L \EXP{\uhl-\uhlmo}
\eeq
holds. Moreover, one expects that the variance of the inter-level differences $\uhl-\uhlmo$ will be small, because these numerical approximations are similar, and these variances should decay as $l$ increases. This observation motivates the definition of the \defn{multi-level Monte-Carlo} estimator
\beqs
\QhatMLhL \de \Yzhat + \sum_{l=1}^L \Ylhat,
\eeqs
where $\Yzhat$ is a Monte-Carlo estimator of $\EXP{\uhz}$ using $\Nz$ samples of $\uhz$, and $\Ylhat$ is a Monte-Carlo estimator of $\EXP{\uhl-\uhlmo}$ using $\Nl$ samples.

Because one expects the variance $\VAR{\uhl-\uhlmo}$ to decrease as $l$ increases, one expects that $\Nl$ can also be chosen to decrease as $l$ increases. Moreover, as the computational cost of performing numerical solves is higher for finer meshes (i.e., larger $l$), we expect that the MLMC estimator allows us to perform a large number of (cheap) solves on the coarser meshes, and a small number of (expensive) solves on the fine meshes. This should result in a considerable reduction in computational effort.

The error analysis for $\QhatMLhL$ then seeks to answer the question `How should we choose $\hL$ and $\Nz,\No,\ldots,\NL$ to ensure the root-mean-squared error is less than $\eps$? (with minimal computational work)' (for some pre-chosen tolerance $\eps > 0$). We give this error analysis (and an expression for the resulting expected computational effort required) in \cref{sec:mlmcan} below.

\subsection{Issues in MC and MLMC methods for the Helmholtz equation}

Analysing MC and MLMC methods for the Helmholtz equation has two issues that are not present in the analysis of these methods for, e.g., the stationary diffusion equation. We discuss each of these in turn.

Firstly, the behaviour of the methods for the Helmholtz equation will be $k$-dependent and we would like our analysis of these methods to be completely $k$-explicit. In particular, finite-element-error estimates for the Helmholtz equation are $k$-dependent (recall the extensive discussion in \cref{sec:helmfedisc} above), and therefore our proofs (especially for MLMC methods) will need to incorporate this $k$-dependence. As the standard results for MLMC methods (understandably) do not include any $k$-dependence, we will need to re-prove some of these results with the $k$-dependence incorporated explicitly.

Secondly, the finite-element approximation $\uh$ of the solution of the Helmholtz equation may not exist for all $h > 0,$ and the criteria to prove its existence and uniqueness may be dependent on the coefficients $A$ and $n.$ Recall from the definitions of $\hk{a}{b}$-accuracy and -data-accuracy (\cref{def:hkacc,def:hkdataacc}) that the finite-element solution only exists for $h$ sufficiently small (with the notions of $\hk{a}{n}$-accuracy and -data-accuracy characterising `sufficiently small' in terms of $k$) and that the criteria for `sufficiently small' also depend on the coefficients $A$ and $n$ (see \cref{rem:accuracyhetero}). When $A$ and $n$ are stochastic, the fact that the criteria depend on $A$ and $n$ mean that the criteria are now path-dependent. This path-dependence poses an issue for MC and MLMC methods; our algorithm may require us to compute $\uh(\omegasj)$ (the approximation to $u(\omegasj)$ on the mesh with mesh size $h$), but there is no guarantee that $\uh(\omegasj)$ exists for this particular realisation $\omegasj.$ Therefore, we need to modify our approach to deal with this path-dependence. Such a modification to MC and MLMC methods for path-dependent existence criteria was given by Graham, Parkinson, and Scheichl in \cite{GrPaSc:19} (and in Parkinson's PhD thesis \cite{Pa:18}) in the context of the Radiative Transport Equation, an integro-differential equation with similar path-dependent existence and uniqueness criteria. We will adopt their approach for dealing with the path-dependence.

\subsection{Literature Review}
We now provide a short literature review of MLMC methods, with a particular focus on relevant results for our subsequent work on the stochastic Helmholtz equation. For a wider-ranging overview of the literature, we refer the reader to the review article \cite{Gi:15} and the webpage \cite{Gi}, which is kept up-to-date with a range of recent work on MLMC methods.

Multi-level Monte Carlo methods were first introduced by Giles \cite{Gi:08} for time-dependent SDEs, with applications mostly arising in finance (although the ideas were present in earlier work by Heinrich \cite{He:98,He:01} using multilevel methods for parametric integration). MLMC methods were first applied to elliptic (i.e., non-time-dependent) PDEs by Cliffe, Giles, Scheichl, and Teckentrup in \cite{ClGiScTe:11} for the stationary diffusion equation, with an application in porous media flow. In particular, the statement of the MLMC complexity theorem \cite[Theorem 1]{ClGiScTe:11} is the basis for our statement of a complexity theorem for the Helmholtz equation in \cref{thm:mlmccomp} below. We highlight that a key result of \cite[Theorem 1]{ClGiScTe:11} is that Multi-Level Monte-Carlo \emph{always} outperforms Monte-Carlo, at least in the setting given in \cite{ClGiScTe:11}.

MLMC methods were applied to linear wave-propagation problems (in the time-domain) by \v{S}ukys, Mishra, and Schwab in the works \cite{SuMiSc:13,MiScSu:16} and in \v{S}ukys' PhD thesis \cite{Su:14}. They discretised the individual realisations of the wave problems using a finite-volume method in space and specialised time-stepping algorithms (see, e.g., \cite[Section 3.1]{MiScSu:16}). A key issue in these works was that the coefficient-dependence of the CFL condition in the time-stepping algorithm causes the CFL condition to be random for the UQ problem. THerefore in \cite{SuMiSc:13} the authors analyse the error against the expected work (analagous to our analysis below, except in \cite{SuMiSc:13} \emph{all} of the levels are scaled in order to ensure a CFL condition is satisfied, see \cite[Text following Theorem 5]{SuMiSc:13}; in our analysis below, only those meshes that are `too large' are scaled). In \cite{MiScSu:16} the authors present more realisatic test cases, and a load-balancing algorithm for applying the MLMC method on high-performance computers. The authors see that the MLMC algorithm consistently outperforms the MC algorithm.

Another collection of relevant work is that of Graham, Scheichl, and Parkinson \cite{GrPaSc:18,Pa:18,GrPaSc:19} on UQ methods (including MLMC methods) for the Radiative Transport Equation (RTE) as mentioned above. The key methodology in these papers is the technique the authors use to deal with path-dependent existence and uniqueness results for the numerical discretisations (see \cite[Chapter 4]{Pa:18}, \cite[Definition 5.5 ff.]{GrPaSc:19}; \cite{GrPaSc:18} contains initial numerical results). We adopt this methodology in \cref{sec:mlmcsetup} below to tackle similar issues for the Helmholtz equation.

%% In this \lcnamecref{sec:comp} we state and prove an abstract result on the convergence of multi-level Monte Carlo methods, laregly following the proof of \cite[Theorem 1]{ClGiScTe:11}. Our result is a generalisation of \cite[Theorem 1]{ClGiScTe:11} in the following three ways:
%% \ben
%% \item In \cite{ClGiScTe:11} it is assumed that the convergence of the approximate QoIs $\Qhl$, and the cost of producing samples of these QoIs, only depends on the parameter $\hl$ (where, in stochastic PDE applications, $\hl$ is the mesh size for the finite-element discretisation). However, in this work, we assume that the convergence and cost also depend on another parameter $k,$ and we make the dependence of the final computational cost of the MLMC method explicit in $k.$ In our application to the Helmholtz equation, $k$ will be the wavenumber of the problem.
%% \item In \cite{ClGiScTe:11} it is assumed that the approximating QoIS $\Qhl$ exist for all levels $l$. This corresponds to the finite-element solution of the PDE under investigation existing for all mesh sizes $h.$ Whilst this assumption is true for the stationary diffusion equation studied in \cite{ClGiScTe:11}, it is \emph{not} true for the Helmholtz equation that we study here. Therefore we make the additional assumption (\cref{ass:qoie} below) that $\Qhl$ only exists for sufficiently small $\hl.$
%% \item In \cite{ClGiScTe:11} the error $\eps$ incurred in the MLMC method is equally divided between the bias and the variance of the MLMC method (see the Proof of \cref{thm:mlmccomp3}). However, in this work we assume that there is a quantity $\splitting \in (0,1)$ (see \cref{ass:splittingbounds}), possibly dependent on $k$ that allows a vairable `split' of the error between the bias and the variance. Our main use of this is in\optodo{Insert refs once it's done}, where we use this variable splitting to compensate for the fact that to bound the (squared) bias error by $\eps^2/2$ would mean we take $\hL \lesssim k^{-1},$ but to ensure the finite-element solution $\uh$ exists, we must take $\hL \lesssim k^{-3/2}.$
%% \een
%% We now proceed to prove our abstract MLMC convergence result, comtaining the generalisations metioned above.

\section{Set up}\label{sec:mlmcsetup}

We work in the framework of \cref{chap:stochastic}. For $h>0,$ define the random field $\uh:\Omega \rightarrow \Vhp$ by letting $\uh(\omega)$ solve \cref{prob:fevgen} (with $\T = \TR$) with coefficients $A(\omega)$ and $n(\omega)$ (if it exists). For simplicity throughout this \cref{chap:mlmc} we assume $\uh$ is measurable. Let $Q:\HozDDR\rightarrow\RR$ be a (measurable) \defn{quantity of interest} (QoI) of the solution $u$ (so that $Q(u)$ is a random variable). As an abuse of notation, we also use $Q$ to denote $Q \circ u,$ where the context means this notation is unambiguous. If $\uh(\omega)$ exists, we let $\Qh$ denote $Q \circ \uh.$

As hnited at in the preceeding paragraph, the finite-element solution $\uh(\omega)$ may not always exist. Moreover, the existence (or not) of $\uh(\omega)$ is due to the fact that the constants involved in the definitions of accuracy and data-accuracy of a finite-element method (\cref{def:hkacc,def:hkdataacc} above) are dependent on the coefficients $A$ and $n$. Therefore, when $A$ and $n$ are random fields, the associated existence and uniqueness criterion (and a priori bounds) are all path-dependent. To define this path-dependence precisely, we use the following \lcnamecref{def:probdataacc}.

\bas[Probabilistic version of data-accuracy]\label{def:probdataacc}
There exist random variables $\Co$ and $\cotilde$ such that if
\beq\label{eq:probdataacc}
hk^a < \Co(\omega),
\eeq
then $\uh(\omega)$ exists, is unique, and $Q$ and $\Qh$ satisfy
\beqs
\abs{Q(\omega)-\Qh(\omega)} \leq \cotilde(\omega) h^\alpha k^\sigma \Cfg.
\eeqs
\eas

In order to define a finite-element approximation of $u$ (and therefore a random variable $\Qh$) that exists almost surely, we borrow a technique from \cite{GrPaSc:19}. Informally, for a given $h>0,$ for any realisations $\omega \in \Omega$ such that \cref{eq:probdataacc} is \emph{not} satisfied, we compute on a finer mesh (that does satisfy \cref{eq:probdataacc}). For fixed $h>0$ we define
\beq\label{eq:hmaxomega}
\hmaxomega \de \Co(\omega)k^{-a}.
\eeq
We then define
\beq\label{eq:homega}
\homega \de \min\set{h,\hmaxomega}.
\eeq
Observe that $\homega$ \emph{always} satisfies \cref{eq:probdataacc}. We then define
\beqs
\uhtilde(\omega) = u_{\homega}(\omega),
\eeqs
i.e., $\uhtilde$ is the finite-element approximation of $u(\omega)$ on the mesh with mesh size $\homega.$ We then define
\beq\label{eq:Qhtilde}
\Qhtilde = Q \circ \uhtilde.
\eeq

Given $\uhtilde$ has a random mesh size, the cost of computing $\uhtilde$ (or $\Qhtilde$) will be a random variable. Therefore we make the following \lcnamecref{ass:costone} on the cost of computing samples.

\bas[Cost of one sample]\label{ass:costone}
There exist $\cthtilde, \gamma > 0$ such that $\cthtilde$ is independent of $h$ and $k$, and if $\Qhtilde(\omega)$ exists, then
\beqs
\Cost{\Qhtilde(\omega)} \leq \cthtilde(\omega) \homega^{-\gamma},
\eeqs
\eas

The following \lcnamecref{lem:c} shows how, provided the set $\Omegabad$\optodo{Chat about this earlier/here} has small probability, the expected cost of computing a sample of $\Qhtilde$ is governed solely by the specified mesh size $h$ (and not by any over-refinement). The assumption \cref{eq:cass} is the technical version of the statement `$\Omegabad$ has small probability'.

\bas[Assumptions on $\Omegabad$]\label{ass:omegabad}
Assume:
\beq\label{eq:cass}
\cth \de \EXP{\cthtilde}+ \EXP{\cthtilde\Co^{-\gamma}} < \infty
\eeq
\eas

\ble[Expected cost of one sample]\label{lem:c}
If \cref{ass:omegabad} holds, then 
\beq\label{eq:singlecost}
\EXP{\Cost{\Qh}} \leq \cth \mleft(h^{-\gamma}+k^{a\gamma}\mright).
\eeq
\ele

\bpf[Proof of \cref{lem:c}]
The proof follows closely that in \cite[Lemma 5.8]{GrPaSc:19}.
We have
\beq\label{eq:costpf1}
\Cost{\uhtilde(\omega)} \leq \cthtilde(\omega)\homega^{-\gamma} \leq \cthtilde(\omega) \mleft(h^{-\gamma} + \mleft(\hmaxomega\mright)^{-\gamma}\mright)
\eeq
by \cref{ass:costone,eq:homega}. Then using \cref{eq:hmaxomega}, the definition of $\hmaxomega,$ \cref{eq:costpf1} is bounded by
\beq\label{eq:costpf2}
\cthtilde(\omega)h^{-\gamma} + \mleft(\cthtilde\Co\mright)(\omega) k^{a\gamma},
\eeq
and therefore as \cref{eq:cass} holds, we obtain \cref{eq:singlecost}.
\epf

\ble[Convergence of numerical method]\label{ass:a}
Under \cref{def:probdataacc}, there exist constant $\co, \alpha, \sigma> 0$, such that $\co$ is independent of $h$ and $k$, and
\beqs
\abs{\EXP{\Qhtilde-Q}} \leq \co k^\sigma h^{\alpha}.
\eeqs
\ele

\bpf[Proof of \cref{ass:a}]
The proof of \cref{ass:a} is immediate (with $\co = \EXP{\cotilde}$) from the definition of $\Qhtilde$ \cref{eq:Qhtilde}, \cref{def:probdataacc} and the fact that $\homega \leq h$ (by \cref{eq:homega}.
\epf

\bde[Root-mean-squared error]\label{def:rmse}
Given a random variable $Q$ and an estimator $\Qhat$ of Q, the \defn{root-mean-squared error} of $\Qhat$ is
\beqs
\err{\Qhat} \de \mleft(\EXP{\mleft(\Qhat-Q\mright)^2}\mright)^{\half}.
\eeqs
\ede

\section{Monte-Carlo methods}\label{sec:mc}

We now define the Monte-Carlo estimator of $Q$,
\beqs
\QhatMC \de \frac1{\NMC} \sum_{j=1}^{\NMC} \Qhtildesj,
\eeqs
where the $\Qhtildesj$ are independently and identically distributed samples of $\Qhtilde.$

We can prove the following \lcnamecref{thm:hhmc} on the computational complexity of the Monte-Carlo estimator $\QhatMC$

\bth[Computational complexity of Monte-Carlo]\label{thm:hhmc}
Let the assumptions of \cref{lem:c,ass:a} hold. Given $\eps \in (0,1),$ if
\beq\label{eq:NMC}
\NMC  \sim 2\VAR{\Qhtilde}\eps^{-2}
\eeq
and
\beq\label{eq:hMC}
h \sim \mleft(\sqrt{2}\co\mright)^{-\frac1{\alpha}}k^{-\frac\sigma\alpha}\eps^{\frac1{\alpha}},
\eeq
then $\err{\QhatMC} \leq \eps$ and the computational complexity of $\QhatMC$ satisfies
\beqs
\EXP{\CMC} \sim \VAR{\Qhtilde}\mleft(\eps^{-2-\frac{\gamma}{\alpha}}k^{\frac{\gamma\sigma}\alpha} + \eps^{-2}k^{a\gamma}\mright).
\eeqs
\enth

\bpf[Proof of \cref{thm:hhmc}]
The proof is standard, see, e.g., \cite[Section 2.1]{ClGiScTe:11}. We have
\begin{align}
\err{\QhatMC}^2 &= \EXP{\mleft(\QhatMC - \EXP{\QhatMC} + \EXP{\QhatMC} - \EXP{Q}\mright)^2}\nonumber\\
&= \EXP{\mleft(\QhatMC - \EXP{\QhatMC}\mright)^2} + \mleft(\EXP{\QhatMC} - \EXP{Q}\mright)^2\nonumber\\
&= \VAR{\QhatMC} + \mleft(\EXP{\QhatMC} - \EXP{Q}\mright)^2,\label{eq:mccomp1}
\end{align}
where the second line follows from the fact that $\EXP{\QhatMC - \EXP{\QhatMC}} = 0$, and the third line follows from the fact that $\QhatMC$ is an unbiased estimator.

By definition of $\QhatMC$, and the fact that the samples $\Qhtildesj$ are independent, we have
\beq\label{eq:mccomp2}
\VAR{\QhatMC} = \frac1{\NMC^2}\sum_{j=1}^{\NMC}\VAR{\Qhtildesj} = \frac1{\NMC} \VAR{\Qhtilde}.
\eeq
Therefore we can conclude from \cref{eq:mccomp1,eq:mccomp2} that the root-mean-squared-error satisfies
\beq\label{eq:mccomp3}
\err{\QhatMC}^2 = \frac1{\NMC}\VAR{\Qhtilde} + \NLoO{\Qhtilde-Q}^2.
\eeq
By \cref{eq:NMC,eq:hMC}, each of the terms in \cref{eq:mccomp3} is bounded by  $\eps^2/2,$ and therefore $\err{\QhatMC} \leq \eps.$

All that remains is to estimate the (expected) computational complexity. We have
\beqs
\EXP{\Cost{\QhatMC}} = \NMC \EXP{\Cost{\uhtilde}} \leq \NMC \cth h^{-\gamma} \sim 2\VAR{\Qhtilde}\eps^{-2} \mleft(\cth \mleft(\sqrt{2} \co\mright)^{\frac\gamma\alpha}k^{\frac{\gamma\sigma}{\alpha}}\eps^{-\frac\gamma\alpha} + k^{a\gamma}\mright)
\eeqs
as required.
\epf

\section{Multi-level Monte-Carlo}\label{sec:mlmcan}

We define a set of levels$\set{\hl}_{l=0}^L$ ($L$ to be chosen) such that $\hl =\frac{\hlmo}s$ for $l \geq 1$. We then define the correction operators between the levels by $\Yl \de \Qhltilde - \Qhlmotilde, l \geq 1,$ $\Yz = \Qhztilde.$ We let $\Ylhat$ be the Monte-Carlo estimator of $\Yl$,
 \beqs
\Ylhat \de \frac1{\Nl}\sum_{i=1}^{\Nl} \Yli,
 \eeqs
 with $\Nl$ to be chosen, where $\Yli$ denotes independent samples of $\Yl$. Finally we are able to define the \defn{multi-level Monte Carlo estimator}
 \beqs
 \QhatMLhL \de \sum_{l=1}^L \Ylhat,
 \eeqs
 where the $\Ylhat$ are independent.

  %% The following assumptions
  %% % \lcnamecrefs{ass:coarse}
  %%  will form the backbone of our analysis. They are a generalisation of the assumptions contained in \cite{ClGiScTe:11,ChScTe:13} for the MLMC method, the generalisation being that we assume that the quantities below depend not only on the levels $\hl$ but also on some additional parameter $k>1.$ When this theory is applied to the Helmholtz equation, $k$ will be the wavenumber of the Helmholtz equation.

%% The following assumption (which will be realised in a more concrete setting for the Helmholtz equation) concerns the existence of the approximating QoIs $\Qhl.$

%% \bas[Existence of $\Qhl$]\label{ass:qoie}
%% There exist $\Ccoarse,\coarseexp > 0$ with $\Ccoarse$ independent of $k$ such that if
%% \beqs
%% \hl \leq \Ccoarse k^{-\coarseexp},
%% \eeqs
%% then the QoI $\Qhl$ exists.
%% \eas

\bas[Variance of correction operators]\label{ass:b}
There exist $\ct, \beta, \tau > 0$, such that $\ct$ is independent of $h$ and $k,$ and
\beq\label{eq:mlmcassb}
\Vl \de \VAR{\Yl} \leq \ct k^\tau\hl^{\beta},
\eeq
where $\VAR{\cdot}$ denotes variance.
\eas

In order to obtain a nice expression for the cost of computing one sample of $\Qh,$ we require the following assumption on the coarse space:

\bas[Dependence of coarse space on $k$]\label{ass:coarse}
We let
\beqs
\hz = \Ccoarse k^{-a}.
\eeqs
for some chosen constant $\Ccoarse > 0.$
\eas

 
% We write $\Vl$ for $\VAR{\Yl}.$
 
 We want to determine the choices of $L$ and $\Nl, l = 0,\ldots,L,$ such that the root-mean-squared eror (RMSE)
 \beqs
 \err{\QhatMLhL} \de \mleft(\EXP{\mleft(\QhatMLhL - \EXP{Q}\mright)^2}\mright)^{\half}
 \eeqs
 satisfies $\err{\QhatMLhL} \leq \eps,$ for some pre-defined $\eps > 0.$



%\input{nice}



%% \subsection{Lemma}

%% The proof of the main \lcnamecref{thm:mlmccomp} will require the following \lcnamecref{lem:sumbound}.

%% \ble\label{lem:sumbound}
%% If $L$ is given by

%% then, for $s>1$ and $\delta \in \RR,$ we have the bound
%% \beq\label{eq:sumbound}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L+1 & \tif \delta = 0,\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}s^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha} &\tif \delta >0\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}&\tif \delta < 0
%% \end{cases}
%% \eeq
%% \ele

%% \bpf[Proof of \cref{lem:sumbound}]
%% The proof follows that in \cite{ClGiScTe:11}. We first observe that, since $L$ is given by \eqref{eq:Ldef}, it follows that
%% \beq\label{eq:Lbounds}
%% \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) \leq L < \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) + 1.
%% \eeq
%% Rearranging \eqref{eq:Lbounds}, we obtain the bounds
%% \beq\label{eq:saLbounds}
%% \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1} \leq s^{\alpha L} < \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1}s^\alpha.
%% \eeq
%% If $\delta > 0,$ then we use the right-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLpos}
%% s^{\delta L} < \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}s^{\delta},
%% \eeq
%% and if $\delta < 0,$ we use the left-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLneg}
%% s^{\delta L} \leq \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}.
%% \eeq
%% We now observe that, for $\delta \neq 0,$
%% \begin{align}
%% \sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
%% &= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
%% &\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumbound}
%% \end{align}
%% since $s^{-\delta} > 0,$ as $s >0.$ Combining \eqref{eq:ssumbound} with \eqref{eq:sdLpos} and \eqref{eq:sdLneg}, we obtain \eqref{eq:sumbound} in the cases $\delta \neq 0.$ The case $\delta=0$ is straightforward.
%% \epf


The following theorem describes the computational effort needed to obtain RMSE $\leq \eps$. It is exactly the same as \cite[Theorem 1]{ClGiScTe:11}, but with the dependence on all the parameters explicit.%, and with some additional cases enumerated. %\Cref{thm:mlmccomp} contains more cases than in \cite[Theorem 1]{ClGiScTe:11} because \cite[Theorem 1]{ClGiScTe:11} makes the assumption throughout that $\alpha \geq 1/2\min\set{\beta,\gamma}.$ This assumption does not always hold for the Helmholtz equation (see the cases of a direct solver in 3-D below), however, examining the proof of \cite[Theorem 1]{ClGiScTe:11}  shows that in any given case, one only needs the assumption $\alpha \geq \beta/2$ or the assumption $\alpha \geq \gamma2$, never both at the same time. Therefore, for convenience, we explicitly state when these conditions are needed, and for completeness, we give the results when these conditions are violated. 

%% The following \lcnamecref{ass:constants3} will ensure that \cref{ass:qoie} is satisfied.

%%  \bas[$\eps$ sufficiently small]\label{ass:constants3}
%%  Assume
%%  \beqs
%% \eps \leq \sqrt{2} \co \Ccoarse^{\alpha} k^{\sigma-\coarseexp\alpha}.
%%  \eeqs
%%  \eas


%% \bas[Assumptions on $\eps$ and $k$ to simplify expressions in the case $\beta=\gamma$]\label{ass:epsk}
%% \beqs
%% \eps \leq \min\set{\frac{\sqrt{2}\co\Ccoarse^\alpha}{s^{2\alpha}},\frac1{\sqrt{2}\co\Ccoarse^\alpha}},
%% \eeqs
%% and
%% \beqs
%% k^{\sigma-a\alpha} \geq 1.
%% \eeqs
%% \eas
\bth[MLMC Complexity Theorem]\label{thm:mlmccomp}
The number of levels $L$ is given by
\beq\label{eq:Ldef}
L = \max\set{\ceil{\frac1\alpha\log_{s}\mleft(\sqrt{2}\co  \Ccoarse^\alpha k^{\sigma-a\alpha} \eps^{-1}\mright)},0},
\eeq
that is,
\beq\label{eq:hLcond}
\hL \leq \min\set{\mleft(\frac\eps{\sqrt{2}\co k^{\sigma}}\mright)^{\frac1\alpha},\hz},
\eeq
and the number of samples on each computational level is given by
\beq\label{eq:Nl}
\Nl = \ceil{\frac2{\eps^{2}} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^{L} \mleft(\Vj\Cj\mright)^{\half}},
\eeq
where $\Cl \de \cth\hl^{-\gamma}$. In this case, if $L=0$, then computational effort $\CMLhL(\eps)$ required to obtain $\err{\QhatMLhL} \leq \eps$ is given by \cref{thm:hhmc}. Otherwise, $\CMLhL(\eps)$ satisfies the bounds
 
 \begin{numcases}{ \CMLhL(\eps) \lesssim}
k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^{\sigma-a\alpha}}\eps\mright)+2\mright)^2  & if $\beta = \gamma$,\label{eq:mlmchheq}\\ 
k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}} + k^{\frac{\gamma\sigma}{\alpha}}\eps^{-\frac\gamma\alpha} & otherwise.\label{eq:mlmchhoth}
\end{numcases}
 \enth

For an explanation of why one must include the maximum in \cref{eq:Ldef}, see the proof of \cref{thm:mlmchelmholtz} below.
 
 \bpf[Proof of \cref{thm:mlmccomp}]
Throughout the proof, we assume $L>0,$ as the case $L=0$ is given by \cref{thm:hhmc}. We recall the decomposition of the (squared) mean-squared error into the bias error and the sampling error analagous to \cref{eq:mccomp1}
\beqs\label{eq:mlmcdecomp}
\errQhatMLhL^2 = \VAR{\QhatMLhL} + \mleft(\EXP{\QhatMLhL - Q}\mright)^2.
\eeqs
We now proceed to choose the parameters $L$ and $\Nl, l = 0,\ldots,L$ such that we can bound both the bias and the variance by $\eps^2/2.$

We first bound the bias, to do this, we only need to choose $L.$ One can show that the bias is equal to $\abs{\EXP{\QhL - Q}}^2.$ Therefore by \cref{ass:a} a sufficient condition for the bias to be $\leq \eps^2/2$ is (by \cref{ass:a})
\beqs
\co k^\sigma \hL^\alpha \leq \frac{\eps}{\sqrt{2}},
\eeqs
that is, \eqref{eq:hLcond}. As $\hL = \hz s^{-L},$ it follows from \eqref{eq:hLcond} that a sufficient condition for the bias to be $\leq \eps^2/2$ is
\beq\label{eq:Lcondpart}
L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co k^\sigma \hz^\alpha \eps^{-1}\mright)}.
\eeq
As $\hz = \Ccoarse k^{-a},$ we can simplify \eqref{eq:Lcondpart} to obtain \eqref{eq:Ldef}.
% \beqs
% L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma-\coarseexp\alpha} \eps^{-1}\mright)}.
% \eeqs

We now seek to bound the variance. One can show similarly to \cref{eq:mccomp2} that the variance $\VAR{\QhatMLhL} = \sum_{l=0}^L \Nl^{-1} \Vl,$ and the cost is: (following \cite{GrPaSc:19})
\begin{align}
\EXP{\Cost{\QhatMLhL}}&\leq \sum_{l=0}^L \EXP{\Cost{\Ylhat}}\nonumber\\
&= \sum_{l=0}^L \sum_{i=1}^{\Nl} \EXP{\Cost{\Yli}}\nonumber\\
&\leq \sum_{l=0}^L \sum_{i=0}^{\Nl} \mleft(\EXP{\Cost{\Qhltilde}} + \EXP{\Cost{\Qhlmotilde}}\mright)\nonumber\\
%% &\leq \sum_{l=0}^L \Nl \mleft(\cth \hl^{-\gamma} + \cth \hlmo^{-\gamma}\mright)\nonumber\\
&=\sum_{l=0}^L \Nl\mleft(1+s^{-\gamma}\mright) \cth \hl^{-\gamma}\nonumber\\
&= \mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \Nl\Cl\label{eq:Cboundformin}
\end{align}

To find the optimal number of samples per level (the values of $\Nl, l=0,\ldots,L$) we formulate this as an optimisation problem to find the numbers $\Nl$ that minimise \eqref{eq:Cboundformin}, subject to $\VAR{\QhatMLhL}=\eps/2.$ This can be solved using a Lagrange multiplier as in \cite{Gi:15}, and we obtain \cref{eq:Nl}. We now need to infer the computational complexity for MLMC with $L$ given by \eqref{eq:Ldef} and the $\Nl$ given by \eqref{eq:Nl}.

From \cref{eq:Cboundformin}
\begin{align}
\EXP{\Cost{\QhatMLhL}} &\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^{L} \Cl \Nl\nonumber\\
&\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl \mleft(\frac2{\eps^{2}} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^L \mleft(\Vj\Cj\mright)^{\half} + 1\mright) \text{ (by \eqref{eq:Nl})}\nonumber\\
&= 2\eps^{-2}\mleft(1+s^{-\gamma}\mright)\mleft(\sum_{l=0}^L\mleft(\Vl\Cl\mright)^{\half}\mright)^2 + \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl\nonumber\\
&= 2 \ct \cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2} \mleft(\sum_{l=0}^L \hl^{\frac{\beta-\gamma}2}\mright)^2 + \cth \mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \hl^{-\gamma} \text{ (by \cref{ass:b,ass:costone})}\nonumber\\
&= 2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\hz^{\beta-\gamma}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 + \cth\mleft(1+s^{-\gamma}\mright) \hz^{-\gamma} \sum_{l=0}^L s^{\gamma l} \text{ (by definition of } \hl\text{ )}\label{eq:complexitymidway}\\
%% &=2 \ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 + \cth\Cppw^{-\gamma} k^{\rho + \gamma\coarseexp}  \sum_{l=0}^L s^{\gamma l} \text{ (by definition of } \hz\text{ )}\nonumber\\
%% &\leq2\ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 +  \frac{\mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha}\cth s^{\gamma}}{1-s^{-\gamma}}k^{\rho +  \frac{\gamma\sigma}\alpha}\eps^{-\frac\gamma\alpha} \text{ (since }\gamma>0,\text{ by \cref{lem:sumbound})}.\label{eq:complexitymidway}
\end{align}

Using \cref{lem:sumboundnew} with $\Lconst = 1/\alpha,$ $\func = \sqrt{2}\co\Ccoarse^\alpha k^{\sigma - a\alpha}$, and $\delta = \gamma$, the second term in \eqref{eq:complexitymidway} can be bounded (as $\gamma > 0$) by %(letting \csumdelta \de \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha} \Ccoarse^\delta / \mleft(1-s^{-\delta}\mright)$)
\beq\label{eq:firstterm}
\cth\frac{\mleft(1+s^{-\gamma}\mright)  \hz^{-\gamma} s^\gamma \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} \Ccoarse^\gamma}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha-a\gamma} \eps^{-\frac\gamma\alpha}
= \frac{\mleft(1+s^{-\gamma}\mright)\cth \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} s^\gamma}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha} \eps^{-\frac\gamma\alpha}
\eeq

To bound the sum in the first part of \eqref{eq:complexitymidway}, we must distinguish three cases based on $\gamma - \beta.$


If $\gamma=\beta,$ then the first part of \eqref{eq:complexitymidway} becomes (using \cref{lem:sumboundnew} with $\Lconst$ and $\func$ as above, and $\delta = 0$ and \cref{eq:Ldef})
\beq
2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(L+1\mright)^2 \leq 2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^{\sigma-a\alpha}}\eps\mright)+2\mright)^2,
\label{eq:gammaequal}
\eeq
which gives \cref{eq:mlmchheq}.
%We wish to simplify \eqref{eq:gammaequal}, so that it is of the form Constant $\times$ `Terms involving $\eps$ and $k$'. To achieve this simplification, we use \cref{ass:epsk}. As $k^{\sigma-a\alpha} \geq 1$ and $\eps \leq \mleft(\sqrt{2} \co \Ccoarse^{\alpha}\mright)/s^{2\alpha},$ it follows that
%% \beqs
%% 2 \leq \frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright),
%% \eeqs
%% and thus \eqref{eq:gammaequal} can be bounded by
%% \beq\label{eq:gammaequalpart1}
%% 8 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright)\mright)^2.
%% \eeq
%% As $k^\sigma k^{-a\alpha} \geq 1$ and $\eps \leq 1/\mleft(\sqrt{2}\co\Ccoarse^\alpha\mright),$ we can bound \eqref{eq:gammaequalpart1} by (including a change of base in the logarithm)
%% \beq\label{eq:gammaequalfinal}
%% \frac{32 \ct\cth \mleft(1+s^{-\gamma}\mright)}{\alpha^2 \mleft(\loge(s)\mright)^2} k^\tau \mleft(\loge\mleft(\frac{k^{\sigma-a\alpha}}\eps\mright)\mright)^2
%% \eeq
%% and obtain \cref{eq:mlmchheq}.

For simplicity in the proof of \cref{eq:mlmchhoth}, we define
\beqs
\csumdelta \de \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}.
\eeqs

If $\gamma > \beta$ then using \cref{lem:sumboundnew} as above, but with $\delta = (\gamma-\beta)/2$, the first term in \eqref{eq:complexitymidway} becomes
\beq
\eps^{-2}2\ct\cth \mleft(1+s^{-\gamma}\mright) k^\tau \hz^{\beta-\gamma}\mleft(\csumgammambetat s^{\frac{\gamma-\beta}2} k^{\frac{\gamma-\beta}2\frac\sigma\alpha} k^{-a\frac{\gamma-\beta}2} \eps^{-\frac{\gamma-\beta}{2\alpha}}\mright)^2 = \Cgammagtrbeta k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}},\label{eq:gammagtr}
\eeq
where
\beqs
\Cgammagtrbeta \de 2\ct\cth\mleft(1+s^{-\gamma}\mright)\csumgammambetat^2 s^{\gamma-\beta} \Ccoarse^{\beta-\gamma},
\eeqs
that is, an expression of the form \cref{eq:mlmchhoth}. The second equality in \cref{eq:gammagtr} follows from the definition of $\hz$ in \cref{ass:coarse}.
If $\gamma < \beta,$ then analagously the first term in \eqref{eq:complexitymidway} is
\beqs
\Cgammalessbeta k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}},
\eeqs
where
\beq\label{eq:gammaless}
\Cgammalessbeta \de \frac{\Cgammagtrbeta}{s^{\gamma-\beta}}.
\eeq

We now combine \eqref{eq:firstterm}, \eqref{eq:gammaequalfinal}, \eqref{eq:gammagtr}, and \eqref{eq:gammaless} and supress all the constants to obtain the result.
%Removing all the terms that are not of interest from \eqref{eq:gammaequal}, \eqref{eq:gammagtr}, and \eqref{eq:gammaless}, we obtain \eqref{eq:mlmchheq} and \eqref{eq:mlmchhoth}.
\epf


%\input{nasty}

\subsection{Technical lemma}

\ble\label{lem:sumboundnew}
If $L$ is given by
\beq\label{eq:Ldefgen}
L = \ceil{\Lconst\log_{s}\mleft( \func \eps^{-1}\mright)},
\eeq
for some $\Lconst, \func > 0,$ then, for $s>1$ and $\delta \in \RR,$ we have the bound
\beq\label{eq:sumboundgen}
\sum_{l=0}^{L} s^{\delta l} \leq
\begin{cases}
L+1 & \tif \delta = 0,\\
\frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0\\
\frac{1}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst}&\tif \delta < 0
\end{cases}
\eeq
%% \beq\label{eq:sumboundLmo}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L & \tif \delta = 0,\\
%% \frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0\\
%% \frac{1}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst}&\tif \delta < 0
%% \end{cases}
%% \eeq
%\opctodo{Tidy}
\ele

\bpf[Proof of \cref{lem:sumboundnew}]
The proof follows that in \cite{ClGiScTe:11}. We first observe that, since $L$ is given by \eqref{eq:Ldefgen}, it follows that
\beq\label{eq:Lboundsgen}
\Lconst\log_s\mleft(\func \eps^{-1}\mright) \leq L < \Lconst\log_s\mleft(\func \eps^{-1}\mright) + 1.
\eeq
Rearranging \eqref{eq:Lboundsgen}, we obtain the bounds
\beq\label{eq:saLboundsgen}
\mleft( \func\eps^{-1}\mright)^{\alpha \Lconst} \leq s^{\alpha L} < \mleft( \func\eps^{-1}\mright)^{\alpha \Lconst}s^\alpha.
\eeq
If $\delta > 0,$ then we use the right-hand bound in \eqref{eq:saLboundsgen} to obtain
\beq\label{eq:sdLposgen}
s^{\delta L} < \func^{\delta\Lconst}\eps^{-\delta\Lconst}s^{\delta},
\eeq
and if $\delta < 0,$ we use the left-hand bound in \eqref{eq:saLboundsgen} to obtain
\beq\label{eq:sdLneggen}
s^{\delta L} \leq \func^{\delta\Lconst}\eps^{-\delta\Lconst}.
\eeq
We now observe that, for $\delta \neq 0,$
\begin{align}
\sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
&= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
&\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumboundgen}
\end{align}
since $s^{-\delta} > 0,$ as $s >0.$ Combining \eqref{eq:ssumboundgen} with \eqref{eq:sdLposgen} and \eqref{eq:sdLneggen}, we obtain \eqref{eq:sumboundgen} in the cases $\delta \neq 0.$ The case $\delta=0$ is straightforward.
\epf

\section{Application}\label{sec:mlmcapp}

We now apply the above results on the computational complexity of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation to two model QoIs, $\NLtD{u}$ and $\NHokD{u}$, where $u$ is the solution of the TEDP-analogue of \cref{prob:msedp} in \cref{chap:stochastic} (see \cref{rem:tedp}). We choose these two representative QoIs as
\bit
\item The errors in their approximation have different $k$-dependencies (see \cref{lem:mlho,lem:mllt} below), and
  \item We expect other QoIs to have a similar $k$-dependence to $\NHokD{u}$ or $\NLtD{u}$, depending on whether the QoI depends on $u$ and $\grad u$, or just on $u$, respectively.
    \eit

    All that is left for us to determine are the values of $\alpha, \sigma, \beta, \tau, \gamma,$ and $a$ for each of these QoIs. The values of $\alpha$ and $\sigma$ are given by the convergence results for finite-element approximations of $u$; see \cref{thm:fembound} above and \cref{lem:mlho,lem:mllt} below. As we see in \cref{lem:mlho,lem:mllt} below, $\beta = 2\alpha$, and $\tau = 2\sigma$. In our calculations in this \lcnamecref{sec:mlmcapp} we assume $\gamma = d;$ i.e., we assume that we have a Helmholtz solver that scales optimally in the number of degrees of freedom; obtaining such a solver is the subject of much current research, and we refer to, e.g., the recent works \cite{GrSpVa:17,ZeScHeDe:19,TaZeHeDe:19} for a selection of modern solvers achieving close to this optimal scaling.

We use two different values of $a$ below, $a=-(2p+1)/2p$ (where $p$ is the polynomial degree of the finite-elements) and $a=-1$. Based on the finite-element result \cref{thm:fembound}, we should take $a=(2p+1)/2p = 1 + 1/2p;$ such a $k$-dependence is required in \cref{thm:fembound} for the finite-element solution to exist and be unique. However, such a choice of $a$ does not allow the number of levels $L$ to grow with $k$ for $Q = \NHokD{u}$: in \cref{eq:Ldef} above, $L$ depends on $k^{\sigma-a\alpha};$ if $a=(2p+1)/2p,$ $\alpha = 2p$, and $\sigma = 2p+1$ (see \cref{lem:mlho} below for details of these values for $\alpha$ and $\sigma$) then $k^{\sigma-a\alpha} = 1$, and therefore $L$ is $k$-independent. However, if $a = 1,$ then $k^{\sigma-a\alpha} = k,$ and $L$ grows (log-linearly) with $k$. We observe that such a choice for $a$ (keeping the same values for $\alpha$ and $\sigma$ can be heuristically justified; in \cite[Theorem 3.2]{Ai:04} Ainsworth shows for grids with cube elements that if $h \ll 1/k,$ then the phase error (i.e., $k-\kh,$ where $\kh$ is the discrete wavenumber associated with the finite-element solution) is of the order $h^{2p}k^{2p+1}$. We therefore also investigate the beahviour of MC and MLMC under the assumption $a=-1$ and the finite-element errors are given by terms of the same leading order as \cref{eq:femltbound,eq:femhobound}.

To summarise the results that we see in \cref{thm:mlmchelmholtz} below, in terms of $k$-dependence, the results for MC- and MLMC-methods are identical. We also see that, in terms of $\eps$-dependence, MLMC methods are consistently better than MC methods, unless the condition for existence and uniqueness is more restrictive than the condition to keep the error bounded\footnote{In the cases we consider, this only occurs when we take $a = (2p+1)/2p$ and $Q(u) = \NLtD{u},$ so $\alpha = \sigma = 2p.$}. In such a case, for $\eps$ small and/or $k$ small, MLMC out-performs MC, but if $\eps$ and/or $k$ are large, then MLMC is identical to MC (because there are no additional levels).
\begin{table}
  \centering
\begin{tabular}{Sc Sc Sc Sc}
  \toprule
  $Q(u)$ & $a$ & Monte-Carlo & Multi-Level Monte-Carlo\\
  \midrule
      $\NHokD{u}$ & $\displaystyle \frac{2p+1}{2p}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-\frac{d}{2p}}$ \\
  $\NLtD{u}$ & $\displaystyle \frac{2p+1}{2p}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & \makecell{$\displaystyle k^{d} \eps^{-\frac{d}{2p}}$ if $k\eps$ small,\\otherwise $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$} \\
    $\NHokD{u}$ & 1 &$\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-\frac{d}{2p}}$ \\
      $\NLtD{u}$ & 1 & $\displaystyle k^d \eps^{-2-\frac{d}{2p}}$ &$\displaystyle k^d \eps^{-\frac{d}{2p}}$\\
  \bottomrule
\end{tabular}
\caption{Computational complexity of Monte-Carlo and Multi-Level Monte-Carlo algorithms\label{tab:mcresults}}
\end{table}
%% If $Q(\cdot) = \NHokDR{\cdot},$ then $\alpha = 2p,$ $\sigma = 2p+1$. If $Q(\cdot) = \NLtDR{\cdot},$ then $\alpha = 2p,$ $\sigma = 2p$. In both cases, $\beta = 2\alpha,$ $\tau = 2\sigma.$ Assume $\gamma = d$---optimal solver.

%% FINISH TOMORROW.

\section{Verifying the assumptions in \cref{sec:mlmcsetup}, and applying the MC and MLMC theory}\label{sec:mlmcapplying}

We now let $u:\Omega\rightarrow\HokD$ solve the TEDP-analogue of \cref{prob:msedp}, and $\uhtilde:\Omega\rightarrow\Vhp$ solve the stochastic analogue of \cref{prob:fevtedp} (i.e., $\uhtilde$ solves \cref{prob:fevtedp} with coefficients $A(\omega)$ and $n(\omega)$, $\T = ik$, and meshsize $\homega$ pathwise). We assume $\uhtilde$ is measurable.


\ble[Verifying assumptions for $Q(u) = \NHokD{u}$]\label{lem:mlho}
If \cref{ass:coarse} holds with $\Ccoarse$ bounded by $\Chcond \Condn(n) \CAnk^{-\frac1{2p}}$ (as defined in \cref{thm:fembound}) and $a = (2p+1)/2p$, and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p+1,$ $\beta = 4p$, and $\tau = 4p+2$.
\ele

\bpf[Proof of \cref{lem:mlho}]
By the assumptions of the \lcnamecref{lem:mlho}, it is immediate from \cref{eq:femhobound} that \cref{def:probdataacc} holds with $\alpha = 2p$ and $\sigma = 2p+1$. (See \cref{rem:higherorder} for why we can neglect the lower-order terms in \cref{eq:femhobound}.) To show \cref{ass:b}, we follow \cite[Proof of Proposition 4.2]{ChScTe:13} and use the triangle inequality and \cref{def:probdataacc} to show
\beq\label{eq:Ylhatbound}
\Ylhat(\omega) \leq \mleft(\Qhl - Q\mright)(\omega) + \mleft(Q- \Qhlmo\mright)(\omega) \leq \cotilde(\omega)\mleft(\hl^\alpha + \hlmo^\alpha\mright)k^\sigma \Cfg.
\eeq
We then use the fact that $\VAR{\Ylhat} = \EXP{\Ylhat^2} - \EXP{\Ylhat}^2 \leq \EXP{\Ylhat^2}$ and \cref{eq:Ylhatbound} to show \cref{eq:mlmcassb}, with $\ct = \EXP{\cotilde^2}\mleft(1+s^\alpha\mright)^2.$
\epf

\ble[Verifying assumptions for $Q(u) = \NLtD{u}$]\label{lem:mllt}
If \cref{ass:coarse} holds with $\Ccoarse$ bounded by $\Chcond \Condn(n) \CAnk^{-\frac1{2p}}$ (as defined in \cref{thm:fembound}) and $a = (2p+1)/2p$, and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p,$ $\beta = 4p$, and $\tau = 4p$.
\ele

\bpf[Proof of \cref{lem:mllt}]
The proof is exactly analagous to the proof of \cref{lem:mlho}, except we use \cref{eq:femltbound} instead of \cref{eq:femhobound}.
\epf

As stated in \cref{sec:mlmcapp}, whilst we cannot prove analogues of \cref{lem:mlho,lem:mllt} with $a=1$, we wish to study the behaviour of MLMC when $a=1.$ Therefore, we now state analogues of \cref{lem:mlho,lem:mllt} in the case $a=1.$

\bas[Assumptions for $Q(u) = \NHokD{u}$ with $a=1$]\label{ass:mlho}
There exists $\Ccoarse>0$ such that if \cref{ass:coarse} holds with $a = 1$ and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p+1,$ $\beta = 4p$, and $\tau = 4p+2$.
\eas

\bas[Assumptions for $Q(u) = \NLtD{u}$ with $a=1$]\label{ass:mllt}
There exists $\Ccoarse>0$ such that if \cref{ass:coarse} holds with $a = 1$ and if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p,$ $\beta = 4p$, and $\tau = 4p$.
\eas

We are now in a position to state our main \lcnamecref{thm:mcmlmchelmholtz} on the behaviour of MC and MLMC methods for the Helmholtz equation.

To prove a bound on the complexity of the Monte-Carlo method, we require the following \lcnamecref{ass:variance}.

\bas[Variance of $\Qhtilde$ constant]\label{ass:variance}
The variance $\VAR{\Qhtilde}$ is constant with respect to $h$.
\eas

\bth[Computational complexity of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation]\label{thm:mcmlmchelmholtz}
Under \cref{ass:variance,ass:costone,ass:omegabad}, if the assumptions of \cref{lem:mllt,lem:mlho} hold, then the computational complexity of the Monte-Carlo and Multi-Level Monte Carlo methods for estimating $\EXP{Q(u)}$ is given by the first two lines of \cref{tab:mcresults}.

If \cref{ass:mlho,ass:mllt} hold instead of the assumptions of \cref{lem:mllt,lem:mlho}, then then the computational complexity of the Monte-Carlo and Multi-Level Monte Carlo methods for estimating $\EXP{Q(u)}$ is given by the last two lines of \cref{tab:mcresults}, where `$k\eps$ small' means
\beq\label{eq:kepscond}
k\eps < \sqrt{2}\co \Ccoarse^{2p}.
\eeq
\enth

\bpf[Proof of \cref{thm:mcmlmchelmholtz}]
The proof follows immediately from \cref{thm:mlmccomp}, by substituting in the appropriate values of $\alpha,$ $\beta$, etc.. The only case that requires explaining is when $a=(2p+1)/2p$ and $Q(u) = \NLtD{u}$ (so $\alpha = \sigma = 2p.$) In this case, the expression for $L$ in \cref{eq:Ldef} evaluates as
\beq\label{eq:specialL}
L = \max\set{\ceil{\frac1{2p}\log_s\mleft(\sqrt{2}co\Ccoarse^{2p}k^{-1}\eps^{-1}\mright)},0}
\eeq
Observe that for $k$ (or $\eps$) sufficiently large, the first term in the right-hand side of \cref{eq:specialL} may be negative (i.e. if $k^{-1}$ is suffciently close to 0). In such a case, the maximum of the two quantities on the right-hand side of \cref{eq:specialL} will be 0, and in such a case the MLMC algorithm reverts to the MC algorithm. The criterion for the first term to be positive (and so for MLMC to be distinct from MC) is
\beqs
\sqrt{2}co\Ccoarse^{2p}k^{-1}\eps^{-1} > 1,
\eeqs
which is equivalent to the condition \cref{eq:kepscond}, as required.
\epf

\bre[No coarse-space dependence in MLMC cost bounds]
Observe that there is no dependence on the coarse space (with mesh size $\hz$) in the MLMC results in \cref{thm:mcmlmchelmholtz}, i.e., the quantity $a$ does not appear when $\beta \neq \gamma$. (Recall from \cref{ass:coarse} that the coarse space is chosen to be proportional to $k^{-a}$.) This lack of $a$-dependence is surprising at first glance. If $a < \sigma/\alpha,$ then the number of levels $L$ grows with $k$ (as the term $k^{\sigma - a\alpha}$ in \cref{eq:Ldef} will grow with $k$), and one would expect to see this growing number of levels affect the overall computational complexity. However, it seems that whilst the number of levels grows with $k$, reducing the computational cost, on the other hand the growing number of samples (growing because there are more levels on which to compute) offsets the gains from extra levels.

One can see this offsetting in the proof; the terms $\hz^{\beta-\gamma}$ and $\hz^{-\gamma}$ in \cref{eq:complexitymidway} are exactly cancelled by the appearence of $\hz$ in the application of \cref{lem:sumboundnew}. Observe that in \cref{eq:firstterm} the quantity $\hz^{-\gamma}$ ancels with the term $\Ccoarse^{\gamma}k^{-a\gamma}$ arising from \cref{lem:sumboundnew}. Similarly the $k$-dependence of the term $\hz^{\beta-\gamma}$ in \cref{eq:gammagtr} cancels with the term $\mleft(k^{-a(\gamma-\beta)/2}\mright)^2$ arising from \ref{lem:sumbound}.

It remains to be seen whether this coarse-mesh independence is borne out in numerical computations. Such an investigation could be the subject of future work on MLMC methods for the Helmholtz equation.
\ere

\bre[Proving probabilistic bounds on the cost]
In \cite{GrPaSc:19}, the authors extend their bounds on the expectation of the computational cost for MC and MLMC methods to bounds on the \emph{exceedance probabilities} of the computational cost. I.e., they prove bounds of the form
\beq\label{eq:mcprobbound}
\PP\mleft(\Cost{\Qhat} < M(\eps,\delta,\Qhat)\mright) > 1-\delta^2,
\eeq
for some function $M$, where $\Qhat$ is the MC or MLMC estimator (see \cite[Theorems 5.12 and 5.13]{GrPaSc:19}). They make only mild additional assumptions on the randomness to prove bounds of the form \cref{eq:mcprobbound}; these assumptions mean they can bound $\VAR{\Qhtilde}$ and hence $\VAR{\Cost{\Qhat}}.$ The probabilistic bounds \cref{eq:mcprobbound} then follow from bounds on $\VAR{\Cost{\Qhat}}$ using Chebyshev's inequality.

We could apply these proof techniques to prove a probabilistic bound of the form \cref{eq:mcprobbound} for MC and MLMC methods for the Helmholtz equation. However, the calculations for the Helmholtz equation would be conceptually similar to those in \cite{GrPaSc:19}, albeit more involved, as we would need to keep track of the $k$-dependence. Given we expect the results we obtained would be similar to those in \cite{GrPaSc:19}, we elect not to pursue them.
\ere
\optodo{Make sure bibfile displays DOIs}
