\section{Introduction}

In \cref{sec:nbpcqmc} we considered how to speed up solving the individual linear systems in UQ algorithms for the stochastic Helmholtz equation via nearby preconditioning. We now consider how, using a Multi-Level Monte-Carlo method, one can reduce the total number of linear systems we must solve. In particular, we prove bounds on the computational effort needed for Monte Carlo (MC) and Multi-Level Monte-Carlo (MLMC) methods for the stochastic Helmholtz equation. We compare and constrast the behaviour of these methods for different wavenumbers and tolerances and we show that Multi-Level Monte-Carlo methods asymptotically (as the prescribed tolerance goes to 0) require less work than Monte-Carlo methods.

We highlight that, in contrast to our empirical analysis of Quasi-Monte-Carlo (QMC) methods in \cref{sec:nbpcqmcnumerics}, in this \lcnamecref{chap:mlmc} we provide a rigorous analysis of Monte-Carlo and Multi-Level Monte Carlo methods. We prove how Monte-Carlo and Multi-Level Monte-Carlo methods must be adapted for increasing $k$ to ensure the overall error (both numerical and statistical) remains bounded, and we prove bounds on the expected computational cost of both the Monte-Carlo and Multi-Level Monte-Carlo methods.

We now provide a brief overview of this \lcnamecref{chap:mlmc}. In \cref{sec:overview} we give an introduction to Monte-Carlo and Multi-Level Monte-Carlo methods, and discuss some of the challenges in applying them to the Helmholtz equation. We then review literature on Multi-Level Monte-Carlo methods, focusing only on those works that are relevant for our study of Multi-Level Monte-Carlo methods applied to the stochastic Helmholtz equation. In \cref{sec:mlmcsetup} we give an abstract setting for a $k$-dependent analysis of Multi-Level Monte-Carlo methods. In \cref{sec:mc} we prove a bound on the computational work for the Monte-Carlo method in this abstract setting and in \cref{sec:mlmcan} we prove an analogous result for the Multi-Level Monte-Carlo method. Finally, in \cref{sec:mlmcapplying} we show that the stochastic Helmholtz equation fits into this abstract setting, and we then compare and contrast the behaviour of Monte-Carlo and Multi-Level Monte-Carlo methods for the stochastic Helmholtz equation.

\section[Background on MC and MLMC]{Background on both Monte-Carlo and Multi-Level Monte-Carlo   methods}\label{sec:overview}
\subsection{The ideas of Monte-Carlo and Multi-Level Monte-Carlo methods}\label{sec:mlmcideasoverview}
Throughout this \lcnamecref{sec:mlmcideasoverview} we assume our goal is to compute an approximation of $\EXP{Q},$ where $Q:\Omega\rightarrow\RR$ is a random variable. We assume we have access to a family of random variables $\Qh:\Omega\rightarrow\RR$, indexed by $h>0,$ where $\Qh$ approximates $Q$ in a sense made precise in \cref{def:probdataacc} below. We assume we can compute samples of $\Qh$, for any $h>0$. When we consider quantities of interest corresponding to the solution of a stochastic PDE, $Q$ will be a function of the true solution $u,$ and $\Qh$ will be a function of the finite-element approximation $\uh$ of $u.$ However, to explain the ideas behind Monte-Carlo and Multi-Level Monte-Carlo methods we will only occasionally need to mention $u$ and $\uh$. Therefore, for most of this chapter we will instead work with $Q$ and $\Qh.$ Our exposition throughout this \lcnamecref{chap:mlmc} is based on that of Cliffe, Giles, Scheichl, and Teckentrup \cite{ClGiScTe:11}, who proved the first results for Multi-Level Monte-Carlo methods for elliptic PDEs.

\subsubsection{Monte-Carlo Estimators}

The \defn{Monte-Carlo estimator $\QhatMC$} of $Q$ is the simplest possible estimator of $\EXP{Q}.$ The estimator is given by
\beqs
\QhatMC \de \frac1{\NMC} \sum_{j=1}^{\NMC} \Qh\mleft(\omegasj\mright),
\eeqs
where the $\omegasj$ are independent and identically distributed samples from the probability space $\Omega$.

One would expect that reducing $h$ and increasing $\NMC$ would give a more accurate approximation of $\EXP{Q}.$ Therefore our analysis of $\QhatMC$ seeks to answer the question `How should we choose $h$ and $\NMC$ to ensure the error is less than a prescribed tolerance $\eps>0$ (with minimal computational work)?' The standard relationship between $\eps$ and $\NMC$ is that one should take $\NMC \sim \eps^{-2}$, see, e.g., \cite[Text after equation (3)]{ClGiScTe:11}. We prove a generalised version of this relationship in \cref{thm:hhmc} below.

\subsubsection{Multi-Level Monte-Carlo Estimators}

In contrast to the Monte-Carlo estimator, where all of the approximations $\Qh(\omegasj)$ are performed for a single specified mesh size\footnote{For technical reasons due to the randomness of the coefficients, some of these meshes may be refined on a sample-by-sample basis, see \cref{sec:mlmcsetup} below. We ignore this technicality in the current discussion, but it will be fully addressed in \cref{sec:mlmcsetup}.} $h$, the Multi-Level Monte-Carlo estimator computes approximations for a hierarchy of mesh sizes $\hz \geq \ho \geq \cdots \geq \hL.$ The rationale for this computation is the observation that the telescoping sum identity
\beq\label{eq:mlmctelescope}
\EXP{\QhL} = \EXP{\Qhz} + \sum_{l=1}^L \EXP{\Qhl-\Qhlmo}
\eeq
holds and therefore, if one computes estimators $\Yhatz$ for $\EXP{\Qhz}$ and $\Yhatl$ for $\EXP{\Qhl - \Qhlmo}$, then one can construct an estimator for $\EXP{\QhL},$ 
\beqs
\QhatMLhL \de \Yzhat + \sum_{l=1}^L \Ylhat.
\eeqs
In this \lcnamecref{chap:mlmc}, the estimators $\Ylhat$ will be Monte-Carlo estimators using $\Nz$ samples of $\Qhz$ (for $\Yhatz$) and $\Nl$ samples of $\Qhl-\Qhlmo$ (for $\Yhatl,$ $l \geq 1$).

Our analysis of $\QhatMLhL$ then seeks to answer the question `How should we choose $\hL$ and $\Nz,\No,\ldots,\NL$ to ensure the error is less than a prescribed tolerance $\eps>0$ (with minimal computational work)?' The answer is long, and so we answer this question fully in our main, new result, \cref{thm:mlmccomp} below.

The reason one expects the Multi-Level Monte-Carlo estimator to require less computational effort than the Monte-Carlo estimator is that one expects the variance $\VAR{\Qhl-\Qhlmo}$ to decrease as $l$ increases. One expects this decrease because the quantities of interest $\Qhl$ and $\Qhlmo$ are obtained from finite-element approximations $\uhl$ and $\uhlmo$, and one expects these approximations to get closer together as $l$ increases. A basic calculation confirms this; indeed, provided the solution $u$ is sufficiently smooth, and $\hl \sim \hlmo$ uniformly in $l,$ (e.g., we obtain mesh $l$ by uniform refinement of mesh $l-1,$) then
\beqs
\NHo{\uhl - \uhlmo} \leq \NHo{\uhl - u} + \NHo{u-\uhlmo} \lesssim \hl + \hlmo \sim \hlmo \rightarrow 0 \text{ as } l \rightarrow L.
\eeqs
Therefore $\uhl$ and $\uhlmo$ get closer together as $l$ increases, and one expects analogous behaviour for $\Qhl$ and $\Qhlmo$ (if $Q$ is a continuous function of $u$, this behaviour is immediate). Since one takes the number of samples in a Monte-Carlo estimator to be proportional to the variance of the sampled quantity (i.e., $\Qhz$ or $\Qhl-\Qhlmo$ in this case), the fact that $\VAR{\Qhl-\Qhlmo}$ gets smaller as $l$ increases should mean the number of samples of $\Qhl-\Qhlmo$ can decreases as $l$ increases. As the computational cost of performing numerical solves is higher for finer meshes (i.e., the cost of computing $\Qhl-\Qhlmo$ increases as $l$ increases), we expect that the Multi-Level Monte-Carlo estimator allows us to perform a large number of (cheap) solves on the coarser meshes, and a small number of (expensive) solves on the fine meshes, i.e. $\Nz \geq \No \geq \cdots \geq \NL$. Replacing solves on finer meshes with solves on coarser meshes in this way should result in computational savings, as is seen for the stationary diffusion equation in \cite{ClGiScTe:11}.

\subsection{Challenges in Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation}\label{sec:mlmcchallenges}

Analysing Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation has two main challenges that are not present in the analysis of these methods for, e.g., the stationary diffusion equation.

Firstly, the behaviour of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation will be $k$-dependent, because the behaviour of the finite-element method for the Helmholtz equation is $k$-dependent, see \cref{sec:helmfe}. Because of this $k$-dependent behaviour, we would like our analysis of these methods to be completely $k$-explicit. In particular, since we have access to $k$-explicit finite-element-error estimates for the Helmholtz equation in \cref{sec:fem} above, we are able to make our analysis of Monte-Carlo and Multi-Level Monte Carlo methods $k$-explicit; we can re-prove the standard results on computational complexity for Monte-Carlo and Multi-Level Monte-Carlo methods with the $k$-dependence incorporated explicitly.

Secondly, the finite-element approximation $\uh$ of the solution $u$ of the stochastic Helmholtz equation may not exist for all $h > 0,$ and the criteria to prove its existence and uniqueness may be dependent on the coefficients $A$ and $n.$ I.e., for fixed $h$, $\uh(\omegaso)$ may exist and be unique, but $\uh(\omegast)$ may not, for some $\omegaso\neq\omegast\in\Omega$. To see why this is the case, recall from the definitions of $\hk{a}{b}$-accuracy and -data-accuracy for the finite-element solution of the Helmholtz equation (\cref{def:hkacc,def:hkdataacc}) that the finite-element approximation $\uh$ only exists for $h$ sufficiently small (with the definitions of $\hk{a}{b}$-accuracy and -data-accuracy defining `sufficiently small' in terms of $k$ and other quantities). Moreover, the criteria for `sufficiently small' also depend on the coefficients $A$ and $n$ (see \cref{rem:accuracyhetero}). Therefore, when $A$ and $n$ are stochastic, the existence and uniqueness of $\uh(\omega)$ is not only $h$-dependent but also $\omega$-dependent. Putting the above challenge into the language of the random variables $\Qh,$ the random variable $\Qh$ may not exist or be unique for all $h>0,$ and moreover, its existence and uniqueness may be sample-dependent. I.e., $\Qh(\omegaso)$ may exist and be unique, but $\Qh(\omegast)$ may not, for $\omegaso\neq\omegast.$

This sample-dependence poses an issue for Monte-Carlo and Multi-Level Monte-Carlo methods. The method may require us to compute $\Qh(\omegasj)$, but there is no guarantee that $\Qh(\omegasj)$ exists. Therefore, we need to modify our methods to deal with this sample-dependence. Such a modification to Monte-Carlo and Multi-Level Monte-Carlo methods for sample-dependent existence and uniqueness criteria was given by Graham, Parkinson, and Scheichl in \cite{GrPaSc:19} (and in Parkinson's PhD thesis \cite{Pa:18}), in the context of the Radiative Transport Equation (RTE). The RTE is an integro-differential equation whose numerical approximations have similar sample-dependent existence and uniqueness criteria to the Helmholtz equation. We adopt their approach for dealing with the sample-dependence, this approach is discussed in \cref{sec:mlmcsetup} below.

\subsection{Literature Review of Multi-Level Monte-Carlo methods}
We focus our literature review on (i) foundational works in Multi-Level Monte-Carlo methods, to provide a little context for our work on the Helmholtz equation, and (ii) applications of Multi-Level Monte-Carlo methods to problems sharing the challenges outlined in \cref{sec:mlmcchallenges} above. As far as we are aware, there is no prior work on Multi-Level Monte-Carlo methods explicitly incorporating the dependence on an additional parameter, and so we just mention works dealing with sample-dependent criteria for the numerical approximation. For a wider-ranging overview of the literature, we refer the reader to the review article \cite{Gi:15} and the webpage \cite{Gi}, the latter of which is kept up-to-date with a range of recent work on Multi-Level Monte-Carlo methods.

Multi-level Monte Carlo methods for stochastic differential equations were first introduced by Giles \cite{Gi:08} for time-dependent SDEs, with applications mostly arising in finance, although the ideas were present in earlier work by Heinrich \cite{He:98,He:01} on multilevel methods for parametric integration. Multi-Level Monte-Carlo methods were first applied to elliptic (i.e., non-time-dependent) PDEs by Barth, Schwab, and Zollinger in \cite{BaScZo:11} and Cliffe, Giles, Scheichl, and Teckentrup in \cite{ClGiScTe:11} for the stationary diffusion equation with an application in porous media flow. In particular, the statement of the Multi-Level Monte-Carlo complexity theorem in \cite[Theorem 1]{ClGiScTe:11} is the basis for our statement of a Multi-Level Monte-Carlo complexity theorem for the Helmholtz equation in \cref{thm:mlmccomp} below. We highlight that a key result of \cite[Theorem 1]{ClGiScTe:11} is that Multi-Level Monte-Carlo methods \emph{always} outperform Monte-Carlo methods, at least in the setting given in \cite{ClGiScTe:11}.

We mention the work of Scarabosio \cite{Sc:17}, who applied the Multi-Level Monte-Carlo method to a Helmholtz transmission problems with an uncertain boundary (i.e. a Helmholtz problem where $A$ and $n$ are piecewise constant, both jumping across a random interface). Her particular emphasis was on quantities of interest given by point evaluations of the solution; other UQ algorithms do not behave well for such QoIs, see \cite[Section 3.3]{Sc:17}. She works under the assumption that $k$ is small, i.e., a `large wavelength assumption' \cite[Assumption 3.1]{Sc:17}.  In this setting she shows that the Multi-Level Monte-Carlo method for the transmission problem fits into the standard framework of Multi-Level Monte-Carlo methods \cite[Proposition 4.2]{Sc:17} (as in, e.g., \cite{ClGiScTe:11,Gi:15}) and that the numerical behaviour of the method is as predicted by the theory \cite[Section 6]{Sc:17}.

We now highlight two bodies of work on Multi-Level Monte-Carlo methods with sample-dependent criteria; the work of Mishra, Schwab, and \v{S}ukys on Monte-Carlo and Multi-Level Monte-Carlo methods for time-domain wave propagation and the work of Graham, Parkinson, and Scheichl on Monte-Carlo and Multi-Level Monte-Carlo methods for the Radiative Transport Equation.

The work of Mishra, Schwab, and \v{S}ukys covers Monte-Carlo and Multi-Level Monte-Carlo methods for a range of linear and nonlinear hyperbolic problems, see, e.g., \cite{Su:14}. However, we focus just on their results for linear problems, as then the PDE involved is the time-domain wave equation with random coefficients and random initial data, whose Fourier transform in time is the Helmholtz equation (recall the discussion in \cref{sec:motivapp}). This work on linear wave propagation is contained in the papers \cite{SuMiSc:13,MiScSu:16} and in \v{S}ukys' PhD thesis \cite{Su:14}. They discretised the individual realisations of the wave problems using a finite-volume method in space and specialised time-stepping algorithms in time (see, e.g., \cite[Section 3.1]{MiScSu:16}). Because the PDEs in these works have random coefficients, the CFL condition for the numerical method (this condition depends on the coefficients) is also random, meaning the number of time steps used in the time-stepping algorithm is random. (The spatial discretisation is fixed across all realisations.) In \cite{SuMiSc:13} the authors analyse the error against the expected work (analagous to our analysis in \cref{sec:mc,sec:mlmcan} below). In \cite{MiScSu:16} the authors present more realistic test cases, and a load-balancing algorithm for applying the Multi-Level Monte-Carlo method on high-performance computers. The load-balancing algorithm is needed because the different individual solves have different computational requirements, because of the random number of timesteps mentioned above. They see that the Multi-Level Monte-Carlo method consistently outperforms the Monte-Carlo method.

Another collection of relevant work is that of Graham, Scheichl, and Parkinson \cite{GrPaSc:18,Pa:18,GrPaSc:19} on UQ methods (including Multi-Level Monte-Carlo methods) for the Radiative Transport Equation (RTE), as mentioned above. The main relevance of this work for our study of the Helmholtz equation is that, as mentioned above, proving the numerical approximation of the solution of the RTE exists and is unique requires a coefficient-dependent discretisation condition (see \cite[Theorem 4.12]{GrPaSc:19}). (This condition is analogous to a mesh constraint, except the RTE is not discretised with a traditional mesh, as it is defined on both spatial and angular variables.) When this discretisation constraint is carried over into a UQ setting, the RTE has a sample-dependent discretisation condition. Therefore, for some samples a given discretisation may be too coarse to guarantee existence and uniqueness. This sample-dependence is very similar to the situation we encounter for the Helmholtz equation, where the condition to ensure data-accuracy is $A$- and $n$-dependent (see \cref{cor:dataacc} above), and therefore there will be a sample-dependent condition in the UQ setting.

The remedy proposed for this sample-dependence by Graham, Parkinson, and Scheichl is to \emph{selectively} refine the discretisation \emph{only} for those samples that require a finer discretisation. We adopt this strategy for the Helmholtz equation, as outlined in \cref{sec:mlmcsetup} below. Moreover, Graham, Parkinson, and Scheichl show that (under suitable assumptions on the randomness, that are satisfied for a range of realistic random field models) this sample-wise refinement does not affect the asymptotics of the expected cost of the algorithm, because it only needs to happen on a set of samples of small measure, see \cite[Lemma 5.8]{GrPaSc:19}. We obtain similar results for the Helmholtz equation in \cref{lem:c} below.

%% In this \lcnamecref{sec:comp} we state and prove an abstract result on the convergence of multi-level Monte Carlo methods, laregly following the proof of \cite[Theorem 1]{ClGiScTe:11}. Our result is a generalisation of \cite[Theorem 1]{ClGiScTe:11} in the following three ways:
%% \ben
%% \item In \cite{ClGiScTe:11} it is assumed that the convergence of the approximate QoIs $\Qhl$, and the cost of producing samples of these QoIs, only depends on the parameter $\hl$ (where, in stochastic PDE applications, $\hl$ is the mesh size for the finite-element discretisation). However, in this work, we assume that the convergence and cost also depend on another parameter $k,$ and we make the dependence of the final computational cost of the MLMC method explicit in $k.$ In our application to the Helmholtz equation, $k$ will be the wavenumber of the problem.
%% \item In \cite{ClGiScTe:11} it is assumed that the approximating QoIS $\Qhl$ exist for all levels $l$. This corresponds to the finite-element solution of the PDE under investigation existing for all mesh sizes $h.$ Whilst this assumption is true for the stationary diffusion equation studied in \cite{ClGiScTe:11}, it is \emph{not} true for the Helmholtz equation that we study here. Therefore we make the additional assumption (\cref{ass:qoie} below) that $\Qhl$ only exists for sufficiently small $\hl.$
%% \item In \cite{ClGiScTe:11} the error $\eps$ incurred in the MLMC method is equally divided between the bias and the variance of the MLMC method (see the Proof of \cref{thm:mlmccomp3}). However, in this work we assume that there is a quantity $\splitting \in (0,1)$ (see \cref{ass:splittingbounds}), possibly dependent on $k$ that allows a vairable `split' of the error between the bias and the variance. Our main use of this is in\opstodo{Insert refs once it's done}, where we use this variable splitting to compensate for the fact that to bound the (squared) bias error by $\eps^2/2$ would mean we take $\hL \lesssim k^{-1},$ but to ensure the finite-element solution $\uh$ exists, we must take $\hL \lesssim k^{-3/2}.$
%% \een
%% We now proceed to prove our abstract MLMC convergence result, comtaining the generalisations metioned above.

\section[Abstract MC and MLMC setting]{An abstract setting for both the\\Multi-Level Monte-Carlo and Monte-Carlo\\methods, motivated by the Helmholtz equation}\label{sec:mlmcsetup}
We now define the concepts and quantities needed to define and discuss Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation. However, at this stage we work at an abstract level, i.e., we consider random variables $Q$ and $\Qh$ rather than the solution $u$ of a stochastic Helmholtz equation and its approximations $\uh.$ This abstraction will help simplify the presentation of the additional challenges one has for the Helmholtz equation. However, when constructing this abstract setting, our definitions will be motivated by properties of the finite-element solution of the Helmholtz equation. Therefore in \cref{sec:mlmcapplying} below, we will show that the Helmholtz equation fits into our abstract setting, and therefore our abstract results are applicable to the Helmholtz equation itself.

We assume that we have a parameter $k>0$ (corresponding to the wavenumber $k$ in the Helmholtz equation), that there exists a random variable $Q:\Omega\rightarrow\RR$ depending on $k$, and that our goal is to approximate $\EXP{Q}.$ Our first aim would be to define a family of random variables $\mleft(\Qh\mright)_{h>0}$ (corresponding to the finite-element approximations $\uh$). However, as has been discussed in \cref{sec:mlmcchallenges}, the existence and uniqueness of finite-element approximations of the Helmholtz equation is sample-dependent, and therefore we want our abstract setting to reflect this dependence.

If we recall our finite-element-error bound in \cref{thm:fembound} above (from which we concluded the $h$-finite-element method for the heterogeneous Helmholtz equation is $\hka{(2p+1)/2p}$-data-accurate in \cref{cor:dataacc}), then we see that the existence and uniqueness criteria and the error bound are $h$-, $k$-, $A$-, and $n$-dependent. (For simplicity, in this \lcnamecref{chap:mlmc} we assume $\CAnk \sim 1,$ i.e., the Helmholtz problem is nontrapping almost surely, although one could easily generalise the results of this \lcnamecref{chap:mlmc} to the trapping case, albeit with a worse $k$-dependence.) Therefore, when we move to the UQ case, where $A$ and $n$ are random fields, the existence and uniqueness criterion will be $h$-, $k$-, and sample-dependent. Motivated by this dependence, we make the following assumption on the existence and uniqueness of the random variable $\Qh$.

\bas[Probabilistic version of \cref{thm:fembound}]\label{def:probdataacc}
There exist random variables $\Co$ and $\cotilde$, with $\EXP{\cotilde} < \infty$, and constants $a, \alpha, \sigma >0$ all independent of $h$ and $k$ such that, for $h>0$ if
\beq\label{eq:probdataacc}
h < \Co(\omega)k^{-a},
\eeq
then $\Qh(\omega)$ exists, is unique, and satisfies
\beq\label{eq:Qhbound}
\abs{Q(\omega)-\Qh(\omega)} \leq \cotilde(\omega) h^\alpha k^\sigma.
\eeq
\eas

\bre[Comments on \cref{def:probdataacc}]

\

\bit
\item As an example, \cref{thm:fembound} shows \cref{def:probdataacc} holds for the stochastic Helmholtz equation with $Q(\cdot)=\NHokD{\cdot}$, $a = (2p+1)/2p,$ $\alpha=2p,$ and $\sigma = 2p+1$, where we have used the fact that as discussed in \cref{rem:higherorder}, the final terms in the bounds \cref{eq:femltbound,eq:femhobound} are the dominant terms.
  
\item In principle, one can obtain explicit formulae for $\Co$ and $\cotilde$ from \cref{eq:hfemcond,eq:femltbound,eq:femhobound}. However, as noted in \cref{rem:nsharp,rem:explicita}, \cref{eq:hfemcond,eq:femltbound,eq:femhobound} may not depend optimally on $n$, and are not completely explicit in their $A$-dependence. Therefore, using \cref{eq:hfemcond,eq:femltbound,eq:femhobound} to define $\Co$ and $\cotilde$ would mean $\Co$ and $\cotilde$ may not depend optimally on $\omega,$ nor be completely explicit in their $\omega$-dependence. Therefore we do not specify (here, or in \cref{sec:mlmcapplying} below) the form of $\Co$ or $\cotilde.$


\eit
\ere

A crucial consequence of \cref{def:probdataacc} is that, as stated above, for a given $h>0$ the value $\Qh(\omega)$ may not be defined for all $\omega \in \Omega.$ To cope with this issue, we follow the approach of Graham, Parkinson, and Scheichl in \cite{GrPaSc:19}. For a fixed $h>0,$ we define the set $\Omegabad = \set{\omega \in \Omega \st \text{\cref{eq:probdataacc} is not satisfied}}.$ On $\Omegabad$ we refine the mesh on a sample-by-sample basis so that \cref{eq:probdataacc} \emph{is} satisfied on the refined mesh. We then show that this additional refinement does not change the $h$-dependence of the expected cost of a single sample. (The proof of this fact requires the assumption that $\Omegabad$ has small probability; this assumption is stated more formally in \cref{ass:omegabad} below, and is proved by Graham, Scheichl, and Parkinson in the neutron-transport context in \cite[Lemma 5.3]{GrPaSc:19}.)

We now give the above scheme more precisely. For fixed $h>0$, and given $\omega \in \Omega,$ we define
\beq\label{eq:hmaxomega}
\hmaxomega = \Co(\omega)k^{-a},
\eeq
that is, $\hmaxomega$ is the largest mesh size that satisfies \cref{eq:probdataacc}. We then define
\beq\label{eq:homega}
\homega = \min\set{h,\hmaxomega},
\eeq
that is, the behaviour of $\homega$ as $h\downarrow 0$ is governed by $h$, but $\homega$ is always small enough so that it satisfies \cref{eq:probdataacc}. We can now define the quantity
\beq\label{eq:Qhtilde}
\Qhtilde(\omega) = \Qhomega(\omega).
\eeq
Observe that, by construction, $\Qhtilde(\omega)$ exists for all $\omega \in \Omega,$ because if $\omega \in \Omegabad,$ then $\homega = \hmaxomega$ and by definition of $\hmaxomega,$ the value $\Qhmaxomega(\omega)$ exists.

\bre[Is $\Qhtilde$ a random variable?]\label{rem:Qhtilderandom}
Throughout this \lcnamecref{chap:mlmc}, we assume $\Qhtilde$ is a random variable. One could, in principle, prove this fact, but the proof would likely be very involved. One would need to show the map $(\omega,h) \mapsto \Qh(\omega)$ is measurable (for all pairs $(\omega,h)$ such that this map is defined) with respect to a suitable $\sigma$-algebra, and then combine this fact with the fact that $\homega$ is a random variable (and thus measurable) to conclude that the map $\omega \mapsto \Qhtilde$ is measurable. Proving that the map $(\omega,h) \mapsto \Qh(\omega)$ is measurable in the context of finite-element discretisations of the Helmholtz equation would be very technical, and would contribute little to the discussion of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation. Therefore, we instead assume $\Qhtilde$ is a random variable.
\ere

Because $\Qhtilde$ is associated with a random mesh size $\homega,$ the cost of computing one realisation of $\Qhtilde$ will also be a random variable. Therefore, we make the following \lcnamecref{ass:costone} on the cost of computing one realisation of $\Qhtilde.$ In particular, we assume that the cost is driven by the \emph{actual} mesh size that is used in the computations, $\homega.$ We let $\Cost{\cdot}$ denote the cost of computing one realisation of a random variable.

\bas[Cost of one realisation of $\Qhtilde$]\label{ass:costone}
There exists $ \gamma > 0$ and a positive random variable $\cthtilde$, where $\cthtilde$ does not depend on $h$ and $k$, such that
\beqs
\Cost{\Qhtilde(\omega)} \leq \cthtilde(\omega) \homega^{-\gamma},
\eeqs
\eas

We can now show that, provided the set $\Omegabad$ has small probability (in a sense made precise in \cref{ass:omegabad} below), the expected cost of computing one realisation of $\Qhtilde$ is driven only by $h.$ I.e., the expectation does not `see' the additional refinement needed for $\omega \in \Omegabad$, because these samples occur with low probability.

\bas\label{ass:omegabad}
The quantity
\beq\label{eq:cass}
\cth \de \EXP{\cthtilde\mleft(1+\Co^{-\gamma}\mright)}
\eeq
is finite.
\eas

One can conclude from \cref{ass:omegabad} that the set $\Omegabad$ has low probability, as in \cite[Text at the bottom of p. 21]{GrPaSc:19}. Observe that $\Co$ governs where the mesh needs to be refined (since if $\Co(\omega)$ is small, then a smaller mesh size is needed). Therefore if terms involving $\Co^{-1}$ have finite expectation (as in \cref{eq:cass}), then $\Co$ is small with low probability, i.e., $\Omegabad$ has low probability.

\ble[Expected cost of one sample of $\Qhtilde$]\label{lem:c}
If \cref{ass:omegabad,ass:costone} hold, then 
\beq\label{eq:singlecost}
\EXP{\Cost{\Qhtilde}} \leq \cth \mleft(h^{-\gamma}+k^{a\gamma}\mright).
\eeq
\ele

\bpf[Proof of \cref{lem:c}]
The proof follows closely that in \cite[Lemma 5.8]{GrPaSc:19}.
We have
\beq\label{eq:costpf1}
\Cost{\Qhtilde(\omega)} \leq \cthtilde(\omega)\homega^{-\gamma} \leq \cthtilde(\omega) \mleft(h^{-\gamma} + \mleft(\hmaxomega\mright)^{-\gamma}\mright)
\eeq
by \cref{ass:costone,eq:homega}. Then using \cref{eq:hmaxomega} and \cref{eq:costpf1} we obtain the bound
\beq\label{eq:costpf2}
\Cost{\Qhtilde(\omega)} \leq \cthtilde(\omega)h^{-\gamma} + \mleft(\cthtilde\Co^{-\gamma}\mright)(\omega) k^{a\gamma},
\eeq
and therefore since \cref{ass:omegabad} holds, we obtain \cref{eq:singlecost}.
\epf

To prove results on the expected computational cost and convergence of Monte-Carlo and Multi-Level Monte-Carlo methods, we need not only the previous \lcnamecref{lem:c} on the expected computational cost of a single sample of $\Qhtilde,$ but also the following \lcnamecref{ass:a} on the convergence of $\Qhtilde$ to $Q$, analagous to \cite[Theorem 5.14]{GrPaSc:19}.

\ble[Convergence of $\Qhtilde$ to $Q$]\label{ass:a}
Under \cref{def:probdataacc}
\beqs
\EXP{\abs{\Qhtilde-Q}} \leq \co k^\sigma h^{\alpha},
\eeqs
where $\co = \EXP{\cotilde}.$
\ele

\bpf[Proof of \cref{ass:a}]
The proof is immediate from \cref{eq:Qhtilde}, \cref{def:probdataacc} and the fact that $\homega \leq h$ (by \cref{eq:homega}).
\epf

Before we move on to study Monte-Carlo and Multi-Level Monte-Carlo methods, we define the notion of error that we use when studying these methods.

\bde[Root-mean-squared error]\label{def:rmse}
Given a random variable $Q$ and an estimator $\Qhat$ of Q, the \defn{root-mean-squared error} of $\Qhat$ is
\beqs
\err{\Qhat} \de \mleft(\EXP{\abs{\Qhat-Q}^2}\mright)^{\half}.
\eeqs
\ede

\section{Monte-Carlo methods}\label{sec:mc}

We now prove a $k$-explicit bound on the expected computational complexity of the Monte-Carlo method in the above abstract setting (which is, of course, motivated by the stochastic Helmholtz equation). Recall that the Monte-Carlo estimator of $Q$ is defined by
\beqs
\QhatMC = \frac1{\NMC} \sum_{j=1}^{\NMC} \Qhtildesj,
\eeqs
where the $\Qhtildesj$ are independently and identically distributed samples of $\Qhtilde.$

We have the following \lcnamecref{thm:hhmc} on the computational complexity of the Monte-Carlo estimator $\QhatMC$, which is a generalisation of the standard proof of the complexity of the Monte-Carlo method (see, e.g., \cite[Section 2.1]{ClGiScTe:11}) to the above $k$-dependent setting. In this \lcnamecref{thm:hhmc}, the notation $\sim$ denotes a hidden constant that is independent of $h,$ $k,$ and $\eps.$

\bth[Computational complexity of Monte-Carlo]\label{thm:hhmc}
Let \cref{ass:costone,ass:omegabad,def:probdataacc} hold. Given $\eps \in (0,1),$ if
\beq\label{eq:hMC}
h \sim \mleft(\sqrt{2}\co\mright)^{-\frac1{\alpha}}k^{-\frac\sigma\alpha}\eps^{\frac1{\alpha}},
\eeq
and
\beq\label{eq:NMC}
\NMC  \sim 2\VAR{\Qhtilde}\eps^{-2}
\eeq
then
\beq\label{eq:mcerror}
\err{\QhatMC} \sim \eps
\eeq
and the computational complexity of $\QhatMC$ satisfies
\beq\label{eq:mccost}
\EXP{\CMC} \sim \VAR{\Qhtilde}\mleft(\eps^{-2-\frac{\gamma}{\alpha}}k^{\frac{\gamma\sigma}\alpha} + \eps^{-2}k^{a\gamma}\mright).
\eeq
\enth

The first term in \cref{eq:mccost} is analgous to the standard cost term one obtains in the analysis of Monte-Carlo methods (see, e.g., \cite[Section 2.1]{ClGiScTe:11}). The second term in \cref{eq:mccost} arises from the $k$-dependence of $\cref{eq:hmaxomega}$. The reason the second term in \cref{eq:mccost} has a better $\eps$-dependence than the first term is that \cref{eq:probdataacc} is an $\eps$-independent criterion, whereas ensuring the error is small (via the mesh constraint \cref{eq:hMC}) is an $\eps$-dependent criterion.

\bpf[Proof of \cref{thm:hhmc}]
The proof is almost identical to the standard proof for Monte-Carlo methods, see, e.g., \cite[Section 2.1]{ClGiScTe:11}. We can first perform a so-called bias--variance decomposition of the error
\begin{align}
\err{\QhatMC}^2 &=  \EXP{ \abs{\EXP{Q}-\EXP{\QhatMC} + \EXP{\QhatMC} - \QhatMC}^2} \nonumber\\
&= \abs{\EXP{Q}-\EXP{\QhatMC}}^2 + \EXP{\abs{\EXP{\QhatMC} - \QhatMC}^2} \nonumber\\
&= \abs{\EXP{Q} - \EXP{\QhatMC}}^2+\VAR{\QhatMC},\label{eq:mccomp1}
\end{align}
where the second line follows from first due to the fact that $\EXP{\QhatMC - \EXP{\QhatMC}} = 0$, and the third line follows from the second by the definition of the variance. The first term in \cref{eq:mccomp1} is the `bias' (i.e., the error introduced by the discretisation), and the second term in \cref{eq:mccomp1} is the variance of the estimator $\QhatMC.$

By definition of $\QhatMC$, and the fact that the samples $\Qhtildesj$ are independent, we have
\beq\label{eq:mccomp2}
\VAR{\QhatMC} = \frac1{\NMC^2}\sum_{j=1}^{\NMC}\VAR{\Qhtildesj} = \frac1{\NMC} \VAR{\Qhtilde}.
\eeq
Therefore we can conclude from \cref{eq:mccomp1,eq:mccomp2} that the root-mean-squared-error satisfies
\beq\label{eq:mccomp3}
\err{\QhatMC}^2 = \abs{\EXP{\Qhtilde-Q}}^2 + \frac1{\NMC}\VAR{\Qhtilde}.
\eeq
By \cref{eq:hMC,ass:a} the first term in \cref{eq:mccomp3} is proportional to $\eps^2/2$, and by \cref{eq:NMC} the second term in \cref{eq:mccomp3} is proportional to $\eps^2/2$, and therefore \cref{eq:mcerror} holds. All that remains is to estimate the (expected) computational complexity. We have
\begin{align*}
  \EXP{\Cost{\QhatMC}} &= \NMC \EXP{\Cost{\Qhtilde}}\\
  &\leq \NMC \cth \mleft(h^{-\gamma} + k^{a\gamma}\mright) \text{ by \cref{lem:c},}\\
  &\sim 2\VAR{\Qhtilde}\eps^{-2} \mleft(\cth \mleft(\sqrt{2} \co\mright)^{\frac\gamma\alpha}k^{\frac{\gamma\sigma}{\alpha}}\eps^{-\frac\gamma\alpha} + k^{a\gamma}\mright)\text{ by \cref{eq:NMC,eq:hMC}}
\end{align*}
as required.
\epf

\section{Multi-level Monte-Carlo methods}\label{sec:mlmcan}
We now analyse the Multi-Level Monte-Carlo method in the $k$-dependent abstract setting given above. Aside from the $k$-dependence and the sample-dependent existence and uniqueness criterion (the latter of which has been discussed and dealt with through introducing the random variables $\Qhtilde$ above), our approach and final result is analogous to the standard Multi-Level Monte-Carlo complexity result given in, e.g., \cite[Theorem 1]{ClGiScTe:11}. Recall that the goal is to choose the number of levels $L$ and the numbers of samples on each level $\Nl$ to acheive a root-mean-squared error of at most $\eps$ with minimal cost. Our main result, showing how to achieve this goal, is \cref{thm:mlmccomp} below. We now give precise details of the setup for Multi-Level Monte-Carlo.

We define a set of levels $\set{\hl}_{l=0}^L$ (with $L$ to be chosen) such that
\beq\label{eq:hl}
\hl =\frac{\hlmo}s
\eeq
for $s > 1$ and $l \geq 1$. In particular
\beq\label{eq:hL}
\hL = s^{-L} \hz.
\eeq
(Observe that when $\hl$ corresponds to the mesh width of a finite-element mesh, then \cref{eq:hl} is achieved if we obtain successive meshes by uniform refinement.) We then define the correction operators between the levels by
\beq\label{eq:Yldef}
\Yl \de \Qhltilde - \Qhlmotilde, l \geq 1,\quad \Yz = \Qhztilde.
\eeq
Observe that by construction
\beq\label{eq:expectationtelescope}
\EXP{\Yz} + \sum_{l=1}^L \EXP{\Yl} = \EXP{\Qhztilde + \sum_{l=1}^L \Qhltilde - \Qhlmotilde} = \EXP{\QhLtilde}.
\eeq
We let $\Ylhat$ be the Monte-Carlo estimator of $\Yl$, i.e.,
 \beq\label{eq:Ylhatdef}
\Ylhat \de \frac1{\Nl}\sum_{j=1}^{\Nl} \Ylj,
 \eeq
 with $\Nl$ to be chosen, where the $\Ylj$ are independent samples of $\Yl$. Note that it follows that the estimators $\Ylhat$ are independent of each other. (To simplify the notation, we do not include $\Nl$ in the notation for $\Ylhat$.) Finally we define the \defn{multi-level Monte Carlo estimator} of $Q$
 \beqs
 \QhatMLhL \de \sum_{l=1}^L \Ylhat.
 \eeqs

 As we discussed in \cref{sec:mlmcideasoverview} above, the reason the Multi-Level Monte-Carlo method delivers a lower computational cost than the Monte-Carlo method is that the variance of the estimators $\Ylhat$ decreases as $l$ increases. Therefore the more expensive simulations (for higher $l$) need fewer samples. To quantify the behaviour of these variances, we assume $\VAR{\Yl}$ has the following property, c.f. the behaviour of the error in \cref{eq:Qhbound}. (The similarity in the form of \cref{eq:mlmcassb} below and \cref{eq:Qhbound} is no coincidence, one usually proves bounds of the form \cref{eq:mlmcassb} via bounds of the form \cref{eq:Qhbound}; see the proof of \cref{lem:mlho} below for an example of this proof technique.)

  %% The following assumptions
  %% % \lcnamecrefs{ass:coarse}
  %%  will form the backbone of our analysis. They are a generalisation of the assumptions contained in \cite{ClGiScTe:11,ChScTe:13} for the MLMC method, the generalisation being that we assume that the quantities below depend not only on the levels $\hl$ but also on some additional parameter $k>1.$ When this theory is applied to the Helmholtz equation, $k$ will be the wavenumber of the Helmholtz equation.

%% The following assumption (which will be realised in a more concrete setting for the Helmholtz equation) concerns the existence of the approximating QoIs $\Qhl.$

%% \bas[Existence of $\Qhl$]\label{ass:qoie}
%% There exist $\Ccoarse,\coarseexp > 0$ with $\Ccoarse$ independent of $k$ such that if
%% \beqs
%% \hl \leq \Ccoarse k^{-\coarseexp},
%% \eeqs
%% then the QoI $\Qhl$ exists.
%% \eas

\bas[Variance of correction operators]\label{ass:b}
There exist $\ct, \beta, \tau > 0$, such that $\ct$ is independent of $h$ and $k,$ and
\beq\label{eq:mlmcassb}
\Vl \de \VAR{\Yl} \leq \ct \hl^{\beta}k^\tau.
\eeq
\eas

As we will see in \cref{thm:mlmccomp} below, the interplay between $\beta$ and $\gamma$ (i.e., the interplay between the variances and the cost of computing a single sample) governs the behaviour of the cost of the Multi-Level Monte-Carlo method.

We make the following simplifying \lcnamecref{ass:coarse}, that the coarse mesh $\hz$ has the same $k$-dependence as the criterion for existence and uniqueness \cref{eq:probdataacc}.% If we did not make this \lcnamecref{ass:coarse}, then the relationship between $\hz$ and $\hzomega$ would be $k$-dependent, making the analysis of the Multi-Level Monte-Carlo method more involved. Moreover, not making this \lcnamecref{ass:coarse} would not give any additional computational gains, as the mesh on which we compute (with mesh size $\hzomega$) would be considerably finer than the specified mesh (with mesh size $\hz$).

\bas[Dependence of coarse space on $k$]\label{ass:coarse}
Let $\Ccoarse > 0$ be independent of $k$ and 
\beq\label{eq:coarse}
\hz = \Ccoarse k^{-a}.
\eeq
\eas

 
% We write $\Vl$ for $\VAR{\Yl}.$
 


%\input{nice}



%% \subsection{Lemma}

%% The proof of the main \lcnamecref{thm:mlmccomp} will require the following \lcnamecref{lem:sumbound}.

%% \ble\label{lem:sumbound}
%% If $L$ is given by

%% then, for $s>1$ and $\delta \in \RR,$ we have the bound
%% \beq\label{eq:sumbound}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L+1 & \tif \delta = 0,\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}s^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha} &\tif \delta >0\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}&\tif \delta < 0
%% \end{cases}
%% \eeq
%% \ele

%% \bpf[Proof of \cref{lem:sumbound}]
%% The proof follows that in \cite{ClGiScTe:11}. We first observe that, since $L$ is given by \eqref{eq:Ldef}, it follows that
%% \beq\label{eq:Lbounds}
%% \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) \leq L < \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) + 1.
%% \eeq
%% Rearranging \eqref{eq:Lbounds}, we obtain the bounds
%% \beq\label{eq:saLbounds}
%% \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1} \leq s^{\alpha L} < \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1}s^\alpha.
%% \eeq
%% If $\delta > 0,$ then we use the right-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLpos}
%% s^{\delta L} < \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}s^{\delta},
%% \eeq
%% and if $\delta < 0,$ we use the left-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLneg}
%% s^{\delta L} \leq \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}.
%% \eeq
%% We now observe that, for $\delta \neq 0,$
%% \begin{align}
%% \sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
%% &= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
%% &\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumbound}
%% \end{align}
%% since $s^{-\delta} > 0,$ as $s >0.$ Combining \eqref{eq:ssumbound} with \eqref{eq:sdLpos} and \eqref{eq:sdLneg}, we obtain \eqref{eq:sumbound} in the cases $\delta \neq 0.$ The case $\delta=0$ is straightforward.
%% \epf


%, and with some additional cases enumerated. %\Cref{thm:mlmccomp} contains more cases than in \cite[Theorem 1]{ClGiScTe:11} because \cite[Theorem 1]{ClGiScTe:11} makes the assumption throughout that $\alpha \geq 1/2\min\set{\beta,\gamma}.$ This assumption does not always hold for the Helmholtz equation (see the cases of a direct solver in 3-D below), however, examining the proof of \cite[Theorem 1]{ClGiScTe:11}  shows that in any given case, one only needs the assumption $\alpha \geq \beta/2$ or the assumption $\alpha \geq \gamma2$, never both at the same time. Therefore, for convenience, we explicitly state when these conditions are needed, and for completeness, we give the results when these conditions are violated. 

%% The following \lcnamecref{ass:constants3} will ensure that \cref{ass:qoie} is satisfied.

%%  \bas[$\eps$ sufficiently small]\label{ass:constants3}
%%  Assume
%%  \beqs
%% \eps \leq \sqrt{2} \co \Ccoarse^{\alpha} k^{\sigma-\coarseexp\alpha}.
%%  \eeqs
%%  \eas


%% \bas[Assumptions on $\eps$ and $k$ to simplify expressions in the case $\beta=\gamma$]\label{ass:epsk}
%% \beqs
%% \eps \leq \min\set{\frac{\sqrt{2}\co\Ccoarse^\alpha}{s^{2\alpha}},\frac1{\sqrt{2}\co\Ccoarse^\alpha}},
%% \eeqs
%% and
%% \beqs
%% k^{\sigma-a\alpha} \geq 1.
%% \eeqs
%% \eas

We can now state our main theorem on the complexity of the Multi-Level Monte-Carlo mthod in the $k$-dependent abstract setting above. In particular, we show
\bit
\item how the number of levels $L$ should be chosen, and
  \item how the number of samples $\Nl$ on each level should be chosen
    \eit
    so that the root-mean-squared error of the Multi-Level Monte-Carlo estimator is of the order $\eps$ with minimal work. Observe that \cref{thm:mlmccomp} is analogous to the standard Multi-Level Monte-Carlo complexity theorem, see, e.g., \cite[Theorem 1]{ClGiScTe:11}, but adapted for our $k$-dependent setting.

We let
    \beq\label{eq:Cldef}
\Cl \de \cth\mleft(\hl^{-\gamma} + k^{a\gamma}\mright),
\eeq
i.e., $\Cl$ is the bound on the expected cost of computing one sample of $\Qhltilde$ (see \cref{lem:c}).

\bth[Computational Complexity of Multi-Level Monte-Carlo]\label{thm:mlmccomp}
Under \cref{ass:costone,ass:omegabad,def:probdataacc,ass:coarse,ass:b}, if $L$ is given by
\beq\label{eq:Ldef}
L = \max\set{\ceil{\frac1\alpha\log_{s}\mleft(\sqrt{2}\co  \Ccoarse^\alpha k^{\sigma-a\alpha} \eps^{-1}\mright)},0},
\eeq
the number of samples on each computational level is given by
\beq\label{eq:Nl}
\Nl = \ceil{2\eps^{-2} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^{L} \mleft(\Vj\Costj\mright)^{\half}},
\eeq
$\eps < 1$, and
\beqs
\alpha \geq \half \min\set{\beta,\gamma},
\eeqs
then $\err{\QhatMLhL} \leq \eps$ and, if $L \geq 1$, the computational cost of $\QhatMLhL$ satisfies 
\beq
\label{eq:mlmchhbounds}
\EXP{\Cost{\QhatMLhL}} \lesssim
\begin{cases}
  \mleft(k^{\tau-a\mleft(\beta-\gamma\mright)} + k^{\frac{\gamma\sigma}{\alpha}}\mright)\eps^{-2} & \tif \beta > \gamma,\\
k^{\tau}\eps^{-2}\mleft(\mleft(\log_s\mleft(\eps^{-1}k^{\sigma-a\alpha}\mright)\mright)^2 +1\mright) + k^{\frac{\gamma\sigma}{\alpha}}\eps^{-2}  & \tif \beta = \gamma,\\ 
k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}} + k^{\frac{\gamma\sigma}{\alpha}}\eps^{-2} & \tif \gamma > \beta.
\end{cases}
\eeq
 However, if $L=0$, then $\Cost{\QhatMLhL}$ is given by \cref{thm:hhmc}.
 \enth
 The proof of \cref{thm:mlmccomp} is given on \cpageref{page:mlmccompproof} below. It is surprising that in \cref{eq:Ldef} we must take the maximum to ensure $L$ is non-negative; this requirement is due to a subtle point about the values of $a,$ $\alpha$, and $\sigma,$ see \cref{sec:mlmcmodel} below. The assumption that $\alpha \geq \min\set{\beta,\gamma}/2$ is standard in studies of Multi-Level Monte-Carlo methods, in order to simplify the expressions involving $\eps$ in (the equivalent results to) \cref{eq:mlmchhbounds}, see, e.g. \cite[Theorem 1]{ClGiScTe:11}.
 
 \bre[The finest mesh size in \cref{thm:mlmccomp}]
Observe that if the number of additional levels $L$ is given by \cref{eq:Ldef}, then one can simplify the dependence on $\Ccoarse$ and $a$ using \cref{eq:hL,eq:coarse} to obtain
\beq\label{eq:hLcond}
\hL = \min\set{\mleft(\frac\eps{\sqrt{2}\co k^{\sigma}}\mright)^{\frac1\alpha},\hz}.
\eeq
 \ere
 In the proof of \cref{thm:mlmccomp}, we will need to bound sums of the form $\sum_{l=0}^L s^{\delta l}$, where $L$ is given by \cref{eq:Ldef} and $\delta$ is some constant. Therefore, we first prove these bounds in the following \lcnamecref{lem:sumboundnew}, that contains an abstract version of \cref{eq:Ldef}, before proceeding to the proof of \cref{thm:mlmccomp}.
\ble[Bounds on sums occuring in the proof of \cref{thm:mlmccomp}]\label{lem:sumboundnew}
If $L$ is given by
\beq\label{eq:Ldefgen}
L = \ceil{\Lconst\log_{s}\mleft( \func \eps^{-1}\mright)},
\eeq
for some $\Lconst, \func > 0,$ then, for $s>1$ and $\delta \in \RR,$ we have the bounds
\beq\label{eq:sumboundgen}
\sum_{l=0}^{L} s^{\delta l} \leq
\begin{cases}
L+1 & \tif \delta = 0,\\
\frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0,\\
\frac{s^{-\delta}}{s^{-\delta}-1}&\tif \delta < 0.
\end{cases}
\eeq
%% \beq\label{eq:sumboundLmo}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L & \tif \delta = 0,\\
%% \frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0\\
%% \frac{1}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst}&\tif \delta < 0
%% \end{cases}
%% \eeq
%\opctodo{Tidy}
\ele

\bpf[Proof of \cref{lem:sumboundnew}]
 The case $\delta=0$ is immediate. For $\delta \neq 0$ the proof follows that in \cite[Appendix A]{ClGiScTe:11}. We first observe that, for $\delta > 0,$
\begin{align}
\sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
&= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
&\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumboundgen}
\end{align}
since $s^{\delta L} \geq  s^{-\delta}$.

Then, since $L$ is given by \eqref{eq:Ldefgen}, it follows that the bound
\beq\label{eq:Lboundsgen}
%\Lconst\log_s\mleft(\func \eps^{-1}\mright) \leq
L < \Lconst\log_s\mleft(\func \eps^{-1}\mright) + 1
\eeq
holds. Rearranging \eqref{eq:Lboundsgen}, we obtain the bound
\beq\label{eq:saLboundsgen}
%\mleft( \func\eps^{-1}\mright)^{\Lconst} \leq
s^{L} < \mleft( \func\eps^{-1}\mright)^{\Lconst}s.
\eeq
From \eqref{eq:saLboundsgen} we can then obtain
\beq\label{eq:sdLposgen}
s^{\delta L} < \func^{\delta\Lconst}\eps^{-\delta\Lconst}s^{\delta}.
\eeq
Combining \cref{eq:sdLposgen,eq:ssumboundgen}, we obtain \cref{eq:sumboundgen} in the case $\delta > 0.$

For the case $\delta < 0$, we observe
\begin{align*}
\sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
&= \frac{s^{-\delta} - s^{\delta L}}{s^{-\delta}-1}\nonumber\\
&\leq \frac{s^{-\delta}}{s^{-\delta}-1},%\label{eq:ssumboundgen2}
\end{align*}
since $s^{-\delta} \geq  s^{\delta L}$, that is, \cref{eq:sumboundgen} in the case $\delta < 0.$
\epf

We are now in a position to prove \cref{thm:mlmccomp}.

\bpf[Proof of \cref{thm:mlmccomp}]
\label{page:mlmccompproof}
Throughout the proof, we assume $L>0.$ In the case $L=0$, the Multi-Level Monte-Carlo estimator becomes the Monte-Carlo estimator, whose behaviour is given by \cref{thm:hhmc}.

We recall the bias--variance decomposition of the (squared) mean-squared error analagous to \cref{eq:mccomp1}
\beq\label{eq:mlmcdecomp}
\errQhatMLhL^2 = \abs{\EXP{\QhatMLhL - Q}}^2 + \VAR{\QhatMLhL},
\eeq
where the first term in \cref{eq:mlmcdecomp} is the bias and the second term is the variance.
We now proceed to choose the parameters $L$ and $\Nl,$ $l = 0,\ldots,L$, such that we can bound both the bias and the variance by $\eps^2/2,$ thereby making $\err{\QhatMLhL} \leq \eps.$

We first bound the bias. To do this, we only need to choose $L$ large enough, i.e., choose $\hL$ small enough. By the construction of the Multi-Level Monte-Carlo estimator $\QhatMLhL,$ it follows that $\EXP{\QhatMLhL} = \EXP{\QhLtilde},$ see \cref{eq:expectationtelescope}. Therefore the bias term in \cref{eq:mlmcdecomp} is equal to $\abs{\EXP{\QhLtilde - Q}}^2.$ By \cref{ass:a} with $h=\hL,$ a sufficient condition for the bias term to be at most $\eps^2/2$ is
\beq\label{eq:biascondition}
\co k^\sigma \hL^\alpha \leq \frac{\eps}{\sqrt{2}},
\eeq
which, when rearranged, gives the first term in \eqref{eq:hLcond}. As $\hL = \hz s^{-L},$ it follows from rearranging \cref{eq:biascondition} that a sufficient condition for the bias term to be $\leq \eps^2/2$ is
\beq\label{eq:Lcondpart}
L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co k^\sigma \hz^\alpha \eps^{-1}\mright)}.
\eeq
Under \cref{ass:coarse}, since $\hz = \Ccoarse k^{-a},$ we can simplify \eqref{eq:Lcondpart} to obtain the first term in \eqref{eq:Ldef}, as required.
% \beqs
% L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma-\coarseexp\alpha} \eps^{-1}\mright)}.
% \eeqs

We now seek to bound the variance term in \cref{eq:mlmcdecomp} with minimal cost. I.e., we choose the numbers of samples $\Nl$ such that the variance term is at most $\eps^2/2$ and the computational cost is minimised. Similar to the expression \cref{eq:mccomp2} for the variance of the Monte-Carlo estimator, one can show that the variance of the Multi-Level Monte-Carlo estimator is given by
\beq\label{eq:mlmcvariance}
\VAR{\QhatMLhL} = \sum_{l=0}^L \frac{\Vl}{\Nl},
\eeq
and the expected cost of $\QhatMLhL$ is: (following \cite{GrPaSc:19})
\begin{align}
\EXP{\Cost{\QhatMLhL}}&\leq \sum_{l=0}^L \EXP{\Cost{\Ylhat}}\nonumber\\
&= \sum_{l=0}^L \sum_{j=1}^{\Nl} \EXP{\Cost{\Ylj}}\text{ by the definition of } \Ylhat \text{ \cref{eq:Ylhatdef},}\nonumber\\
&\leq \sum_{l=0}^L \sum_{j=0}^{\Nl} \mleft(\EXP{\Cost{\Qhltilde}} + \EXP{\Cost{\Qhlmotilde}}\mright)\text{ by the definition of }\Yl \text{ \cref{eq:Yldef},}\nonumber\\
%% &\leq \sum_{l=0}^L \Nl \mleft(\cth \hl^{-\gamma} + \cth \hlmo^{-\gamma}\mright)\nonumber\\
&=\sum_{l=0}^L \Nl\mleft(1+s^{-\gamma}\mright) \cth \mleft(\hl^{-\gamma}+k^{a\gamma}\mright) \text{ by \cref{lem:c},}\nonumber\\
&= \mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \Nl\Cl,\label{eq:Cboundformin}
\end{align}
by the definition of $\Cl$, \cref{eq:Cldef}.

We now find an optimal number of samples for each level. To find this optimal number of samples we formulate this task as an optimisation problem:
Find $\Nz,\No,\ldots,\NL > 0$ to minimise \cref{eq:Cboundformin} subject to
\beqs
\sum_{l=0}^L \frac{\Vl}{\Nl} = \frac{\eps^2}2.
\eeqs
This is exactly the formulation used in \cite[Section 1.3]{Gi:15}, and therefore as in \cite[Section 1.3]{Gi:15} we can use a Lagrange multiplier to solve this minimisation problem, resulting in the values of $\Nl$ as defined in \cref{eq:Nl}. (The ceiling function in \cref{eq:Nl} is introduced because the values of $\Nl$ solving the optimisation problem may not be integers, however, the number of samples in the Multi-Level Monte-Carlo method must be integers. Increasing the optimal values of $\Nl$ slightly (by using the ceiling) will decrease the variance (as the variance is given by \cref{eq:mlmcvariance}), and so we will still have $\VAR{\QhatMLhL} \leq \eps^2/2.$)

We now infer the expected computational complexity of the Multi-Level Monte-Carlo meth\-od with $L$ given by \eqref{eq:Ldef} and the $\Nl$ given by \eqref{eq:Nl}. To simplify the calculation, we first bound $\Cl$ purely in terms of $\hl,$ rather than $\hl$ and $k,$ as in \cref{lem:c}. From \cref{lem:c} we have $\Cl \leq \cth \mleft(\hl^{-\gamma} k^{a\gamma}\mright),$ and therefore
\begin{align*}
  \Cl &\leq \cth \mleft(\hl^{-\gamma} + \Ccoarse^{\gamma} \hz^{-\gamma}\mright) \quad\text{by \cref{ass:coarse}}\\
  &= \cth \mleft(\hl^{-\gamma} + \Ccoarse^{\gamma} \mleft(\hl s^l\mright)^{-\gamma}\mright) \quad\text{by definition of } \hl \text{ \cref{eq:hl}}\\
  &= \cth\mleft(1+\Ccoarse^{\gamma}s^{-\gamma l}\mright)\hl^{-\gamma}\\
  &\leq \cth \mleft(1+\Ccoarse^{\gamma}\mright)\hl^{-\gamma}
\end{align*}
because $\gamma > 0$ and $s>1,$ so $s^{-\gamma l} < 1.$

We can now bound the expected computational complexity. From the expression \cref{eq:Cboundformin}, we have
\begin{align}
\EXP{\Cost{\QhatMLhL}} &\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^{L} \Cl \Nl\nonumber\\
&\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl \mleft(\frac2{\eps^{2}} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^L \mleft(\Vj\Costj\mright)^{\half} + 1\mright) \text{ (by \eqref{eq:Nl}),}\nonumber\\
&= 2\eps^{-2}\mleft(1+s^{-\gamma}\mright)\mleft(\sum_{l=0}^L\mleft(\Vl\Cl\mright)^{\half}\mright)^2 + \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl\nonumber\\
&= 2 \ct \cth \mleft(1+\Ccoarse^{\gamma}\mright)\mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2} \mleft(\sum_{l=0}^L \hl^{\frac{\beta-\gamma}2}\mright)^2\nonumber\\
&\quad\quad+ \cth  \mleft(1+\Ccoarse^{\gamma}\mright)\mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \hl^{-\gamma}\nonumber\\
&\quad\quad\quad\quad\text{(by \cref{ass:b,ass:coarse,lem:c}),}\nonumber\\
&= 2 \ct\cth \mleft(1+\Ccoarse^{\gamma}\mright) \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\hz^{\beta-\gamma}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2\nonumber\\
&\quad\quad+ \cth \mleft(1+\Ccoarse^{\gamma}\mright)\mleft(1+s^{-\gamma}\mright) \hz^{-\gamma} \sum_{l=0}^L s^{\gamma l},\label{eq:complexitymidway}
%% &=2 \ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 + \cth\Cppw^{-\gamma} k^{\rho + \gamma\coarseexp}  \sum_{l=0}^L s^{\gamma l} \text{ (by definition of } \hz\text{ )}\nonumber\\
%% &\leq2\ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 +  \frac{\mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha}\cth s^{\gamma}}{1-s^{-\gamma}}k^{\rho +  \frac{\gamma\sigma}\alpha}\eps^{-\frac\gamma\alpha} \text{ (since }\gamma>0,\text{ by \cref{lem:sumbound})}.\label{eq:complexitymidway}
\end{align}
by definition of $\hl.$

We now bound the two sums in \cref{eq:complexitymidway} using \cref{lem:sumboundnew}. Using \cref{lem:sumboundnew} with $\Lconst = 1/\alpha,$ $\func = \sqrt{2}\co\Ccoarse^\alpha k^{\sigma - a\alpha}$, and $\delta = \gamma>0$, the second term in \eqref{eq:complexitymidway} can be bounded by %(letting \csumdelta \de \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha} \Ccoarse^\delta / \mleft(1-s^{-\delta}\mright)$)
\begin{align}
&\cth \mleft(1+\Ccoarse^{\gamma}\mright)\frac{\mleft(1+s^{-\gamma}\mright)  \hz^{-\gamma} s^\gamma \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} \Ccoarse^\gamma}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha-a\gamma} \eps^{-\frac\gamma\alpha}\nonumber\\
  &\quad\quad= \frac{\mleft(1+s^{-\gamma}\mright)\cth \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} s^\gamma \mleft(1+\Ccoarse^{\gamma}\mright)}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha} \eps^{-\frac\gamma\alpha}.\nonumber\\
  &\quad\quad\leq \frac{\mleft(1+s^{-\gamma}\mright)\cth \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} s^\gamma \mleft(1+\Ccoarse^{\gamma}\mright)}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha} \eps^{-2},\label{eq:firstterm}
\end{align}
since $\alpha \geq \gamma/2.$

To bound the first sum in \eqref{eq:complexitymidway}, we must distinguish three cases, $\gamma=\beta,$ $\gamma > \beta,$ and $\gamma < \beta.$


If $\gamma=\beta,$ then the first part of \eqref{eq:complexitymidway} becomes (using \cref{lem:sumboundnew} with $\Lconst$ and $\func$ as above, and $\delta = 0$ and \cref{eq:Ldef})
\begin{align}
  &2 \ct\cth \mleft(1+\Ccoarse^{\gamma}\mright) \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(L+1\mright)^2\nonumber\\
  &\quad\quad\leq 2 \ct\cth \mleft(1+\Ccoarse^{\gamma}\mright) \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\eps^{-1}\sqrt{2} \co \Ccoarse^\alpha k^{\sigma-a\alpha}\mright)+2\mright)^2\nonumber\\
  &\quad\quad = 2 \ct\cth \mleft(1+\Ccoarse^{\gamma}\mright) \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha\mleft(\log_s\mleft(\eps^{-1}k^{\sigma-a\alpha}\mright) + \log_s\mleft(\sqrt{2} \co \Ccoarse^\alpha\mright) + 2\mright)\mright)^2\nonumber\\
  &\quad\quad\lesssim k^{\tau}\eps^{-2}\mleft(\mleft(\log_s\mleft(\eps^{-1}k^{\sigma-a\alpha}\mright)\mright)^2 +1\mright)
\label{eq:gammaequal}
\end{align}
In the case $\gamma > \beta$, to simplify the notation, we let
%We wish to simplify \eqref{eq:gammaequal}, so that it is of the form Constant $\times$ `Terms involving $\eps$ and $k$'. To achieve this simplification, we use \cref{ass:epsk}. As $k^{\sigma-a\alpha} \geq 1$ and $\eps \leq \mleft(\sqrt{2} \co \Ccoarse^{\alpha}\mright)/s^{2\alpha},$ it follows that
%% \beqs
%% 2 \leq \frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright),
%% \eeqs
%% and thus \eqref{eq:gammaequal} can be bounded by
%% \beq\label{eq:gammaequalpart1}
%% 8 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright)\mright)^2.
%% \eeq
%% As $k^\sigma k^{-a\alpha} \geq 1$ and $\eps \leq 1/\mleft(\sqrt{2}\co\Ccoarse^\alpha\mright),$ we can bound \eqref{eq:gammaequalpart1} by (including a change of base in the logarithm)
%% \beq\label{eq:gammaequalfinal}
%% \frac{32 \ct\cth \mleft(1+s^{-\gamma}\mright)}{\alpha^2 \mleft(\loge(s)\mright)^2} k^\tau \mleft(\loge\mleft(\frac{k^{\sigma-a\alpha}}\eps\mright)\mright)^2
%% \eeq
%% and obtain \cref{eq:mlmchheq}.
\beqs
\csumdelta \de \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}.
\eeqs

Then using \cref{lem:sumboundnew} with $\Lconst$ and $\func$ as above, but $\delta = (\gamma-\beta)/2 > 0$, the first term in \eqref{eq:complexitymidway} becomes
\begin{align}
&  \eps^{-2}2\ct\cth\mleft(1+\Ccoarse^{\gamma}\mright) \mleft(1+s^{-\gamma}\mright) k^\tau\eps^{-2} \hz^{\beta-\gamma}\mleft(\csumgammambetat s^{\frac{\gamma-\beta}2} k^{\frac{\gamma-\beta}2\frac\sigma\alpha} k^{-a\frac{\gamma-\beta}2} \eps^{-\frac{\gamma-\beta}{2\alpha}}\mright)^2\nonumber\\
  &\quad\quad= \Cgammagtrbeta k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}},\label{eq:gammagtr}
\end{align}
where
\beqs
\Cgammagtrbeta \de 2\ct\cth\mleft(1+\Ccoarse^{\gamma}\mright)\mleft(1+s^{-\gamma}\mright)\csumgammambetat^2 s^{\gamma-\beta} \Ccoarse^{\beta-\gamma};
\eeqs
and the second equality in \cref{eq:gammagtr} follows from the definition of $\hz$ in \cref{ass:coarse}.

If $\gamma < \beta,$ then using \cref{lem:sumboundnew} with $\Lconst$ and $\func$ as above, but with $\delta = (\gamma-\beta)/2 < 0$, the first term in \eqref{eq:complexitymidway} is
\beq\label{eq:gammalessbeta}
\Cgammalessbeta k^{\tau  -a\mleft(\beta-\gamma\mright)} \eps^{-2},
\eeq
where
\beqs\label{eq:gammaless}
\Cgammalessbeta \de 2\ct \cth\mleft(1+\Ccoarse^{\gamma}\mright) \mleft(1+s^{-\gamma}\mright)\Ccoarse^{\beta-\gamma}\frac{s^{\beta-\gamma}}{\mleft(s^{\frac{\beta-\gamma}2}-1\mright)^2}.
\eeqs

We now combine \cref{eq:complexitymidway,eq:firstterm,eq:gammaequal,eq:gammagtr,eq:gammalessbeta} and supress all the constants to obtain \cref{eq:mlmchhbounds}.
%Removing all the terms that are not of interest from \eqref{eq:gammaequal}, \eqref{eq:gammagtr}, and \eqref{eq:gammaless}, we obtain \eqref{eq:mlmchheq} and \eqref{eq:mlmchhoth}.
\epf

%% If $Q(\cdot) = \NHokDR{\cdot},$ then $\alpha = 2p,$ $\sigma = 2p+1$. If $Q(\cdot) = \NLtDR{\cdot},$ then $\alpha = 2p,$ $\sigma = 2p$. In both cases, $\beta = 2\alpha,$ $\tau = 2\sigma.$ Assume $\gamma = d$---optimal solver.

%% FINISH TOMORROW.

\section[The Helmholtz equation in the abstract setting]{Placing the stochastic Helmholtz equation in the abstract $k$-dependent Monte-Carlo\\ and Multi-Level Monte-Carlo setting}\label{sec:mlmcapplying}

We now show that the stochastic Helmholtz equation fits into the abstract $k$-dependent setting given above. We use the abstract results on the computational complexity of Monte-Carlo and Multi-Level Monte-Carlo methods in \cref{thm:hhmc,thm:mlmccomp} to derive fully $k$-explicit complexity bounds for Monte-Carlo and Multi-Level Monte-Carlo methods for the stochastic Helmholtz equation, given in \cref{thm:mcmlmchelmholtz}.

\subsection{Model problem and quantities of interest}\label{sec:mlmcmodel}

We let $u:\Omega\rightarrow\HokD$ solve the TEDP-analogue of \cref{prob:msedp} (see \cref{rem:tedp}), and let $\uhtilde:\Omega\rightarrow\Vhp$ solve the stochastic analogue of \cref{prob:fevtedp}. (I.e., $\uhtilde$ solves \cref{prob:fevtedp} sample-wise with coefficients $A(\omega)$ and $n(\omega)$, $\T = ik$, and meshsize $\homega$.) We assume $\uhtilde$ is measurable, see \cref{rem:Qhtilderandom}. Further, we assume that the stochastic Helmholtz equation is nontrapping almost surely, i.e., the TEDP-analogues of \cref{con:hh-fAn,con:hh-hetero,thm:hh-hetero} hold. We consider two quantities of interest (QoIs) of the solution $u$; the two norms $\NLtD{u}$ and $\NHokD{u},$ where $D$ is the computational domain.

\bre[Why consider these QoIs?]
We consider the norms $\NLtD{u}$ and $\NHokD{u}$ as QoIs because the Helmholtz equation is an elliptic PDE, and therefore it is natural to consider terms depending on $u$ and $\grad u$ (and these are, arguably, the simplest such terms). Moreover, we expect different $k$-dependence of the computational complexity for QoIs involving $u$ compared to QoIs involving  $\grad u$ (c.f., \cref{thm:fembound,def:probdataacc}). Considering both $\NLtD{u}$ and $\NHokD{u}$ as QoIs will allow us to see if this is the case.
\ere

\subsubsection{The values of $\alpha,$ $\sigma,$ $\beta,$ $\tau,$ $\gamma,$ and $a$}\label{sec:valuesmlmc}
For the two QoIs $\NLtD{u}$ and $\NHokD{u},$ the provable values of $\alpha,$ $\sigma,$ $\beta,$ and $\tau$ are given in \cref{lem:mlho,lem:mllt} below. Their values are obtained straightforwardly from \cref{thm:fembound} above. However, determining the value of $\gamma$, and especially the value of $a$, is more involved. In addition, we note that in practice the value of, in particular, $\alpha$ may be larger than predicted by the theory, see, e.g., \cite[Section 4]{ClGiScTe:11}. In this \lcnamecref{sec:valuesmlmc}, however, we will work with the provable values.

\paragraph{The value of $\gamma$} The value of $\gamma$ represents the efficiency of the solver one uses to solve the linear systems arising from the finite-element discretisations of the individual Helmholtz problems. Recall that the number of degrees of freedom in the linear systems is of the order $h^{-d}$ (if the mesh size for the finite-element mesh is $h$). In the following analysis we take $\gamma = d,$ i.e. we assume that we have access to an optimal Helmholtz solver, that can solve linear systems with $N$ unknowns arising from finite-element discretisations of Helmholtz problems in $\cO(N)$ time. Obtaining such a solver is the subject of much current research, and we refer to, e.g., the recent works \cite{GrSpVa:17,ZeScHeDe:19,TaZeHeDe:19} for a selection of modern solvers achieving close to this optimal scaling.

\paragraph{The values of $a$} In our analysis below, we consider two different values of $a$, $a=(2p+1)/2p$ (where $p$ is the polynomial degree of the finite-elements) and $a=1$. We now explain why $a=(2p+1)/2p$ is the natural choice, but has some limitations in the Multi-Level Monte-Carlo method. We then go on to explain how the choice $a=1$ removes these limitations, and we the discuss whether the choice $a=1$ is reasonable.

The first choice of $a$ is motivated by the finite-element results in \cref{thm:fembound} above. Recall from \cref{eq:hfemcond} that if $hk^{(2p+1)/2p}$ is sufficiently small (if $\CAnk \sim 1,$ with hidden constant dependent on $A$ and $n$), then the finite-element solution $\uh$ exists, is unique, and satisfies the error bounds in both the $L^2$- and $H^1_k$-norms. Therefore, since \cref{def:probdataacc} is concerned with existence, uniqueness, and error bounds for $\Qh,$ the choice $a=(2p+1)/2p$ is natural.

However, certain choices of $a$ and $Q$ mean that the number of levels $L$ will not grow with $k.$ In \cref{eq:Ldef} above, $L$ depends on $k^{\sigma-a\alpha};$ i.e., the $k$-dependence of $L$ is governed by the relationship between $a$ (i.e., the $k$-dependence of the coarsest level) and $\sigma/\alpha$ (the $k$-dependence of the finest level---see \cref{eq:hLcond}). Taking $a=(2p+1)/2p$ and $Q = \NHokD{u}$, so that $\alpha = 2p$ and $\sigma = 2p+1$ (see \cref{lem:mlho} below for details of why these values for $\alpha$ and $\sigma$ are correct) then $k^{\sigma-a\alpha} = 1$, and therefore $L$ is $k$-independent.

It may, however, be interesting to study the case where the number of levels $L$ \emph{increases} with $k$. If we instead choose $a=1$ (i.e., the condition for existence and uniqueness \cref{eq:probdataacc} simply requires a fixed number of points per wavelength), then we would have $k^{\sigma - a\alpha} = k,$ and so the number of levels $L$ would increase with $k.$ 

Therefore, the question arises, `Is the choice $a=1$ (with $\alpha$ and $\sigma$ still given by \cref{thm:fembound}) reasonable?'

In 1-d, the answer is `yes'. In \cite[Corollary 3.2]{IhBa:97} and \cite[Theorem 4.27 and equation (4.7.41)]{Ih:98} Ihlenburg and Babu\v{s}ka prove that the $h$-finite-element method for the homogeneous Helmholtz equation in 1-d is $\hk{1}{(2p+1)/2p}$-accurate; i.e., finite-element error bounds of a form similar to those in \cref{thm:fembound} hold if $hk$ is sufficiently small. Translated into the multi-level context, this result implies that for $d=1$, \cref{def:probdataacc} holds with $a=1$. We note that this result has \emph{not} been proved in higher dimensions (see the discussion in \cref{sec:helmfedisc}). However, we will assume that \cref{def:probdataacc} holds in higher dimensions with $a=1;$ i.e., we make the following \lcnamecrefs{ass:mlho}, and we will prove results on the computational complexity of Monte-Carlo and Multi-Level Monte-Carlo under these \lcnamecrefs{ass:mlho} (as well as when $a=(2p+1)/2p$).

\bas[Assumptions for $Q(u) = \NHokD{u}$ with $a=1$]\label{ass:mlho}
In the setting given at the beginning of \cref{sec:mlmcmodel}, if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $a=1,$ $\alpha = 2p$, $\sigma = 2p+1,$ $\beta = 4p$, and $\tau = 4p+2$, for some random variables $\Co$, $\cotilde$, and $\ct.$
\eas

\bas[Assumptions for $Q(u) = \NLtD{u}$ with $a=1$]\label{ass:mllt}
In the setting given at the beginning of \cref{sec:mlmcmodel}, if $Q(u) = \NLtD{u}$, then \cref{def:probdataacc,ass:b} hold with $a=1,$ $\alpha = 2p$, $\sigma = 2p,$ $\beta = 4p$, and $\tau = 4p$, for some random variables $\Co$, $\cotilde$, and $\ct.$
\eas


\subsection{Main result on Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation}
We are now in a position to state our main result on the computational complexity of Monte-Carlo and Multi-Level Monte-Carlo methods applied to the Helmholtz equation, \cref{thm:mcmlmchelmholtz} below. We first verify \cref{def:probdataacc} for each of our QoIs in the following two \lcnamecrefs{lem:mlho}.

\ble[Verifying assumptions for $Q(u) = \NHokD{u}$]\label{lem:mlho}
In the setting given at the beginning of \cref{sec:mlmcmodel}, if $Q(u) = \NHokD{u}$, then \cref{def:probdataacc,ass:b} hold with $a=(2p+1)/2p,$ $\alpha = 2p$, $\sigma = 2p+1,$ $\beta = 4p$, $\tau = 4p+2$, and $\Co$ and $\cotilde$ given by the constants in \cref{eq:hfemcond,eq:femhobound} respectively, and $\ct = \EXP{\cotilde^2}\mleft(1+s^\alpha\mright)^2.$
%$\Chcond(\omega) \Condn(n(\omega)\CAnk(\omega)^{-\frac1{2p}}$ and 
%\cref{ass:coarse} holds with $\Ccoarse(\omega)$ bounded sample-wise by $\Chcond(\omega) \Condn(n(\omega)) \CAnk^{-\frac1{2p}}(\omega)$ (with $\Chcond$ and $\Condn$ as defined in \cref{thm:fembound}, and $\CAnk$ as defined in \cref{ass:htwo}), $a = (2p+1)/2p$, and

\ele

\bpf[Proof of \cref{lem:mlho}]
By the assumptions of this \lcnamecref{lem:mlho}, it is immediate from \cref{eq:femhobound} that \cref{def:probdataacc} holds with $\alpha = 2p$ and $\sigma = 2p+1$. (See \cref{rem:higherorder} for why we can neglect the lower-order terms in \cref{eq:femhobound}.) To show \cref{ass:b}, we follow \cite[Proof of Proposition 4.2]{ChScTe:13} and use the triangle inequality and \cref{def:probdataacc} to show
\beq\label{eq:Ylhatbound}
\abs{\Ylhat(\omega)} \leq \abs{\mleft(\Qhltilde - Q\mright)(\omega)} + \abs{\mleft(Q- \Qhlmotilde\mright)(\omega)} \leq \cotilde(\omega)\mleft(\hl^\alpha + \hlmo^\alpha\mright)k^\sigma \Cfg.
\eeq
We then use \cref{eq:Ylhatbound} and the fact that $\VAR{\Ylhat} = \EXP{\abs{\Ylhat}^2} - \abs{\EXP{\Ylhat}}^2 \leq \EXP{\abs{\Ylhat}^2}$ to show \cref{eq:mlmcassb}, with $\ct = \EXP{\cotilde^2}\mleft(1+s^\alpha\mright)^2.$
\epf

\ble[Verifying assumptions for $Q(u) = \NLtD{u}$]\label{lem:mllt}
In the setting given at the beginning of \cref{sec:mlmcmodel}, if $Q(u) = \NLtD{u}$, then \cref{def:probdataacc,ass:b} hold with $a=(2p+1)/2p,$ $\alpha = 2p$, $\sigma = 2p,$ $\beta = 4p$, $\tau = 4p$, and $\Co$ and $\cotilde$ given by the constants in \cref{eq:hfemcond,eq:femltbound} respectively, and $\ct = \EXP{\cotilde^2}\mleft(1+s^\alpha\mright)^2.$
%If \cref{ass:coarse} holds with $\Ccoarse(\omega)$ bounded sample-wise by $\Chcond(\omega) \Condn(n(\omega)) \CAnk^{-\frac1{2p}}(\omega)$ (with $\Chcond$ and $\Condn$ as defined in \cref{thm:fembound}, and $\CAnk$ as defined in \cref{ass:htwo}), $a = (2p+1)/2p$, and $Q(u) = \NLtD{u}$, then \cref{def:probdataacc,ass:b} hold with $\alpha = 2p$, $\sigma = 2p,$ $\beta = 4p$, and $\tau = 4p$.
\ele

\bpf[Proof of \cref{lem:mllt}]
The proof is exactly analagous to the proof of \cref{lem:mlho}, except we use \cref{eq:femltbound} instead of \cref{eq:femhobound}.
\epf



We require the following \lcnamecref{ass:variance} on the variance of the approximations $\Qhtilde.$ Such an assumption is standard, see, e.g., \cite[Text below equation (3)]{ClGiScTe:11}.

\bas\label{ass:variance}
The variance $\VAR{\Qhtilde}$ is constant with respect to $h$.
\eas


\bth[Computational complexity of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation]
Suppose \cref{ass:costone,ass:omegabad,ass:coarse} (on the cost of one realisation of $\Qhtilde$, on the integrability of a combination of constants related to the size of $\Omegabad$, and on the coarse space)  and \cref{ass:variance} hold.

\ben
\item If the assumptions of \cref{lem:mllt,lem:mlho} hold, and $\ct$ as defined in \cref{lem:mllt,lem:mlho} is finite, then the Monte-Carlo and Multi-Level Monte-Carlo methods achieve a root-mean-squared error of at most $\eps$, and their computational complexity (up to factors independent of $h$ and $k$) is given by  the first two lines of \cref{tab:mcresults}, where `$k\eps$ small' means
\beq\label{eq:kepscond}
k\eps < \sqrt{2}\co \Ccoarse^{2p}.
\eeq

\item If \cref{ass:mlho,ass:mllt} hold instead of the assumptions of \cref{lem:mllt,lem:mlho}, then the Monte-Carlo and Multi-Level Monte-Carlo methods achieve a root-mean-squared error of at most $\eps$, and their computational complexity (up to factors independent of $h$ and $k$) is given by the last two lines of \cref{tab:mcresults}.
  \een
\label{thm:mcmlmchelmholtz}
\enth

The proof of \cref{thm:mcmlmchelmholtz} is given on \cpageref{page:mcmlmchelmholtzproof} below.

\begin{table}[h]
  \centering
\begin{tabular}{Sc Sc Sc Sc}
  \toprule
  $Q(u)$ & $a$ & Monte-Carlo & Multi-Level Monte-Carlo\\
  \midrule
      $\NHokD{u}$ & $\displaystyle \frac{2p+1}{2p}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2}$ \\
  $\NLtD{u}$ & $\displaystyle \frac{2p+1}{2p}$ & $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & \makecell{$\displaystyle k^{d} \eps^{-2}$ if $k\eps$ small,\\otherwise $\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$} \\
    $\NHokD{u}$ & 1 &$\displaystyle k^{d\frac{2p+1}{2p}} \eps^{-2-\frac{d}{2p}}$ & $\displaystyle k^{d+2} \eps^{-2}$ \\
      $\NLtD{u}$ & 1 & $\displaystyle k^d \eps^{-2-\frac{d}{2p}}$ &$\displaystyle k^d \eps^{-2}$\\
  \bottomrule
\end{tabular}
\caption{Computational complexity of Monte-Carlo and Multi-Level Monte-Carlo algorithms\label{tab:mcresults}}
\end{table}

We now discuss the results in \cref{thm:mcmlmchelmholtz}. The results for Multi-Level Monte-Carlo methods are consistently better than those for Monte-Carlo methods in terms of $\eps$-dependence, unless the condition for existence and uniqueness of $\uh$ is more restrictive than the condition to keep the error bounded\footnote{In the cases we consider, this scenario only occurs when we take $a = (2p+1)/2p$ and $Q(u) = \NLtD{u},$ so $\alpha = \sigma = 2p.$}. In such a case, for $\eps$ small and/or $k$ small, Multi-Level Monte-Carlo out-performs Monte-Carlo, but if $\eps$ and/or $k$ are large, then Multi-Level Monte-Carlo is identical to Monte-Carlo (because there are no additional levels, i.e., in \cref{eq:Ldef} the first term in the maximum is negative, and so $L=0$).

However, the $k$-dependence of the Multi-Level Monte-Carlo and Monte-Carlo methods (and which method has the more favourable $k$-dependence) is more complicated, and so we discuss it in more detail.

\paragraph{Identical $k$-dependence} Observe that in the cases (i) $a = (2p+1)/2p$ and $Q(u) = \NHokD{u}$ and (ii) $a=1$ and $Q(u) = \NLtD{u}$, the $k$-dependence of the Monte-Carlo and Multi-Level Monte-Carlo methods is the same. This is unsurprising; in each case the criterion on the coarse space \cref{eq:coarse} has the same $k$-dependence as the definition of $\hL$ \cref{eq:hLcond}, since $a = \sigma/\alpha.$ Consequently, the number of levels $L$ is independent of $k$ (see also that the factor $k^{\sigma - a\alpha}$ in \cref{eq:Ldef} is $k$-independent in each of these cases). Since the number of levels is independent of $k$, and the $k$-dependence of the coarse and fine levels is the same, it is unsurprising that the computational complexity of the two estimators has the same $k$-dependence.

However, in the other two cases, the $k$-dependence of the complexity of the Multi-Level Monte-Carlo method is different to that of the Monte-Carlo method. We consider each of these cases in turn.

\paragraph{When Multi-Level Monte-Carlo has better $k$-dependence} In the case $a=(2p+1)/2p$ and $Q(u) = \NLtD{u},$ we see that the $k$-dependence of the Multi-Level Monte-Carlo method is \emph{better} than that of the Monte-Carlo method. To understand this improvement, observe that the $k$-dependence of the criterion \cref{eq:coarse} on the coarse space ($\hz \lesssim k^{-(2p+1)/2p}$) is \emph{more} restrictive than the $k$-dependence one would otherwise impose to ensure the bias error is small (`$\hL^{2p}k^{2p}$ is sufficiently small'). Therefore, if we take $\hL \sim k^{-(2p+1)/2p},$ (to satisfy the coarse space requirement) the bias error (of the order $\hL^{2p}k^{2p} = k^{-1}$) will \emph{decrease} as $k$ increases. Moreover, the variance of the Multi-Level Monte-Carlo estimator (given by \cref{eq:mlmcvariance}) will also decrease as $k$ increases. Even on the coarsest level, the variance will be of the order $\hz^{4p}k^{4p} = k^{-2}$ (see \cref{ass:b}). Therefore, because the variance on each level decreases as $k$ increases, the number of samples on each level will also decrease as $k$ increases, reducing the overall computational cost in a $k$-dependent way.

\paragraph{When Multi-Level Monte-Carlo has worse $k$-dependence} Conversely, in the case  $a=1$ and $Q(u) = \NHokD{u},$ we see that the $k$-dependence of the cost of the Multi-Level Monte-Carlo estimator is \emph{worse} than that of the Monte-Carlo estimator. The reason for this worse dependence is, in essence, the converse of the reason for the improved dependence in the discussion above. The difference between the coarse space (of the order $k^{-1}$) and the fine space (with $\hL$ of the order $k^{-(2p+1)/2p}$, see \cref{eq:hLcond}) increases as $k$ increases, and therefore the number of levels $L$ will increase as $k$ increases (see \cref{eq:Ldef}, and observe that in this case $k^{\sigma - a\alpha} = k$). Moreover, on any level $l$ where $\hl \gtrsim k^{-(2p+1)/2p}$ (i.e., not the finest level), the variance $\Vl$ will increase as $k$ increases, since $\Vl \sim \hl^{4p}k^{4p+1}.$ Therefore, on each level the variance (and thus the number of samples) will increase as $k$ increases, resulting in an overall $k$-dependent increase in the computational cost.

It remains to be seen how these theoretical predictions are borne out in numerical computations; such computations should be the subject of future research.


\bre[Proving probabilistic bounds on the cost]
In \cite{GrPaSc:19}, the authors extend their bounds on the \emph{expectation} of the computational cost for Monte-Carlo and Multi-Level Monte-Carlo methods for the radiative transport equation to bounds on the \emph{exceedance probabilities} of the computational cost. I.e., they prove bounds of the form
\beq\label{eq:mcprobbound}
\PP\mleft(\Cost{\Qhat} < M(\eps,\delta,\Qhat)\mright) > 1-\delta^2,
\eeq
for some function $M$, where $\Qhat$ is the Monte-Carlo or Multi-Level Monte-Carlo estimator (see \cite[Theorems 5.12 and 5.13]{GrPaSc:19}). They make only mild additional assumptions on the randomness to prove bounds of the form \cref{eq:mcprobbound}; these assumptions mean they can bound $\VAR{\Qhtilde}$ and hence $\VAR{\Cost{\Qhat}}.$ The probabilistic bounds \cref{eq:mcprobbound} then follow from bounds on $\VAR{\Cost{\Qhat}}$ using Chebyshev's inequality.

We could apply these proof techniques to prove a probabilistic bound of the form \cref{eq:mcprobbound} for Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation. However, the calculations for the Helmholtz equation would be conceptually similar to those in \cite{GrPaSc:19}, albeit more involved, as we would need to keep track of the $k$-dependence. Given we expect the results we obtain would be similar to those in \cite{GrPaSc:19}, we elect not to pursue them.
\ere
\subsection{Proof of \cref{thm:mcmlmchelmholtz}}
We first prove that the assumptions in the abstract setting of \cref{sec:mlmcsetup,sec:mc,sec:mlmcan} hold for the stochastic Helmholtz equation, before applying the theory developed in \cref{sec:mlmcsetup,sec:mc,sec:mlmcan} to prove \cref{thm:mcmlmchelmholtz}. Recall that we have assumed the stochastic Helmholtz problem is almost-surely nontrapping. In particular, when we apply \cref{thm:fembound}, the constant $\CAnk$ will be independent of $k.$

%% \bre[The $\omega$-dependence of $\Ccoarse$ in \cref{lem:mlho,lem:mllt}]
%% The $\omega$-dependence of the terms in the bound on $\Ccoarse(\omega)$ in \cref{lem:mlho,lem:mllt} come from the fact that the terms $\Chcond,$ $\Condn(n),$ and $\CAnk$ in \cref{thm:fembound} may depend on $A$ and $n$. As $A$ and $n$ are random fields, $\Chcond,$ $\Condn(n),$ and $\CAnk$ are now random variables.
%% \ere

%% As stated above, we want to analyse the behaviour of Monte-Carlo and Multi-Level Monte-Carlo methods for the Helmholtz equation when $a=1.$ However, as stated above, we cannot prove results on the behaviour of finite-element discretisations of the Helmholtz equation analogous to \cref{lem:mlho,lem:mllt} when $a=1.$ Therefore we state the analogues of \cref{lem:mlho,lem:mllt} in the case $a=1$ as assumptions, under which we will investigate the behaviour of Monte-Carlo and Multi-Level Monte-Carlo methods.

\bpf[Proof of \cref{thm:mcmlmchelmholtz}]
\label{page:mcmlmchelmholtzproof}
The proof follows immediately from the case $\beta > \gamma$ in \cref{thm:mlmccomp}, because we have $\beta = 4p$ or $4p+1$  (depending on the QoI), for $p\geq 1$ and $\gamma=d.$ We then substitute the appropriate values of $a$, $\alpha,$ $\beta$, etc. into \cref{eq:mlmchhbounds}, and identify which of the terms $k^{\tau - a\mleft(\beta-\gamma\mright)}$ or $k^{\gamma\sigma/\alpha}$ dominates for large $k$, and which of the terms $\eps^{-2}$ or $\eps^{-\gamma/\alpha}$ dominates for small $\eps.$

Two cases require explaining a little further. Firstly, the case $a=(2p+1)/2p$ and $Q(u) = \NLtD{u}$ (so $\alpha = \sigma = 2p.$) In this case, the expression for $L$ in \cref{eq:Ldef} evaluates as
  \beq\label{eq:specialL}
  L = \max\set{\ceil{\frac1{2p}\log_s\mleft(\sqrt{2}\co\Ccoarse^{2p}k^{-1}\eps^{-1}\mright)},0}
\eeq
Observe that for $k$ (or $\eps$) sufficiently large, the first term in the maximum in \cref{eq:specialL} may be negative (i.e. if $\mleft(\eps k\mright)^{-1}$ is suffciently close to 0, then the logarithm will be negative). In such a case, the maximum of the two quantities on the right-hand side of \cref{eq:specialL} will be 0, and in such a case the Multi-Level Monte-Carlo method reverts to the Monte-Carlo algorithm since $L=0,$ i.e., there are no additional levels of refinement. The criterion for the first term to be positive (and so for the Multi-Level Monte-Carlo method to be distinct from the Monte-Carlo method) is
\beqs
\sqrt{2}\co\Ccoarse^{2p}k^{-1}\eps^{-1} > 1,
\eeqs
which is equivalent to the condition \cref{eq:kepscond}.

In the case that the condition \cref{eq:kepscond} holds we can apply \cref{eq:mlmchhbounds}, and by substituting in the appropriate values of $a,$ $\alpha$, etc., the right-hand side of \cref{eq:mlmchhbounds} becomes
\beq\label{eq:firstmlmccond}
k^{\mleft(\frac{2p+1}{2p}d\mright)-2}\eps^{-2} + k^d \eps^{-\frac{d}{2p}}.
\eeq
To see which of the $k$-dependent terms in \cref{eq:firstmlmccond} dominates for large $k,$ observe that
\beqs
d \geq \frac{2p+1}{2p}d -2
\eeqs
if, and only if, $p \geq d/4.$ As $p \geq 1$ and $d \leq 3,$ we always have $p \geq d/4,$ and hence the $k^d$ term dominates.

Secondly, when $a=1$ and $Q(u) = \NHokD{u}$, on substituting the appropriate values of $a,$ $\alpha$, etc. into \cref{eq:mlmchhbounds}, the right-hand side of \cref{eq:mlmchhbounds} becomes
\beq\label{eq:secondmlmccond}
k^{d+2}\eps^{-2} + k^{\frac{2p+1}{2p}d}\eps^{-\frac{d}{2p}}.
\eeq
Observe that, analagously to above, $d + 2 \geq (2p+1)d/(2p)$ if, and only if, $p \geq d/4,$ and therefore the $k^{d+2}$ term in \cref{eq:secondmlmccond} dominates.
\epf

%% \bre[No coarse-space dependence in Multi-Level Monte-Carlo cost bounds]
%% Observe that there is no dependence on the coarse space (which has mesh size $\hz$) in the Multi-Level Monte-Carlo results in \cref{thm:mcmlmchelmholtz}. I.e., the quantity $a$ does not appear when $\beta \neq \gamma$. (Recall from \cref{ass:coarse} that the coarse space is chosen to be proportional to $k^{-a}$.)

%% This lack of $a$-dependence is surprising at first glance. If $a < \sigma/\alpha,$ then the number of levels $L$ grows with $k$ (as the term $k^{\sigma - a\alpha}$ in \cref{eq:Ldef} will grow with $k$), and one would expect to see this growing number of levels affect the overall computational complexity. However, it seems that whilst the number of levels grows with $k$, reducing the computational cost, on the other hand the growing number of samples (growing because there are more levels on which to compute) offsets the gains from extra levels.

%% One can see this offsetting in the proof; the terms $\hz^{\beta-\gamma}$ and $\hz^{-\gamma}$ on the left-hand sides of  \cref{eq:complexitymidway,eq:gammagtr} respectively are cancelled by the terms involving $\hz$ from the application of \cref{lem:sumboundnew}, see the right-hand sides of \cref{eq:complexitymidway,eq:gammagtr}. (Observe that in \cref{eq:firstterm} the quantity $\hz^{-\gamma}$ cancels with the term $\Ccoarse^{\gamma}k^{-a\gamma}$ arising from \cref{lem:sumboundnew}. Similarly the $k$-dependence of the term $\hz^{\beta-\gamma}$ in \cref{eq:gammagtr} cancels with the term $\mleft(k^{-a(\gamma-\beta)/2}\mright)^2$ arising from \ref{lem:sumboundnew}.)

%% It remains to be seen whether this coarse-mesh independence is seen in numerical computations. Such an investigation could be the subject of future work on Multi-Level Monte-Carlo methods for the Helmholtz equation.
%% \ere

