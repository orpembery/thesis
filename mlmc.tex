%% In this \lcnamecref{sec:comp} we state and prove an abstract result on the convergence of multi-level Monte Carlo methods, laregly following the proof of \cite[Theorem 1]{ClGiScTe:11}. Our result is a generalisation of \cite[Theorem 1]{ClGiScTe:11} in the following three ways:
%% \ben
%% \item In \cite{ClGiScTe:11} it is assumed that the convergence of the approximate QoIs $\Qhl$, and the cost of producing samples of these QoIs, only depends on the parameter $\hl$ (where, in stochastic PDE applications, $\hl$ is the mesh size for the finite-element discretisation). However, in this work, we assume that the convergence and cost also depend on another parameter $k,$ and we make the dependence of the final computational cost of the MLMC method explicit in $k.$ In our application to the Helmholtz equation, $k$ will be the wavenumber of the problem.
%% \item In \cite{ClGiScTe:11} it is assumed that the approximating QoIS $\Qhl$ exist for all levels $l$. This corresponds to the finite-element solution of the PDE under investigation existing for all mesh sizes $h.$ Whilst this assumption is true for the stationary diffusion equation studied in \cite{ClGiScTe:11}, it is \emph{not} true for the Helmholtz equation that we study here. Therefore we make the additional assumption (\cref{ass:qoie} below) that $\Qhl$ only exists for sufficiently small $\hl.$
%% \item In \cite{ClGiScTe:11} the error $\eps$ incurred in the MLMC method is equally divided between the bias and the variance of the MLMC method (see the Proof of \cref{thm:mlmccomp3}). However, in this work we assume that there is a quantity $\splitting \in (0,1)$ (see \cref{ass:splittingbounds}), possibly dependent on $k$ that allows a vairable `split' of the error between the bias and the variance. Our main use of this is in\optodo{Insert refs once it's done}, where we use this variable splitting to compensate for the fact that to bound the (squared) bias error by $\eps^2/2$ would mean we take $\hL \lesssim k^{-1},$ but to ensure the finite-element solution $\uh$ exists, we must take $\hL \lesssim k^{-3/2}.$
%% \een
%% We now proceed to prove our abstract MLMC convergence result, comtaining the generalisations metioned above.

\section{Set up}

We work in the framework of \cref{chap:stochastic}. For $h>0,$ define the random field\optodo{Assume or show it is one} $\uh:\Omega \rightarrow \Vhp$ by letting $\uh(\omega)$ solve \cref{prob:fevgen} (with $\T = \TR$) with coefficients $A(\omega)$ and $n(\omega)$ (if it exists). Let $Q:\HozDDR\rightarrow\RR$ be a (measurable) \defn{quantity of interest} (QoI) of the solution $u$ (so that $Q(u)$ is a random variable). As an abuse of notation, we also use $Q$ to denote $Q \circ u,$ where the context means this notation is unambiguous. If $\uh(\omega)$ exists, we let $\Qh$ denote $Q \circ \uh.$

As hnited at in the preceeding paragraph, the finite-element solution $\uh(\omega)$ may not always exist. Moreover, the existence (or not) of $\uh(\omega)$ is due to the fact that the constants involved in the definitions of accuracy and data-accuracy of a finite-element method (\cref{def:hkacc,def:hkdataacc} above) are dependent on the coefficients $A$ and $n$. Therefore, when $A$ and $n$ are random fields, the associated existence and uniqueness criterion (and a priori bounds) are all path-dependent. To define this path-dependence precisely, we use the following \lcnamecref{def:probdataacc}.

\bas[Probabilistic version of data-accuracy]\label{def:probdataacc}
There exist random variables $\Co$ and $\cotilde$ such that if
\beq\label{eq:probdataacc}
hk^a < \Co(\omega),
\eeq
then $\uh(\omega)$ exists, is unique, and $Q$ and $\Qh$ satisfy
\beqs
\abs{Q(\omega)-\Qh(\omega)} \leq \cotilde(\omega) h^\alpha k^\sigma \Cfg.
\eeqs
\eas

In order to define a finite-element approximation of $u$ (and therefore a random variable $\Qh$) that exists almost surely, we borrow a technique from \cite{GrPaSc:19}. Informally, for a given $h>0,$ for any realisations $\omega \in \Omega$ such that \cref{eq:probdataacc} is \emph{not} satisfied, we compute on a finer mesh (that does satisfy \cref{eq:probdataacc}). For fixed $h>0$ we define
\beq\label{eq:hmaxomega}
\hmaxomega \de \Co(\omega)k^{-a}.
\eeq
We then define
\beq\label{eq:homega}
\homega \de \min\set{h,\hmaxomega}.
\eeq
Observe that $\homega$ \emph{always} satisfies \cref{eq:probdataacc}. We then define
\beqs
\uhtilde(\omega) = u_{\homega}(\omega),
\eeqs
i.e., $\uhtilde$ is the finite-element approximation of $u(\omega)$ on the mesh with mesh size $\homega.$ We then define
\beq\label{eq:Qhtilde}
\Qhtilde = Q \circ \uhtilde.
\eeq
\optodo{Assume or prove that $\uhtilde$ is a random field}

Given $\uhtilde$ has a random mesh size, the cost of computing $\uhtilde$ (or $\Qhtilde$) will be a random variable\optodo{Prove?}. Therefore we make the following \lcnamecref{ass:costone} on the cost of computing samples.

\bas[Cost of one sample]\label{ass:costone}
There exist $\cthtilde, \gamma > 0$ such that $\cthtilde$ is independent of $h$ and $k$, and if $\Qhtilde(\omega)$ exists, then
\beqs
\Cost{\Qhtilde(\omega)} \leq \cthtilde(\omega) \homega^{-\gamma},
\eeqs
\eas

The following \lcnamecref{lem:c} shows how, provided the set $\Omegabad$\optodo{Chat about this earlier/here} has small probability, the expected cost of computing a sample of $\Qhtilde$ is governed solely by the specified mesh size $h$ (and not by any over-refinement). The assumption \cref{eq:cass} is the technical version of the statement `$\Omegabad$ has small probability'.

\bas[Assumptions on $\Omegabad$]\label{ass:omegabad}
Assume:
\beq\label{eq:cass}
\cth \de \EXP{\cthtilde}+ \EXP{\cthtilde\Co^{-\gamma}} < \infty
\eeq
\eas

\ble[Expected cost of one sample]\label{lem:c}
If \cref{ass:omegabad} holds, then 
\beq\label{eq:singlecost}
\EXP{\Cost{\Qh}} \leq \cth \mleft(h^{-\gamma}+k^{a\gamma}\mright).
\eeq
\ele
\optodo{Show/assume cost is RV}
\bpf[Proof of \cref{lem:c}]
The proof follows closely that in \cite[Lemma 5.8]{GrPaSc:19}.
We have
\beq\label{eq:costpf1}
\Cost{\uhtilde(\omega)} \leq \cthtilde(\omega)\homega^{-\gamma} \leq \cthtilde(\omega) \mleft(h^{-\gamma} + \mleft(\hmaxomega\mright)^{-\gamma}\mright)
\eeq
by \cref{ass:costone,eq:homega}. Then using \cref{eq:hmaxomega}, the definition of $\hmaxomega,$ \cref{eq:costpf1} is bounded by
\beq\label{eq:costpf2}
\cthtilde(\omega)h^{-\gamma} + \mleft(\cthtilde\Co\mright)(\omega) k^{a\gamma},
\eeq
and therefore as \cref{eq:cass} holds, we obtain \cref{eq:singlecost}.
\epf

\ble[Convergence of numerical method]\label{ass:a}
Under \cref{def:probdataacc}, there exist constant $\co, \alpha, \sigma> 0$, such that $\co$ is independent of $h$ and $k$, and
\beqs
\abs{\EXP{\Qhtilde-Q}} \leq \co k^\sigma h^{\alpha}.
\eeqs
\ele

\bpf[Proof of \cref{ass:a}]
The proof of \cref{ass:a} is immediate (with $\co = \EXP{\cotilde}$) from the definition of $\Qhtilde$ \cref{eq:Qhtilde}, \cref{def:probdataacc} and the fact that $\homega \leq h$ (by \cref{eq:homega}.
\epf


\optodo{Define RMSE}
\section{Monte-Carlo methods}

We now define the Monte-Carlo estimator of $Q$,
\beqs
\QhatMC \de \frac1{\NMC} \sum_{j=1}^{\NMC} \Qhtildesj,
\eeqs
where the $\Qhtildesj$ are independently and identically distributed samples of $\Qhtilde.$

We can prove the following \lcnamecref{thm:hhmc} on the computational complexity of the Monte-Carlo estimator $\QhatMC$

\bth[Computational complexity of Monte-Carlo]\label{thm:hhmc}
Let the assumptions of \cref{lem:c,ass:a} hold. Given $\eps \in (0,1),$ if
\beq\label{eq:NMC}
\NMC  \sim 2\VAR{\Qhtilde}\eps^{-2}
\eeq
and
\beq\label{eq:hMC}
h \sim \mleft(\sqrt{2}\co\mright)^{-\frac1{\alpha}}k^{-\frac\sigma\alpha}\eps^{\frac1{\alpha}},
\eeq
then $\err{\QhatMC} \leq \eps$ and the computational complexity of $\QhatMC$ satisfies
\beqs
\EXP{\CMC} \sim \VAR{\Qhtilde}\mleft(\eps^{-2-\frac{\gamma}{\alpha}}k^{\frac{\gamma\sigma}\alpha} + \eps^{-2}k^{a\gamma}\mright).
\eeqs
\enth

\bpf[Proof of \cref{thm:hhmc}]
The proof is standard, see, e.g., \cite[Section 2.1]{ClGiScTe:11}. We have
\begin{align}
\err{\QhatMC}^2 &= \EXP{\mleft(\QhatMC - \EXP{\QhatMC} + \EXP{\QhatMC} - \EXP{Q}\mright)^2}\nonumber\\
&= \EXP{\mleft(\QhatMC - \EXP{\QhatMC}\mright)^2} + \mleft(\EXP{\QhatMC} - \EXP{Q}\mright)^2\nonumber\\
&= \VAR{\QhatMC} + \mleft(\EXP{\QhatMC} - \EXP{Q}\mright)^2,\label{eq:mccomp1}
\end{align}
where the second line follows from the fact that $\EXP{\QhatMC - \EXP{\QhatMC}} = 0$, and the third line follows from the fact that $\QhatMC$ is an unbiased estimator.

By definition of $\QhatMC$, and the fact that the samples $\Qhtildesj$ are independent, we have
\beq\label{eq:mccomp2}
\VAR{\QhatMC} = \frac1{\NMC^2}\sum_{j=1}^{\NMC}\VAR{\Qhtildesj} = \frac1{\NMC} \VAR{\Qhtilde}.
\eeq
Therefore we can conclude from \cref{eq:mccomp1,eq:mccomp2} that the root-mean-squared-error satisfies
\beq\label{eq:mccomp3}
\err{\QhatMC}^2 = \frac1{\NMC}\VAR{\Qhtilde} + \NLoO{\Qhtilde-Q}^2.
\eeq
By \cref{eq:NMC,eq:hMC}, each of the terms in \cref{eq:mccomp3} is bounded by  $\eps^2/2,$ and therefore $\err{\QhatMC} \leq \eps.$

All that remains is to estimate the (expected) computational complexity. We have
\beqs
\EXP{\Cost{\QhatMC}} = \NMC \EXP{\Cost{\uhtilde}} \leq \NMC \cth h^{-\gamma} \sim 2\VAR{\Qhtilde}\eps^{-2} \mleft(\cth \mleft(\sqrt{2} \co\mright)^{\frac\gamma\alpha}k^{\frac{\gamma\sigma}{\alpha}}\eps^{-\frac\gamma\alpha} + k^{a\gamma}\mright)
\eeqs
as required.
\epf

\section{Multi-level Monte-Carlo}

We define a set of levels$\set{\hl}_{l=0}^L$ ($L$ to be chosen) such that $\hl =\frac{\hlmo}s$ for $l \geq 1$. We then define the correction operators between the levels by $\Yl \de \Qhltilde - \Qhlmotilde, l \geq 1,$ $\Yz = \Qhztilde.$ We let $\Ylhat$ be the Monte-Carlo estimator of $\Yl$,
 \beqs
\Ylhat \de \frac1{\Nl}\sum_{i=1}^{\Nl} \Yli,
 \eeqs
 with $\Nl$ to be chosen, where $\Yli$ denotes independent samples of $\Yl$. Finally we are able to define the \defn{multi-level Monte Carlo estimator}
 \beqs
 \QhatMLhL \de \sum_{l=1}^L \Ylhat,
 \eeqs
 where the $\Ylhat$ are independent.

  %% The following assumptions
  %% % \lcnamecrefs{ass:coarse}
  %%  will form the backbone of our analysis. They are a generalisation of the assumptions contained in \cite{ClGiScTe:11,ChScTe:13} for the MLMC method, the generalisation being that we assume that the quantities below depend not only on the levels $\hl$ but also on some additional parameter $k>1.$ When this theory is applied to the Helmholtz equation, $k$ will be the wavenumber of the Helmholtz equation.

%% The following assumption (which will be realised in a more concrete setting for the Helmholtz equation) concerns the existence of the approximating QoIs $\Qhl.$

%% \bas[Existence of $\Qhl$]\label{ass:qoie}
%% There exist $\Ccoarse,\coarseexp > 0$ with $\Ccoarse$ independent of $k$ such that if
%% \beqs
%% \hl \leq \Ccoarse k^{-\coarseexp},
%% \eeqs
%% then the QoI $\Qhl$ exists.
%% \eas

\bas[Variance of correction operators]\label{ass:b}
There exist $\ct, \beta, \tau > 0$, such that $\ct$ is independent of $h$ and $k,$ and
\beqs
\Vl \de \VAR{\Yl} \leq \ct k^\tau\hl^{\beta},
\eeqs  where $\VAR{\cdot}$ denotes variance.
\eas

In order to obtain a nice expression for the cost of computing one sample of $\Qh,$ we require the following assumption on the coarse space:

\bas[Dependence of coarse space on $k$]\label{ass:coarse}
We let
\beqs
\hz = \Ccoarse k^{-a}.
\eeqs
for some chosen constant $\Ccoarse > 0.$
\eas

 
% We write $\Vl$ for $\VAR{\Yl}.$
 
 We want to determine the choices of $L$ and $\Nl, l = 0,\ldots,L,$ such that the root-mean-squared eror (RMSE)
 \beqs
 \err{\QhatMLhL} \de \mleft(\EXP{\mleft(\QhatMLhL - \EXP{Q}\mright)^2}\mright)^{\half}
 \eeqs
 satisfies $\err{\QhatMLhL} \leq \eps,$ for some pre-defined $\eps > 0.$



%\input{nice}



%% \subsection{Lemma}

%% The proof of the main \lcnamecref{thm:mlmccomp} will require the following \lcnamecref{lem:sumbound}.

%% \ble\label{lem:sumbound}
%% If $L$ is given by

%% then, for $s>1$ and $\delta \in \RR,$ we have the bound
%% \beq\label{eq:sumbound}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L+1 & \tif \delta = 0,\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}s^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha} &\tif \delta >0\\
%% \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}&\tif \delta < 0
%% \end{cases}
%% \eeq
%% \ele

%% \bpf[Proof of \cref{lem:sumbound}]
%% The proof follows that in \cite{ClGiScTe:11}. We first observe that, since $L$ is given by \eqref{eq:Ldef}, it follows that
%% \beq\label{eq:Lbounds}
%% \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) \leq L < \frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha \eps^{-1}\mright) + 1.
%% \eeq
%% Rearranging \eqref{eq:Lbounds}, we obtain the bounds
%% \beq\label{eq:saLbounds}
%% \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1} \leq s^{\alpha L} < \sqrt{2}\co \Ccoarse^\alpha k^{\sigma}\mesh(k)^\alpha\eps^{-1}s^\alpha.
%% \eeq
%% If $\delta > 0,$ then we use the right-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLpos}
%% s^{\delta L} < \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}s^{\delta},
%% \eeq
%% and if $\delta < 0,$ we use the left-hand bound in \eqref{eq:saLbounds} to obtain
%% \beq\label{eq:sdLneg}
%% s^{\delta L} \leq \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Cppw^{\delta}k^{\frac{\delta\sigma}{\alpha}}\mesh(k)^\delta\eps^{-\frac\delta\alpha}.
%% \eeq
%% We now observe that, for $\delta \neq 0,$
%% \begin{align}
%% \sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
%% &= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
%% &\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumbound}
%% \end{align}
%% since $s^{-\delta} > 0,$ as $s >0.$ Combining \eqref{eq:ssumbound} with \eqref{eq:sdLpos} and \eqref{eq:sdLneg}, we obtain \eqref{eq:sumbound} in the cases $\delta \neq 0.$ The case $\delta=0$ is straightforward.
%% \epf


The following theorem describes the computational effort needed to obtain RMSE $\leq \eps$. It is exactly the same as \cite[Theorem 1]{ClGiScTe:11}, but with the dependence on all the parameters explicit.%, and with some additional cases enumerated. %\Cref{thm:mlmccomp} contains more cases than in \cite[Theorem 1]{ClGiScTe:11} because \cite[Theorem 1]{ClGiScTe:11} makes the assumption throughout that $\alpha \geq 1/2\min\set{\beta,\gamma}.$ This assumption does not always hold for the Helmholtz equation (see the cases of a direct solver in 3-D below), however, examining the proof of \cite[Theorem 1]{ClGiScTe:11}  shows that in any given case, one only needs the assumption $\alpha \geq \beta/2$ or the assumption $\alpha \geq \gamma2$, never both at the same time. Therefore, for convenience, we explicitly state when these conditions are needed, and for completeness, we give the results when these conditions are violated. 

%% The following \lcnamecref{ass:constants3} will ensure that \cref{ass:qoie} is satisfied.

%%  \bas[$\eps$ sufficiently small]\label{ass:constants3}
%%  Assume
%%  \beqs
%% \eps \leq \sqrt{2} \co \Ccoarse^{\alpha} k^{\sigma-\coarseexp\alpha}.
%%  \eeqs
%%  \eas


\bas[Assumptions on $\eps$ and $k$ to simplify expressions in the case $\beta=\gamma$]\label{ass:epsk}
\beqs
\eps \leq \min\set{\frac{\sqrt{2}\co\Ccoarse^\alpha}{s^{2\alpha}},\frac1{\sqrt{2}\co\Ccoarse^\alpha}},
\eeqs
and
\beqs
k^{\sigma-a\alpha} \geq 1.
\eeqs
\eas

\bth[MLMC Complexity Theorem]\label{thm:mlmccomp}
If \cref{ass:epsk} holds,  $L$ is given by
\beq\label{eq:Ldef}
L = \ceil{\frac1\alpha\log_{s}\mleft(\sqrt{2}\co  \Ccoarse^\alpha k^{\sigma-a\alpha} \eps^{-1}\mright)},
\eeq
that is,
\beq\label{eq:hLcond}
\hL \leq \mleft(\frac\eps{\sqrt{2}\co k^{\sigma}}\mright)^{\frac1\alpha},
\eeq
and the number of samples on each computational level is given by
\beq\label{eq:Nl}
\Nl = \ceil{\frac2{\eps^{2}} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^{L} \mleft(\Vj\Cj\mright)^{\half}},
\eeq
where $\Cl \de \cth\hl^{-\gamma}$, then computational effort $\CMLhL(\eps)$ required to obtain $\err{\QhatMLhL} \leq \eps$ satisfies the bounds
 
 \begin{numcases}{ \CMLhL(\eps) \lesssim}
k^\tau \mleft(\loge\mleft(\frac{k^{\sigma-a\alpha}}\eps\mright)\mright)^2 \eps^{-2} + k^{\frac{\gamma\sigma}\alpha} \eps^{-\frac\gamma\alpha}  & if $\beta = \gamma$,\label{eq:mlmchheq}\\ 
k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}} + k^{\frac{\gamma\sigma}{\alpha}}\eps^{-\frac\gamma\alpha} & otherwise.\label{eq:mlmchhoth}
\end{numcases}
 \enth
 \bpf[Proof of \cref{thm:mlmccomp}]
We recall the decomposition of the (squared) mean-squared error into the bias error and the sampling error analagous to \cref{eq:mccomp1}
\beqs\label{eq:mlmcdecomp}
\errQhatMLhL^2 = \VAR{\QhatMLhL} + \mleft(\EXP{\QhatMLhL - Q}\mright)^2.
\eeqs
We now proceed to choose the parameters $L$ and $\Nl, l = 0,\ldots,L$ such that we can bound both the bias and the variance by $\eps^2/2.$

We first bound the bias, to do this, we only need to choose $L.$ One can show that the bias is equal to $\abs{\EXP{\QhL - Q}}^2.$ Therefore by \cref{ass:a} a sufficient condition for the bias to be $\leq \eps^2/2$ is (by \cref{ass:a})
\beqs
\co k^\sigma \hL^\alpha \leq \frac{\eps}{\sqrt{2}},
\eeqs
that is, \eqref{eq:hLcond}. As $\hL = \hz s^{-L},$ it follows from \eqref{eq:hLcond} that a sufficient condition for the bias to be $\leq \eps^2/2$ is
\beq\label{eq:Lcondpart}
L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co k^\sigma \hz^\alpha \eps^{-1}\mright)}.
\eeq
As $\hz = \Ccoarse k^{-a},$ we can simplify \eqref{eq:Lcondpart} to obtain \eqref{eq:Ldef}.
% \beqs
% L = \ceil{\frac1\alpha\log_s\mleft(\sqrt{2}\co\Ccoarse^\alpha k^{\sigma-\coarseexp\alpha} \eps^{-1}\mright)}.
% \eeqs

We now seek to bound the variance. One can show similarly to \cref{eq:mccomp2} that the variance $\VAR{\QhatMLhL} = \sum_{l=0}^L \Nl^{-1} \Vl,$ and the cost is: (following \cite{GrPaSc:19})
\begin{align}
\EXP{\Cost{\QhatMLhL}}&\leq \sum_{l=0}^L \EXP{\Cost{\Ylhat}}\nonumber\\
&= \sum_{l=0}^L \sum_{i=1}^{\Nl} \EXP{\Cost{\Yli}}\nonumber\\
&\leq \sum_{l=0}^L \sum_{i=0}^{\Nl} \mleft(\EXP{\Cost{\Qhltilde}} + \EXP{\Cost{\Qhlmotilde}}\mright)\nonumber\\
%% &\leq \sum_{l=0}^L \Nl \mleft(\cth \hl^{-\gamma} + \cth \hlmo^{-\gamma}\mright)\nonumber\\
&=\sum_{l=0}^L \Nl\mleft(1+s^{-\gamma}\mright) \cth \hl^{-\gamma}\nonumber\\
&= \mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \Nl\Cl\label{eq:Cboundformin}
\end{align}

To find the optimal number of samples per level (the values of $\Nl, l=0,\ldots,L$) we formulate this as an optimisation problem to find the numbers $\Nl$ that minimise \eqref{eq:Cboundformin}, subject to $\VAR{\QhatMLhL}=\eps/2.$ This can be solved using a Lagrange multiplier as in \cite{Gi:15}, and we obtain \cref{eq:Nl}. We now need to infer the computational complexity for MLMC with $L$ given by \eqref{eq:Ldef} and the $\Nl$ given by \eqref{eq:Nl}.

From \cref{eq:Cboundformin}
\begin{align}
\EXP{\Cost{\QhatMLhL}} &\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^{L} \Cl \Nl\nonumber\\
&\leq \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl \mleft(\frac2{\eps^{2}} \mleft(\frac{\Vl}{\Cl}\mright)^{\half}\sum_{j=0}^L \mleft(\Vj\Cj\mright)^{\half} + 1\mright) \text{ (by \eqref{eq:Nl})}\nonumber\\
&= 2\eps^{-2}\mleft(1+s^{-\gamma}\mright)\mleft(\sum_{l=0}^L\mleft(\Vl\Cl\mright)^{\half}\mright)^2 + \mleft(1+s^{-\gamma}\mright)\sum_{l=0}^L \Cl\nonumber\\
&= 2 \ct \cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2} \mleft(\sum_{l=0}^L \hl^{\frac{\beta-\gamma}2}\mright)^2 + \cth \mleft(1+s^{-\gamma}\mright) \sum_{l=0}^L \hl^{-\gamma} \text{ (by \cref{ass:b,ass:costone})}\nonumber\\
&= 2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\hz^{\beta-\gamma}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 + \cth\mleft(1+s^{-\gamma}\mright) \hz^{-\gamma} \sum_{l=0}^L s^{\gamma l} \text{ (by definition of } \hl\text{ )}\label{eq:complexitymidway}\\
%% &=2 \ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 + \cth\Cppw^{-\gamma} k^{\rho + \gamma\coarseexp}  \sum_{l=0}^L s^{\gamma l} \text{ (by definition of } \hz\text{ )}\nonumber\\
%% &\leq2\ct\cth \Cppw^{\beta-\gamma}k^{\tau + \rho+\coarseexp\mleft(\gamma - \beta\mright)}\eps^{-2}\mleft(\sum_{l=0}^L s^{l\mleft(\frac{\gamma-\beta}2\mright)}\mright)^2 +  \frac{\mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha}\cth s^{\gamma}}{1-s^{-\gamma}}k^{\rho +  \frac{\gamma\sigma}\alpha}\eps^{-\frac\gamma\alpha} \text{ (since }\gamma>0,\text{ by \cref{lem:sumbound})}.\label{eq:complexitymidway}
\end{align}

Using \cref{lem:sumboundnew} with $\Lconst = 1/\alpha,$ $\func = \sqrt{2}\co\Ccoarse^\alpha k^{\sigma - a\alpha}$, and $\delta = \gamma$, the second sum in \eqref{eq:complexitymidway} can be bounded (as $\gamma > 0$) by %(letting \csumdelta \de \mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha} \Ccoarse^\delta / \mleft(1-s^{-\delta}\mright)$)
\beq\label{eq:firstterm}
\cth\frac{\mleft(1+s^{-\gamma}\mright)  \hz^{-\gamma} s^\gamma \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} \Ccoarse^\gamma}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha-a\gamma} \eps^{-\frac\gamma\alpha}
= \frac{\mleft(1+s^{-\gamma}\mright)\cth \mleft(\sqrt{2}\co\mright)^{\frac\gamma\alpha} s^\gamma}{1-s^{-\gamma}} k^{\frac{\gamma\sigma}\alpha} \eps^{-\frac\gamma\alpha}
\eeq

To bound the sum in the first part of \eqref{eq:complexitymidway}, we must distinguish three cases based on $\gamma - \beta.$


If $\gamma=\beta,$ then the first part of \eqref{eq:complexitymidway} becomes (using \cref{lem:sumboundnew} with $\Lconst$ and $\func$ as above, and $\delta = 0$ and \cref{eq:Ldef})
\beq
2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(L+1\mright)^2 \leq 2 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^{\sigma-a\alpha}}\eps\mright)+2\mright)^2.
\label{eq:gammaequal}
\eeq
We wish to simplify \eqref{eq:gammaequal}, so that it is of the form Constant $\times$ `Terms involving $\eps$ and $k$'. To achieve this simplification, we use \cref{ass:epsk}. As $k^{\sigma-a\alpha} \geq 1$ and $\eps \leq \mleft(\sqrt{2} \co \Ccoarse^{\alpha}\mright)/s^{2\alpha},$ it follows that
\beqs
2 \leq \frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright),
\eeqs
and thus \eqref{eq:gammaequal} can be bounded by
\beq\label{eq:gammaequalpart1}
8 \ct\cth \mleft(1+s^{-\gamma}\mright)k^{\tau}\eps^{-2}\mleft(\frac1\alpha \log_s \mleft(\frac{\sqrt{2} \co \Ccoarse^\alpha k^\sigma k^{-a\alpha}}\eps\mright)\mright)^2.
\eeq
As $k^\sigma k^{-a\alpha} \geq 1$ and $\eps \leq 1/\mleft(\sqrt{2}\co\Ccoarse^\alpha\mright),$ we can bound \eqref{eq:gammaequalpart1} by (including a change of base in the logarithm)
\beq\label{eq:gammaequalfinal}
\frac{32 \ct\cth \mleft(1+s^{-\gamma}\mright)}{\alpha^2 \mleft(\loge(s)\mright)^2} k^\tau \mleft(\loge\mleft(\frac{k^{\sigma-a\alpha}}\eps\mright)\mright)^2
\eeq
and obtain \cref{eq:mlmchheq}.

For simplicity in the proof of \cref{eq:mlmchhoth}, we define
\beqs
\csumdelta \de \frac{\mleft(\sqrt{2}\co\mright)^{\frac\delta\alpha}\Ccoarse^{\delta}}{1-s^{-\delta}}.
\eeqs

If $\gamma > \beta$ then using \cref{lem:sumboundnew} as above, but with $\delta = (\gamma-\beta)/2$, the first term in \eqref{eq:complexitymidway} becomes\optodo{Check if the below is right - it seems to be saying the complexity is independent of the coarse mesh}
\beq
\eps^{-2}2\ct\cth \mleft(1+s^{-\gamma}\mright) k^\tau \hz^{\beta-\gamma}\mleft(\csumgammambetat s^{\frac{\gamma-\beta}2} k^{\frac{\gamma-\beta}2\frac\sigma\alpha} k^{-a\frac{\gamma-\beta}2} \eps^{-\frac{\gamma-\beta}{2\alpha}}\mright)^2 = \Cgammagtrbeta k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}},\label{eq:gammagtr}
\eeq
where
\beqs
\Cgammagtrbeta \de 2\ct\cth\mleft(1+s^{-\gamma}\mright)\csumgammambetat^2 s^{\gamma-\beta} \Ccoarse^{\beta-\gamma},
\eeqs
that is, an expression of the form \cref{eq:mlmchhoth}. The second equality in \cref{eq:gammagtr} follows from the definition of $\hz$ in \cref{ass:coarse}.
If $\gamma < \beta,$ then analagously the first term in \eqref{eq:complexitymidway} is
\beqs
\Cgammalessbeta k^{\tau + \mleft(\gamma-\beta\mright)\frac\sigma\alpha} \eps^{-2-\frac{\gamma-\beta}{\alpha}},
\eeqs
where
\beq\label{eq:gammaless}
\Cgammalessbeta \de \frac{\Cgammagtrbeta}{s^{\gamma-\beta}}.
\eeq

We now combine \eqref{eq:firstterm}, \eqref{eq:gammaequalfinal}, \eqref{eq:gammagtr}, and \eqref{eq:gammaless} and supress all the constants to obtain the result.
%Removing all the terms that are not of interest from \eqref{eq:gammaequal}, \eqref{eq:gammagtr}, and \eqref{eq:gammaless}, we obtain \eqref{eq:mlmchheq} and \eqref{eq:mlmchhoth}.
\epf


%\input{nasty}

\subsection{Technical lemma}

\ble\label{lem:sumboundnew}
If $L$ is given by
\beq\label{eq:Ldefgen}
L = \ceil{\Lconst\log_{s}\mleft( \func \eps^{-1}\mright)},
\eeq
for some $\Lconst, \func > 0,$ then, for $s>1$ and $\delta \in \RR,$ we have the bound
\beq\label{eq:sumboundgen}
\sum_{l=0}^{L} s^{\delta l} \leq
\begin{cases}
L+1 & \tif \delta = 0,\\
\frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0\\
\frac{1}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst}&\tif \delta < 0
\end{cases}
\eeq
%% \beq\label{eq:sumboundLmo}
%% \sum_{l=0}^{L} s^{\delta l} \leq
%% \begin{cases}
%% L & \tif \delta = 0,\\
%% \frac{s^{\delta}}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst} &\tif \delta >0\\
%% \frac{1}{1-s^{-\delta}}\func^{\delta\Lconst}\eps^{-\delta\Lconst}&\tif \delta < 0
%% \end{cases}
%% \eeq
%\optodo{Tidy}
\ele

\bpf[Proof of \cref{lem:sumboundnew}]
The proof follows that in \cite{ClGiScTe:11}. We first observe that, since $L$ is given by \eqref{eq:Ldefgen}, it follows that
\beq\label{eq:Lboundsgen}
\Lconst\log_s\mleft(\func \eps^{-1}\mright) \leq L < \Lconst\log_s\mleft(\func \eps^{-1}\mright) + 1.
\eeq
Rearranging \eqref{eq:Lboundsgen}, we obtain the bounds
\beq\label{eq:saLboundsgen}
\mleft( \func\eps^{-1}\mright)^{\alpha \Lconst} \leq s^{\alpha L} < \mleft( \func\eps^{-1}\mright)^{\alpha \Lconst}s^\alpha.
\eeq
If $\delta > 0,$ then we use the right-hand bound in \eqref{eq:saLboundsgen} to obtain
\beq\label{eq:sdLposgen}
s^{\delta L} < \func^{\delta\Lconst}\eps^{-\delta\Lconst}s^{\delta},
\eeq
and if $\delta < 0,$ we use the left-hand bound in \eqref{eq:saLboundsgen} to obtain
\beq\label{eq:sdLneggen}
s^{\delta L} \leq \func^{\delta\Lconst}\eps^{-\delta\Lconst}.
\eeq
We now observe that, for $\delta \neq 0,$
\begin{align}
\sum_{l=0}^L s^{\delta l} &= \frac{s^{\delta\mleft(L+1\mright)} -1}{s^{\delta}-1}\nonumber\\
&= \frac{s^{\delta L} - s^{-\delta}}{1-s^{-\delta}}\nonumber\\
&\leq \frac{s^{\delta L}}{1-s^{-\delta}},\label{eq:ssumboundgen}
\end{align}
since $s^{-\delta} > 0,$ as $s >0.$ Combining \eqref{eq:ssumboundgen} with \eqref{eq:sdLposgen} and \eqref{eq:sdLneggen}, we obtain \eqref{eq:sumboundgen} in the cases $\delta \neq 0.$ The case $\delta=0$ is straightforward.
\epf
