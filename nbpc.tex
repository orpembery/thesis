\section{Introduction and Motivation from UQ}\label{sec:intronbpc}

\subsection{Motivation from uncertainty quantification of the Helmholtz equation} 
Consider the stochastic Helmholtz equation 
\beq\label{eq:nbpchh}
\nabla\cdot\big(A(\omega,\bx) \nabla u(\omega,\bx) \big) + k^2 n(\omega,\bx) u(\omega,\bx) =-f(\bx), \quad \bx\in\Dp,
\eeq
as defined in \cref{chap:stochastic}. To calculate quantities of interest of the solution $u(\omega,\cdot)$, one must solve many deterministic Helmholtz problems, with each one corresponding to different realisations of the coefficients $A(\omega,\cdot)$ and $n(\omega,\cdot)$.
Solving all these deterministic problems is a very computationally-intensive task because linear systems arising from discretisations of the Helmholtz equation are notoriously difficult to solve; see the discussion in \cref{sec:numsolve} above.

Moreover, when using preconditioned iterative methods, as described in \cref{sec:numsolve} above, the cost of \emph{constructing} preconditioners for the linear systems is also very expensive. For example, if our preconditioner is given by an exact $LU$ factorisation, then (in 3-d) the cost of calculating the preconditioner is $\cO\mleft(N^2\mright)$ and the cost of applying the preconditioner is $\cO\mleft(N^{4/3}\mright)$ (where $N$ is the number of unknowns), see, e.g., \cite[Section 4]{GaZh:19}.

Observe that the cost of applying the preconditioner is significantly less than the cost of constructing the preconditioner. Therefore, if one calculates a preconditioner for one realisation of \cref{eq:nbpchh} and then re-uses it for many `nearby' (in some sense) realisations of \cref{eq:nbpchh}, then one could obtain considerable computational savings. This idea of `reusing' preconditioners for `nearby' linear systems is the `nearby preconditioning' strategy proposed in this \lcnamecref{chap:nbpc}.

If one does not use an exact $LU$-factorisation as the preconditioner, one may still obtain considerable savings by reusing preconditioners. Even if one uses a modern Helmholtz preconditioner such as a sweeping preconditioner or a domain-decompostion preconditioner (see the recent review article \cite{GaZh:19} for an overview of many types of Helmholtz preconditioners) one still performs $LU$-factorisations (or direct solves) on subdomains of $\Dp$ (see, e.g., \cite[Section 2]{GaZh:19}). Therefore resuing preconditioners, and thereby calculating fewer $LU$ factorisations may still lead to considerable savings.

In this \lcnamecref{chap:nbpc}, for simplicity, we restrict our attention to the case where the preconditioner is an exact $LU$ decomposition. In this case, the only error that arises in the nearby preconditioning process is due to applying a preconditioner constructed for one realisation of the Helmholtz equation to a different, nearby, realisation. Therefore  we do not need to consider the error arising from the preconditioning process itself.% The answer to \cref{it:nbpcq1} above makes $k$-explicit the extent to which the $LU$ factorisation for one realisation of \cref{eq:nbpchh} can be used as an effective preconditioner for Galerkin matrices arising from different realisations of $A$ and $n$.

\subsection{Statement of the problem}\label{sec:problem}
Let $\Aj, \nj$, $j=1,2$ satisfy the properties of $A$ and $n$ in \cref{prob:vedp}, with $\uj$ the corresponding solution and $\Dm,$ $f$, etc. as in \cref{prob:vedp}. Let $\Amatj$, $j=1,2,$ be the Galerkin matrices corresponding to $h$-finite-element discretisations of \cref{prob:vedp} (see \cref{eq:matrixAjdef} below for a precise definition of $\Amatj$). The results we prove in this \lcnamecref{chap:nbpc} also hold for the TEDP, \cref{prob:vtedp}, see \cref{sec:TEDP} below for the minor changes one must make in this case.
 
This \lcnamecref{chap:nbpc} answers the following question:

% Inspired by the usage of enumitem here: https://tex.stackexchange.com/a/58714
\ben[label=Q1., ref=Q1]
\item\label[itemblank]{it:nbpcq1} How small must $\NLi{\Aso - \Ast}$ and 
$\NLi{\nso - \nst}$ be (in some norm, in terms of $k$-dependence) for GMRES 
applied to $(\Amat^{(1)})^{-1}\Amat^{(2)}$ to converge in a $k$-independent number of iterations
%$ to be a good preconditioner for $\Amat^{(2)}$
 for arbitrarily large $k$? 
\een


\subsection{Statement of the main results}\label{sec:main}
Our main results about \cref{it:nbpcq1} are \cref{cor:1,cor:1a} below. \Cref{cor:1a} gives results in the \emph{Euclidean} norm on matrices, denoted by $\|\cdot\|_2$ (induced by the Euclidean norm on vectors), whereas \Cref{cor:1} gives results in the \emph{weighted} norms $\NDmatk{\cdot}$ and $\NDmatkI{\cdot}$. These weighted norms are induced by the corresponding vector norms
\beq\label{eq:Dk}
\NDmatk{\bv}^2:= \big( \Dmatk \bv, \bv\big)_2 = %\big( (\Smat_I + k^2 \Mmat_1)\bv,\bv\big)_2 
\N{v_h}^2_{\HokDR}
\quad \tand
\quad \NDmatkI{\bv}^2:= \big( \Dmatk^{-1} \bv, \bv\big)_2 %= %\big( (\Smat_I + k^2 \Mmat_1)\bv,\bv\big)_2 
%\N{v_h}^2_{\HokDR}
\eeq
where $\Dmatk$ is given in terms of familiar finite-element stiffness- and mass-matrices by \cref{eq:Dk2} below and $\vh =\sum_i \vi \phii$, where the $\phii$ are the finite-element basis functions.

As described in \cref{sec:wpdisc}, the PDE analysis of the Helmholtz equation naturally takes place in the norm $\NHokDR{\cdot}$, and \cref{eq:Dk} shows that the norm $\NDmatk{\cdot}$ is
the discrete analogue of the norm $\NHokDR{\cdot}$. %See \cref{eq:Dk2} and \cref{eq:Dk3} below for this norm expressed in terms of the Euc
The norms $\NDmatk{\cdot}$ and $\NDmatkI{\cdot}$ recently appeared in results about the convergence of domain-decomposition methods %in this norm are proved 
for the Helmholtz equation \cite{GrSpVa:17}, \cite{GrSpZo:18}, and for the time-harmonic Maxwell equations \cite{BoDoGrSpTo:19}. 

\Cref{cor:1,cor:1a} are proved under the \Cref{cond:1nbpc,cond:2} below. These \lcnamecrefs{cond:1nbpc} can be informally stated as 
\bit
\item the obstacle $\Dm$ and the coefficients $\Aso$ and $\nso$ are such that $u^{(1)}$ exists and the problem is \emph{nontrapping} (in the sense described in  \cref{sec:wpdisc} above), and
\item the meshsize $h$ and polynomial degree $p$ in the finite-element method are chosen to depend on $k$ to ensure that the 
finite-element solution to the problem with coefficients $\Aso$ and $\nso$ exists, is unique, and 
%Galerkin method (with coefficients $\Aso$ and $\no$) 
is uniformly accurate as $k\tendi$. 
\eit 

\begin{condition}[Nontrapping bound on $u^{(1)}$]\label{cond:1nbpc}
$\Aso, \nso,$ and $\Dm$ are such that, given $f\in L^2(\DR)$ with $\supp \, f \subset \BR$, 
the solution of \cref{prob:vedp} %(\cref{prob:edp}) 
$u^{(1)}$ exists, is unique, and, given $k_0>0$, $u^{(1)}$ satisfies the bound 
\beq\label{eq:bound1}
\big\|u^{(1)}\big\|_{\HokDR} \leq C^{(1)}_{\rm bound} \N{f}_{L^2(\Dp)} \quad \tfa k\geq k_0,
\eeq
where $C^{(1)}_{\rm bound}$ is independent of $k$, but dependent on $\Aso, \nso, \Dm, R$, and $k_0$.
\end{condition}

\begin{condition}[$k$-independent accuracy of the FE solution for $a^{(1)}(\cdot,\cdot)$]
\label{cond:2}

\

(i) Given $k_0>0$, $h$ and $p$ are such that, given $f\in \LtDR$ with $\supp\, f \subset \BR$, the solution $u_h$ of the Galerkin method \cref{eq:galerkin} with $a(\cdot,\cdot)=a^{(1)}(\cdot,\cdot)$ (and with $\FE(v)$ defined in \cref{eq:Ledp}) exists and is unique for all $k\geq k_0$. % (so that the matrix $\Amato$ is invertible). 
Furthermore, if $f= n\sum_j \alpha_j\phi_j$ for some $\alpha_j \in \CC$ and  $n\in \LiDRRR$  (i.e.~$f$ is an arbitrary element of $\Vhp$ multiplied by $n$), then
\beq\label{eq:bound3}
\N{u-u_h}_{\HokDR} \leq C^{(1)}_{\rm FEM1} \N{f}_{\LtDR} \quad\tfa k\geq k_0, 
\eeq
where $C^{(1)}_{\rm FEM1}$  is independent of $k$ and $h$, but dependent on $\Aso, \nso, \Dm, R, k_0$, and $p$.

(ii) Given $k_0>0$, $h$ and $p$ are such that, given $F\in (\HozDDR)'$, the solution $u_h$ of the Galerkin method \cref{eq:galerkin} 
with $a(\cdot,\cdot)=a^{(1)}(\cdot,\cdot)$
exists, and is unique for all $k\geq k_0$.
%(so that the matrix $\Amato$ is invertible). 
Furthermore, if $F(v)= (A\nabla \widetilde{f},\nabla v)_{\LtDR}$, where $A\in L^\infty(\DR, \RR^{d\times d})$, $A$ is symmetric, and $\widetilde{f} := \sum_j \alpha_j \phi_j$ with $\alpha_j\in \CC$
 (i.e.~$\widetilde{f}$ is an arbitrary element of $\Vhp$), then
\beq\label{eq:bound4}
\N{u-u_h}_{\HokDR} \leq C^{(1)}_{\rm FEM2}\,k\, \N{F}_{(\HokDR)'} \quad\tfa k\geq k_0, 
\eeq
where $C^{(1)}_{\rm FEM2}$  is independent of $k$ and $h$, but dependent on $\Aso, \nso, \Dm, R, k_0$, and $p$.
\end{condition}

\bre[\Cref{eq:bound4} is $k$-independent]\label{rem:yesitis}
Note that \cref{eq:bound4} is a $k$-independent bound on $\NHokDR{\utilde},$ despite the fact that a factor $k$ appears on the right-hand side. The factor $k$ appears because the weighted norm $\NHokDR{\cdot}$ is used in the definition of $\NHokDRs{\cdot},$ and therefore $\NHokDRs{\Ftilde} \sim \NHoDRs{\Ftilde}/k.$
\ere




\bth[Answer to \cref{it:nbpcq1}: $k$-independent weighted GMRES iterations]\label{cor:1}
Assume that $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc}, and $h$ and $p$ satisfy \cref{cond:2}. Given $k_0>0$, there exist constants $\Co$ and $\Ct$  independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso, p$, and $\kz$) such that if 
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:cond}
C_1 \,k \,\NLiDRRRdtd{\Aso-\Ast} +C_2 \, k\, \NLiDRRR{\nso-\nst}
\leq \frac{1}{2}
\eeq
for all $k\geq k_0$, then \emph{both} weighted GMRES working in $\|\cdot\|_{\Dmat_k}$ (and the associated inner product) applied to 
\beq\label{eq:pcsystem1}
(\Amat^{(1)})^{-1}\Amat^{(2)}\bu = \bff
\eeq
\emph{and} weighted GMRES working in $\|\cdot\|_{(\Dmat_k)^{-1}}$ (and the associated inner product) applied to 
\beq\label{eq:pcsystem2}
\Amat^{(2)}(\Amat^{(1)})^{-1}\bv = \bff
\eeq
 converge in a $k$-independent number of iterations for all $k\geq k_0$.
\enth

\bth[Answer to \cref{it:nbpcq1}: $k$-independent (unweighted) GMRES iterations]\label{cor:1a}
Assume that $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc}, and $h$ and $p$ satisfy \cref{cond:2}. Given $k_0>0$,
let $C_1$ and $C_2$ be as in \cref{cor:1}, and let $s_{\pm}$ and $m_{\pm}$ be as in \cref{lem:normequiv} below (note that all these constants are independent of $k$, $h$, and $p$). Then if 
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:conda}
 C_1 \,\left(\frac{s_+}{m_-}\right) \,\frac{1}{h} \,
\NLiDRRRdtd{\Aso-\Ast} + C_2 \, \left(\frac{m_+}{m_-} \right)k \, \NLiDRRR{\nso-\nst}
\leq \frac{1}{2}
\eeq
for all $k\geq k_0$, then standard GMRES (working in the Euclidean norm and inner product) applied to either of the equations \cref{eq:pcsystem1} or \cref{eq:pcsystem2}
%\beqs
%(\Amat^{(1)})^{-1}\Amat^{(2)}\bu = \bff\quad\text{ or } \quad\Amat^{(2)}(\Amat^{(1)})^{-1}\bv = \bff
%\eeqs
 converges in a $k$-independent number of iterations for all $k\geq k_0$.
\enth

Two notes regarding \cref{cor:1,cor1a}: (i) the constants $C_1$ and $C_2$ are expressed explicitly in \cref{eq:C1nbpc} and \cref{eq:C2} below in terms of constants appearing in \cref{cond:1nbpc,cond:2}, and (ii) the $L^\infty(\DR)$ norm on a matrix-valued functions appearing on the right-hand sides of \cref{eq:main1} and \cref{eq:main1a} is defined by
\beqs
\N{A}_{L^\infty(\DR)}:= \esssup_{\bx\in\DR}\N{A(\bx)}_2.
\eeqs


The factor $1/2$ on the right-hand sides of \cref{eq:cond} and \cref{eq:conda} can be replaced by any number $<1$ and the result still holds, although the number of GMRES iterations is then larger -- but still independent of $k$.
%In \cref{sec:proofFEM}, the constant $C_2$ is expressed explicitly in terms of $C_1$.

\bre
When $h\sim  k^{-1}$, the bounds \cref{eq:main1} and \cref{eq:main1a} (and hence also \cref{eq:cond} and \cref{eq:conda}) are identical in their $k$-dependence; however, when $h\ll k^{-1}$ (as one needs to take to overcome the pollution effect, as discussed in \cref{sec:helmfedisc}) the bound \cref{eq:main1a} for standard GMRES is more pessimistic than the bound \cref{eq:main1} for weighted GMRES.
\ere


\paragraph{How sharp are \cref{cor:1,cor:1a} in their $k$-dependence?}
Numerical experiments in \cref{sec:num} indicate that the condition \cref{eq:cond} is sharp, i.e., that the $k$ in \cref{eq:cond} cannot be replaced by $k^\alpha$ for $\alpha<1$. This indicated sharpness of \cref{eq:cond} is also supported by the PDE-result \cref{thm:2} below. Indeed, \cref{thm:2} % and \cref{lem:1} 
 shows that the condition
\beq\label{eq:sufficientlysmall}
k\,
\NLiDRRRdtd{\Aso-\Ast} \quad\text{ and } \quad k\,\NLiDRRR{\nso-\nst}
%\Big) 
\quad\text{ both sufficiently small}
\eeq
is not only an answer to \cref{it:nbpcq1} (about finite-element discretisations), but is also the natural answer to the analogue of \cref{it:nbpcq1} at the level of PDEs, namely 
\ben[label=Q2., ref=Q2]
\item\label[itemblank]{it:nbpcq2}
How small must $\NLiDRRRdtd{\Aso - \Ast}$ and 
$\NLiDRRR{\nso - \nst}$ be (in terms of $k$-dependence) for the relative error in approximating 
%$u^{(1)}$ to be a good approximation to 
$u^{(2)}$ by $u^{(1)}$ to be bounded independently of $k$ for arbitrarily-large $k$? 
\een
\cref{lem:sharp} then shows that the condition ``$k\NLiDRRR{\nso - \nst}$ sufficiently small" is the \emph{provably-sharp} answer to \cref{it:nbpcq2} when $\Aso= \Ast= I$.

%Before stating these PDE results, we define the weighted $H^1$ norm
%\beq\label{eq:1knorm}
%\N{v}^2_{\HokDR} := \N{\grad v}^2_{L^2(\DR)} + k^2 \N{v}^2_{L^2(\DR)} \quad \tfor v \in H^1_{0,D}(\DR),
%\eeq
%where the space $H^1_{0,D}(\DR)$, defined by \cref{eq:spaceEDP} below, is the natural space containing the solution of the exterior Dirichlet problem. 
To state these PDE results, we use the notation for $a,b>0$ that $a\lesssim b$ when $a\leq C b$ for some $C>0$, independent of $k$, and $a\sim b$ if $a\lesssim b$ and $b\lesssim a$.


%The sharpness of \cref{eq:cond} and \cref{eq:main1} is also supported by the answer to the analogue of \cref{it:nbpcq1} at the level of PDEs. Indeed, the following \cref{thm:2} is the analogue of \cref{thm:1} 

\begin{theorem}[Answer to \cref{it:nbpcq2} (the PDE analogue of \cref{it:nbpcq1})]\label{thm:2}
%Given $f\in L^2(\DR)$ such that $\supp \, f \subset \BR$, 
Let $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc}, and let $\Dm$, $\Ast$, and $\nst$ be such that $u^{(2)}$ exists
for any $f\in L^2(\DR)$ such that $\supp \, f \subset \BR$. 
Then, given $k_0>0$, there exists $C_3>0$, independent of $k$ and given explicitly in terms of $\Dm$, $\Aso$, and $\nso$ in \cref{eq:C3} below, such that
\beq\label{eq:PDEbound}
\frac{\big\|u^{(1)}-u^{(2)}\big\|_{\HokDR}
}{
\N{u^{(2)}}_{\HokDR}
}\leq C_3 \,k\, \max\set{\NLiDRRRdtd{\Aso-\Ast}\,,\, \NLiDRRR{\nso-\nst}}
\eeq
for all $k\geq k_0$. 
\end{theorem}

\ble[Sharpness of the bound \cref{eq:PDEbound} when $\Aso = \Ast= I$]\label{lem:sharp}
There exist $f, \,\nso$, and $\nst$ (with $\nso\neq \nst$) such that 
the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of \cref{prob:edp} with $\Aso = \Ast= I$ exist, are unique, and satisfy
\beq\label{eq:sharp1}
\frac{\N{u^{(1)}-u^{(2)}}_{\HokDR}
}{
\N{u^{(2)}}_{\HokDR}
}
\sim 
\frac{\N{u^{(1)}-u^{(2)}}_{L^2(\DR)}
}{
\N{u^{(2)}}_{L^2(\DR)}
}\sim k \NLiDRRR{\nso-\nst}.
\eeq
%\noi (ii) There exist $f, \Aso, \Ast$, (with $\Aso\not\equiv \Ast$), such that 
%the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of the exterior Dirichlet problem with $\nso \equiv \nst\equiv 1$ exist, are unique, and satisfy
%%There exist $f\in L^2(\DR), \Aj \in C^{0,1}(\DR)$, $j=1,2$ (with $\Aso\not\equiv \Ast$), such that the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of the exterior Dirichlet problem with $\nso \equiv \nst\equiv 1$ satisfy
%\beq\label{eq:sharp2}
%\frac{\N{u^{(1)}-u^{(2)}}_{\HokDR}
%}{
%\N{u^{(2)}}_{\HokDR}
%}
%\sim 
%\frac{\N{u^{(1)}-u^{(2)}}_{L^2(\DR)}
%}{
%\N{u^{(2)}}_{L^2(\DR)}
%}\sim k \big\|\Aso-\Ast\big\|_{L^\infty(\DR)}.
%\eeq
\ele

\bre[Physical interpretation for $k$-dependence]\label{rem:physical1k}
It is unsurprising that the condition \cref{eq:sufficientlysmall} is a sufficient condition to answer both \cref{it:nbpcq1} and \cref{it:nbpcq2}. Recall that $1/k$ is proportional to the wavelength $2\pi/k$ of the wave $u$ (at least when $A=I$ and $n=1$). As the wavelength is the natural length scale associated with the wave $u$, it is unsurprising that perturbations of size (up to) $1/k$ give bounded relative difference (in \cref{it:nbpcq2}) and bounded GMRES iterations for the nearby-preconditioned linear system (in \cref{it:nbpcq1}). In some sense, perturbations of size up to $1/k$ are `unseen' by the PDE or numerical method. Perturbations of order $1/k$ being `unseen' by the PDE can also be seen in bounds proved for $u$ where $n = \no + \eta,$ with $\no$ nontrapping and $\NLiDRRR{\eta} \lesssim 1/k,$ see \cref{rem:kdep} above.
\ere


\section{Numerical experiments}\label{sec:num}
\subsection{Investigating the sharpness of \cref{thm:1} and \cref{cor:1}}

\Cref{thm:1,cor:1} were stated for the exterior Dirichlet problem, but, as highlighted in \cref{sec:problem} and shown in \cref{sec:TEDP}, they hold also for the truncated exterior Dirichlet problem (TEDP) \cref{prob:vtedp}. The numerical experiments in this section seek to verify the analogues of \cref{thm:1} and \cref{cor:1} for the TEDP, and investigate their sharpness. More specifically, the experiments seek to verify whether the condition \cref{eq:cond} is:
\ben
\item sufficient, and
\item necessary
  \een
  for \emph{standard} GMRES applied to \cref{eq:pcsystem1} to converge in a number of iterations that is independent of $k.$

Based on the PDE results \cref{thm:2,lem:sharp} above, we expect that the condition \cref{eq:sufficientlysmall} is a necessary and sufficient condition for standard GMRES applied to \cref{eq:pcsystem1} to converge in a $k$-independent number of iterations, even though we can only prove this is a sufficient condition for \emph{weighted} GMRES. We expect this because \cref{eq:sufficientlysmall} is a sufficient condition for \cref{it:nbpcq2}, the PDE analogue of \cref{it:nbpcq1}.

To verify this expected behaviour, we perform numerical experiments for the setup described in \cref{app:compsetup} with $\Aso = I$ and $\nso = 1$. We define $f$ and $\gI$ to correspond to a plane wave incident from the bottom left passing through a homogeneous medium given by coefficients $\Aso$ and $\nso$. We perform experiments for $A$ and $n$ separately, i.e., first we perform experiments with $\Ast=I$ and $\nst$ varying, and then we perform experiments with $\Ast$ varying and $\nst=1.$

We define $\Ast$ and $\nst$ to be piecewise constant on a $10\times10$ square grid, with their values on each square chosen independently at random from a $\Unif\mleft(1-\alpha,1+\alpha\mright)$ distribution, with $\alpha \in (0,1)$ chosen as described below. For $\Ast,$ we also impose the restriction that $\Ast$ is (piecewise) positive-definite almost surely. We solve the linear systems \cref{eq:pcsystem1} for $k = 20,40,60,80,100$ using standard GMRES and record the number of GMRES iterations taken to achieve a (relative) tolerance of $10^{-5}$ (relative to $\Nt{\bfb}$).

We perform experiments for three different functional forms of $\alpha$  taking $\alpha = 0.5,\, 0.5/k^{1/2},$ and $ 0.5/k.$ We expect that when $\alpha = 0.5$ or $\alpha = 0.5/k^{1/2},$ the number of GMRES iterations required for convergence will increase as $k$ increases, whereas we expect that when $\alpha = 0.5/k$ the number of GMRES iterations required for convergence will remain bounded as $k$ increases, even though this behaviour has only been proven for $\NLiDRRRdtd{\Aso-\Ast}$ for weighted GMRES.


  For both $\NLiDRRRdtd{\Aso-\Ast}$ and $\NLiDRRR{\nso-\nst}$ we see the expected behaviour in all three cases. When $\alpha = 0.5$ we see growth in the number of GMRES iterations needed to achieve convergence, and when $\alpha = 0.5/k,$ we see that the number of GMRES iterations is bounded as $k$ increases. When $\alpha = 0.5/k^{1/2}$ we see slower growth as $k$ increases, (at least for $\Aso-\Ast$) but growth nonetheless.

  The behaviour for $\alpha = 0.5$ indicates that the effectiveness of this `nearby preconditioning' strategy deteriorates rapdily as $k$ increases when $\NLiDRRRdtd{\Aso-\Ast}$ and $\NLiDRRR{\nso-\nst}$ are independent of $k$. The behaviour for $\alpha = 0.5/k$ verifies that the condition \cref{eq:sufficientlysmall} is sufficient to achieve bounded GMRES iterations in this case, and the behaviour for $\alpha = 0.5/k^{1/2}$ indicates that this condition is sharp; reducing the magnitude of the difference betwen the Helmholtz problems as $k$ increases, but not at the rate $1/k$ is insufficient for the nearby preconditioning strategy to achieve $k$-independent GMRES iterations.
  \begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
\input{nbpc-linfinity-A-0.pgf}
\caption{GMRES iteration counts for $\alpha = 0.5$}\label{fig:linfinityA0}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
      \centering
\input{nbpc-linfinity-A-1.pgf}
  \caption{GMRES iteration counts for $\alpha = 0.5/k^{1/2}$}\label{fig:linfinityA1}
\end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering
\input{nbpc-linfinity-A-2.pgf}
  \caption{GMRES iteration counts for $\alpha = 0.5/k$}\label{fig:linfinityA2}
\end{subfigure}
\caption{GMRES iteration counts for $\AmatoI\Amatt$ where $\nso=\nst=1$ and $\NLiDRRRdtd{\Aso-\Ast} = \alpha$ as described in \cref{sec:num}.}
\end{figure}

  \begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
      \centering
\input{nbpc-linfinity-n-0.pgf}
\caption{GMRES iteration counts for $\alpha = 0.5$}\label{fig:linfinityn0}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
      \centering
\input{nbpc-linfinity-n-1.pgf}
  \caption{GMRES iteration counts for $\alpha = 0.5/k^{1/2}$}\label{fig:linfinityn1}
\end{subfigure}

    \begin{subfigure}{\textwidth}
      \centering
\input{nbpc-linfinity-n-2.pgf}
  \caption{GMRES iteration counts for $\alpha = 0.5/k$}\label{fig:linfinityn2}
\end{subfigure}
\caption{GMRES iteration counts for $\AmatoI\Amatt$ where $\Aso=\Ast=1$ and $\NLiDRRR{\nso-\nst} = \alpha$ as described in \cref{sec:num}.}
\end{figure}
\optodo{Need to figure out how to get computer modern consistently in axis labels. Some bookmarks saved that may help.}
  


%Say that these are for the TEDP defined in \cref{def:TEDP}.

\section{Definitions and conditions}\label{sec:3}

We now state the necessary technical definitions to prove \cref{cor:1,cor:1a} above.

\subsection{The variational problem and the Galerkin method}\label{sec:vpGm}
As this \lcnamecref{chap:nbpc} concerns finite-element discretisations of the Helmholtz equation, we will work with the variational formulation of \cref{prob:edp}, \cref{prob:vedp} above.

\bre[The EDP with data in $(\HokDR)'$]
In \cref{prob:edp} we defined the EDP with the antilinear functional $\LE$ arising from a function $f\in \LtDR$. In the rest of the \lcnamecref{chap:nbpc}, 
%\item In the rest of the paper, we usually consider the EDP with data given by $F$ defined in \cref{eq:EDPvar}, but sometimes 
we sometimes consider the EDP with general $\LE\in \HozDDRp$ and we indicate when this is the case.
In this latter situation, we define the dual norm by
\beq\label{eq:dualnorm}
\NHokDRp{L}= \sup_{v\in \HozDDR} \frac{\abs{\LE(v)}}{\NHokDR{v}}.
\eeq
%where $\|\cdot\|_{\HokDR}$ is defined by \cref{eq:1knorm}.
\ere

For the remainder of this \lcnamecref{chap:nbpc}, we let $(\Vhp)_{h>0}$ be a family of finite-dimensional, nested subspaces of $\HozDDR$, whose union is dense in $\HozDDR$. More specifically, we let $\Vhp$ consist of piecewise-polynomials on a simplicial mesh $\cTh$ with mesh-size $h$
%\ednote{Euan says: have problem that want to allow $C^{1,1}$ $\Dm$, so that statements later about $H^2$ regularity are covered, but easiest to define triangulation and hence subspaces on Lipschitz domains -- Euan to discuss with Ivan}
and fixed polynomial degree $p$. (Note that the dimension $N$ of $\Vhp$ then satisfies $N\sim h^{-d}$.) As in \cref{rem:crimes} above, we ignore any variational crimes resulting from this discretisation.

We now define the analogue of the finite-element approximation \cref{prob:fevtedp} for \cref{prob:vedp} with general data $L \in \HokDRp.$
\bprob[Finite-element approximation of \cref{prob:vedp} with general data]\label{prob:fevedpgen}
We say that $\uh \in \Vhp$ is the \defn{finite-element approximation of $u$} (the solution to \cref{prob:vedp} with general right-hand side $L \in \HokDRp$) if
\beq\label{eq:galerkin}
\aE(\uh,\vh) = \LE(\vh) \tforall \vh \in \Vhp.
\eeq
\eprob
Observe that implicit in our use of $\aE$ in \cref{eq:galerkin} is the fact that we are realising the Dirichlet-to-Neumann map $\TR$ exactly on $\GR.$
%Definition of Galerkin method

We now define the matrices associated with our finite-element discretisation. Let $\{\phi_i, i= 1, \ldots, N\}$ be a basis for $\Vhp$ with each $\phi_i$ \emph{real-valued}.
Let 
\beq\label{eq:matrixSjdef}
\big(\Smat_{A}\big)_{ij}:= \int_\Omega \big(A \nabla \phi_j)\cdot\nabla \phi_i, \quad
\big(\Mmat_{n}\big)_{ij}:= \int_\Omega n\,\phi_i\, \phi_j,
\quad\tand\quad
\big(\Nmat\big)_{ij}:= \int_{\GR} T_R (\gamma\phi_j) \,\gamma \phi_i
\eeq
be the stiffness, domain-mass, and boundary-mass matrices, respectively. Note that both $\Smat_A$ and $\Mmat_n$ are \emph{real-valued}, but $\Nmat$ is \emph{complex-valued} (because the DtN operator $T_R$ is complex-valued).
Let
\beq\label{eq:matrixAdef}
\Amat := \Smat_{A} - k^2 \Mmat_{n} - \Nmat,
\eeq
and let $u_h:= \sum_j u_j \phi_j$. Then \cref{eq:galerkin} implies that
\beqs
\Amat \bu = \bff,
\eeqs
where $(\bff)_i := \FE(\phi_i)$.
Similar to above we let 
\beq\label{eq:matrixAjdef}
\Amatj := \Smat_{A^{(j)}} - k^2 \Mmat_{n^{(j)}} - \Nmat.
\eeq
Finally, let 
\beq\label{eq:Dk2}
\Dmat_k:= \Smat_I + k^2 \Mmat_1;
\eeq
so that \cref{eq:Dk} holds.
%then the weighted norm $\|\cdot\|_{\Dmat_k}$ is given by 
%\beq\label{eq:Dk3}
%\N{\bv}_{\Dmat_k}^2:=   \N{v_h}^2_{\HokDR}=\big( \Dmat_k \bv,\bv\big)_2,
%\eeq
%for
%$v_h =\sum_i v_i \phi_i$.

We recall the definition of quasi-uniformity (c.f. \cite[Definition 4.4.13]{BrSc:08}):

\bde[Quasi-uniform]\label{def:quasiuniform}
Let $\set{\cTh}_{h>0}$ be a set of triangulations of $\DR$ indexed by their mesh size $h.$ For each $T \in \cTh,$ let $\BT$ denote the largest ball contained in $T$.
If there exists $\rho > 0$ such that 
\beqs
\min\set{\diam \BT \st T \in \cT} \geq \rho h,
\eeqs
then $\set{\cTh}_{h>0}$ is said to be \defn{quasi-uniform}.
\ede

\ble[Norm equivalences of FE functions]\label{lem:normequiv}
If $\set{{\cTh}_{h>0}}$ is quasi-uniform with a nodal basis, then
there exist $m_\pm$ and $s_\pm$, independent of $h$ and $p$, such that
\beq\label{eq:normequiv1}
m_- h^{d/2} \N{\bv}_2 \leq \N{v_h}_{\LtDR} \leq m_+ h^{d/2} \N{\bv}_2,
\eeq
and
\beq\label{eq:normequiv2}
s_- h^{d/2} \N{\bv}_2 \leq \N{\nabla v_h}_{\LtDR} \leq s_+ h^{d/2-1} \N{\bv}_2,
\eeq
for all finite-element functions $v_h =\sum_i v_i \phi_i \in \Vhp$.
\ele

Written in terms of the matrices $\Mmat_1$ and $\Smat_I$ defined in \cref{eq:matrixSjdef}, the bounds \cref{eq:normequiv1} and \cref{eq:normequiv2} are, respectively, the familiar bounds
\beqs
(\Mmat_1 \bv,\bv)_2 \sim h^d \N{\bv}^2_2 \quad\tand\quad h^{d}\N{\bv}^2_2 \lesssim (\Smat_I \bv,\bv)_2 \lesssim h^{d-2} \N{\bv}^2_2.
\eeqs

For a proof of \cref{lem:normequiv}, see\footnote{In \cite[Chapter V, Lemma 2.5] the assumption is made that the meshes underlying $\Vhp$ are \emph{uniform}. However, the definition of uniformity in \cite[Chapter 2, Definition 5.1(4)]{Br:07} is the same as the more standard definition of quasi-uniformity in \cref{def:quasiuniform}.} \cite[Chapter V, Lemma 2.5]{Br:07}.\ednote{Euan---in \cite[Chapter V, Lemma 2.5]{Br:07} this result is written slightly differently (with scaled basis functions and a scaled 2-norm) but it's equivalent to our result. I presume I don't need to put anything 'translating' their result to ours? (Can download the 3rd edition from the CUP website if you use your Bath institutional login.)}

%% \bpf[Sketch proof of \cref{lem:normequiv}]\opntodo{can omit this if can find a good reference. One possibility . Need to check basis scaling business.}
%% The inequalities in \cref{eq:normequiv1} follow from writing $\|v_h\|_{\LtDR}$ as a sum of integrals over elements of the mesh, and then mapping to the reference element \ednote{Euan to discuss with Ivan}.
%% %\beqs
%% %\N{v_h}^2_{L^2(\O
%% %\eeqs
%% Then, \cref{eq:normequiv2} follows from \cref{eq:normequiv1} and the inequalities
%% \beqs
%% \N{v_h}_{L^2(\DR)}\lesssim \N{\nabla v_h}_{L^2(\DR)}\lesssim \frac{1}{h} \N{v_h}_{L^2(\DR)},
%% \eeqs
%% the first of which follows from the Poincar\'e inequality, since $v_h \in \HozDDR$
%% (see, e.g., \cite[Proposition 5.3.4]{BrSc:00}), the second of which follows from a standard inverse estimate (see, e.g., \cite[Theorem 4.5.11]{BrSc:00}).
%% \epf


Finally, we need the concept of the \emph{adjoint} sesquilinear form to $a(\cdot,\cdot)$.
\begin{definition}[The adjoint sesquilinear form $a^*(\cdot,\cdot)$]\label{def:adjoint}
Let $\Dm$, $n$, and $A$, be as in the definition of the EDP (\cref{prob:edp}). The adjoint sesquilinear form, $a^*(\cdot,\cdot)$, to $a(\cdot,\cdot)$ defined in \cref{eq:aedp} is given by
\beq\label{eq:EDPadjoint}
a^*(\vo,\vt) := \int_{\DR} 
\Big((A \grad \vo)\cdot\grad \vtb
 - k^2 n \vo\vtb\Big) - \big\langle \gamma \vo,T_R(\gamma \vt)\big\rangle_{\GR}.
\eeq
\end{definition}

\noi It is then straightforward to check that
\beq\label{eq:A*}
\Amat^* := \Smat_A -k^2 \Mmat_n - \Nmat^*
\eeq
(where $^*$ denotes conjugate transpose) is the Galerkin matrix for the sesquilinear form $a^*(\cdot,\cdot)$; i.e.~$(\Amat^*)_{ij} = a^*(\phi_j, \phi_i)$.

\ble[Link between variational problems involving $a(\cdot,\cdot)$ and $a^*(\cdot,\cdot)$]\label{lem:adjoint}
Given $F\in (\HozDDR)'$, if $u$ is the solution to the variational problem
\beq\label{eq:adjoint1}
a^*(u,v)= F(v) \quad\tfa v\in \HozDDR,
\eeq
then $\overline{u}$ satisfies
\beq\label{eq:adjoint2}
a(\overline{u},v)= \overline{F(\overline{v})} \quad\tfa v\in \HozDDR.
\eeq
\ele\optodo{Consider putting something like this in Chap 2.}

For the proof of \cref{lem:adjoint} we need the following property of the DtN map $T_R$:
\beq\label{eq:DtN}
\big\langle T_R\psi, \overline{\phi} \big\rangle_\Gamma = \big\langle T_R \phi, \overline{\psi}\big\rangle_\Gamma \quad\tfa \phi,\psi \in H^{1/2}(\GR).
\eeq
This property follows from the fact that, if $\uo$ and $\ut$ are solutions of the homogeneous Helmholtz equation $\Delta u +k^2 u=0$ in $\RRd\setminus \overline{\BR}$, both satisfying the Sommerfeld radiation condition \cref{eq:src}, then
\beqs
\int_{\GR} (\gamma \uo)\, \dn \ut = \int_{\GR} (\gamma \ut)\,\dn \uo;
\eeqs
see, e.g., \cite[Lemma 6.13]{Sp:15}.

\bpf[Proof of \cref{lem:adjoint}]
From \cref{eq:adjoint1} we have that 
\beqs
\overline{a^*(u,\overline{v})}= \overline{F(\overline{v})} \quad\tfa v\in \HozDDR.
\eeqs
Using the definition of $a^*(\cdot,\cdot)$ and the property \cref{eq:DtN} in the left-hand side of this last equation, we find \cref{eq:adjoint2}.
\epf



\section{Proofs of the main results}\label{sec:proofs}

The main part of the proofs of \cref{cor:1,cor:1a} is the following \lcnamecref{thm:1}.

\begin{theorem}[Main ingredient of the answer to \cref{it:nbpcq1}]\label{thm:1}
Assume that $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc}, and assume that $h$ and $p$ satisfy \cref{cond:2}. 
Let the $k$- and $h$-independent constants $\mpm$ and $\spm$ be given as in \cref{lem:normequiv}.
Then, given $\kz>0$, there exist $\Co, \Ct>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $\kz$) such that
\begin{align}\nonumber
&\max\Big\{
\NDmatk{\Imat - (\Amat^{(1)})^{-1}\Amat^{(2)}}, 
\N{\Imat -\Amat^{(2)} (\Amat^{(1)})^{-1}}_{(\Dmat_k)^{-1}}
\Big\}\\
&\hspace{3cm} 
\leq C_1 \,k \,
\NLiDRRRdtd{\Aso-\Ast} + C_2 \, k \, \NLiDRRR{\nso-\nst}
\label{eq:main1}
\end{align}
and 
\begin{align}\nonumber
&\max\Big\{
\N{\Imat - (\Amat^{(1)})^{-1}\Amat^{(2)}}_2, 
\N{\Imat -\Amat^{(2)} (\Amat^{(1)})^{-1}}_2
\Big\}\\
&\hspace{0cm} 
\leq C_1 \,\left(\frac{s_+}{m_-}\right) \,\frac{1}{h} \,
\NLiDRRRdtd{\Aso-\Ast} + C_2 \, \left(\frac{m_+}{m_-} \right)k \, \NLiDRRR{\nso-\nst}
\label{eq:main1a}
\end{align}
for all $k\geq k_0$. 
\end{theorem}



\subsection{Proof of \cref{thm:1}} 

The following two lemmas are the heart of the proof of \cref{thm:1}.

\ble[Bounds on $(\Amato)^{-1} \Mmat_{n}$]\label{lem:keylemma1}
Assume that \cref{cond:1nbpc} holds, and assume that Part (i) of \cref{cond:2} holds. Then, for $n\in \LiDRRR$,
\beq\label{eq:keybound1}
\max\Big\{\big\| (\Amato)^{-1} \Mmat_{n} \big\|_{\Dmat_k}, \,\,
\big\|  \Mmat_{n}(\Amato)^{-1} \big\|_{(\Dmat_k)^{-1}}
\Big\}\leq 
C_2
%\frac{m_+}{m_-} \left[ C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}\right] 
\frac{\N{n}_{L^\infty(\DR)}}{k}
\eeq
and 
\beq\label{eq:keybound1a}
\max\Big\{\big\| (\Amato)^{-1} \Mmat_{n} \big\|_2, \,\,
\big\|  \Mmat_{n}(\Amato)^{-1} \big\|_2 
\Big\}\leq 
C_2 
%\frac{m_+}{m_-} \left[ C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}\right] 
\left(\frac{m_+}{m_-}\right) \frac{\N{n}_{L^\infty(\DR)}}{k}
\eeq
for all $k\geq k_0$,
where
\beq\label{eq:C2}
C_2:=%\frac{m_+}{m_-} 
%\left[ 
C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}.%\right].
\eeq
\ele

\ble[Bounds on $(\Amato)^{-1} \Smat_A$]\label{lem:keylemma2}
Assume that \cref{cond:1nbpc} holds, and assume that Part (ii) of \cref{cond:2} holds. Then, for $A\in L^\infty(\DR,\RR^{d\times d})$,
\beq\label{eq:keybound2}
\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_{(\Dmat_k)^{-1}}, \,\,
\big\| \Smat_A (\Amato)^{-1} \big\|_{\Dmat_k}\Big\} \leq C_1\, k\N{A}_{L^\infty(\DR)}
\eeq
and
\beq\label{eq:keybound2a}
\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_2, \,\,
\big\| \Smat_A (\Amato)^{-1} \big\|_2\Big\} \leq C_1\,\left(\frac{s_+}{m_-}\right) \frac{1}{h}\N{A}_{L^\infty(\DR)}
\eeq
%\begin{align}\nonumber
%&\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_2, \,\,
%\big\| \Smat_A (\Amato)^{-1} \big\|_2\Big\}\nonumber \\
%&\hspace{2cm}
% \leq \frac{s_+}{s_-} \left[ C_{\rm FEM2}^{(1)} + 
% \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right]k\N{A}_{L^\infty(\DR)}\label{eq:keybound2}
%% + C_{\rm bound}^{(1)}\right) \frac{\N{n}_{L^\infty(\DR)}}{k}.
%\end{align}
for all $k\geq k_0$, where
\beq\label{eq:C1nbpc}
C_1:=%\frac{s_+}{s_-} 
\left[ C_{\rm FEM2}^{(1)} + 
 \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right]
\eeq
\ele

\bpf[Proof of \cref{thm:1} using \cref{lem:keylemma1,lem:keylemma2}]
Using the definition of the matrices $\Amatj, \SmatA$, and $\Mmatn$ in \cref{eq:matrixAjdef} and \cref{eq:matrixSjdef}, we have
\begin{align}\nonumber
\Imat - (\Amato)^{-1}\Amatt = (\Amato)^{-1}\big(\Amato-\Amatt\big) &=  (\Amato)^{-1}\left( \Smat_{A^{(1)}} - \Smat_{A^{(2)}} - k^2 \big(\Mmat_{n^{(1)}}-\Mmat_{n^{(2)}}\big)\right)\\
&= (\Amato)^{-1}\left( \Smat_{A^{(1)}-A^{(2)}} - k^2 \Mmat_{n^{(1)}-n^{(2)}}\right),\label{eq:idea1}
\end{align}
and similarly 
\beq\label{eq:idea2}
\Imat -\Amatt  (\Amato)^{-1}= \left( \Smat_{A^{(1)}-A^{(2)}} - k^2 \Mmat_{n^{(1)}-n^{(2)}}\right)(\Amato)^{-1}.
\eeq
The bounds  \cref{eq:main1} on $\|\Imat - (\Amato)^{-1}\Amatt\|_2$ and  $\|\Imat - \Amatt(\Amato)^{-1}\|_2$ then follow from using the bounds \cref{eq:keybound1} and \cref{eq:keybound2} in \cref{eq:idea1} and \cref{eq:idea2}.
%
%, and $C_1$, $C_2$ in \cref{eq:main1} are given explicitly by
%\beq\label{eq:C1nbpc}
%C_1:=%\frac{s_+}{s_-} 
%\left[ C_{\rm FEM2}^{(1)} + 
% \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right] \,\,\tand\,\,
%\quad C_2:=  %+ \frac{m_+}{m_-} 
% \left[ C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}\right].
%\eeq
\epf

\

\bpf[Proof of \cref{lem:keylemma1}]
We first concentrate on proving \cref{eq:keybound1}.
Given $\bff \in \CC^N$ and $n\in \LiDRRR$, we create a variational problem whose Galerkin discretisation leads to the equation $\Amato \tbu = \Mmat_n\,\bff$.
Indeed, let $\widetilde{f} := \sum_j f_j \phi_j\in \HozDDR$. Define $\widetilde{u}$ to be the solution of the variational problem 
\beq\label{eq:411}
a^{(1)}(\widetilde{u},v)= (n\widetilde{f},v)_{L^2(\Omega)} \quad\text{ for all } v\in H^1(\Omega),
\eeq
and let $\tu_h$ be the solution of the finite-element approximation of \cref{eq:411}, i.e.,
\beq\label{eq:41}
a^{(1)}(\tu_h,v_h)= (n\widetilde{f},v_h)_{L^2(\Omega)} \quad\text{ for all } v_h\in \Vhp,
\eeq
and let $\tbu$ be the vector of nodal values of $\tu_h$. The definition of $\widetilde{f}$ then implies that \cref{eq:41} is equivalent to the linear system $\Amato \tbu = \Mmat_{n}\,\bff$, and so to obtain a bound on $\|(\Amato)^{-1}\Mmat_n\|_{\Dmat_k}$ we need to bound $\|\tbu\|_{\Dmat_k}$ in terms of $\|\bff\|_{\Dmat_k}$. (Recall $\bff \in \CCN$ was arbitrary.) Because of the definition 
of $\|\cdot\|_{\Dmat_k}$ in \cref{eq:Dk}, this is bound equivalent to bounding $\|\tu_h\|_{\HokDR}$ in terms of $\|\widetilde{f}\|_{\HokDR}$.

%First observe that the bound \cref{eq:bound3} from Part (i) of \cref{cond:2} holds for the solution of the variational problem
%\beqs%\label{eq:411}
%a^{(1)}(u,v)= (n\phi_j,v)_{L^2(\Omega)} \quad\text{ for all } v\in H^1(\Omega),
%\eeqs
%and hence, by linearity, it also holds for the solution $\widetilde{u}$ of the variational problem \cref{eq:411}.

Using %the bounds in \cref{eq:normequiv1}, 
the triangle inequality and the bounds \cref{eq:bound3} and \cref{eq:bound1} from \cref{cond:2,cond:1nbpc} respectively, we find
%Note that the hypotheses imply that the bound on the solution operator 
%\cref{eq:bound_unif} holds (by \cref{cor:uniform}), and also that if $h k\sqrt{|k^2-\eps|} \leq C_1$ then quasi-optimality \cref{eq:qoeps_lemma} holds (by \cref{lem:qo}).
%Starting with \cref{eq:equiv} we then have 
\begin{align}
%m_- h^{d/2}k \N{\tbu}_2 \leq k\N{\tu_h}_{\LtDR}\leq  
\N{\tu_h}_{\HokDR} \leq
\N{\tu-\tu_h}_{\HokDR} + \N{\tu}_{\HokDR} \label{eq:mainevent1}
& \leq C^{(1)}_{\rm FEM1}\NLtDR{n\ftilde} + C^{(1)}_{\rm bound}\NLtDR{n\ftilde} \\ 
& \leq \mleft(C^{(1)}_{\rm FEM1} + C^{(1)}_{\rm bound}\mright)\NLiDRRR{n}\NLtDR{\ftilde} \label{eq:mainevent1a} \\
& \leq\big(C^{(1)}_{\rm FEM1}+  C^{(1)}_{\rm bound}\big)\NLiDRRR{n}\frac{\big\|\widetilde{f}\big\|_{\HokDR}}{k};\nonumber
%& \leq\big(C^{(1)}_{\rm FEM1}+  C^{(1)}_{\rm bound}\big)\N{n}_{L^\infty(\DR)} m_+ h^{d/2} \N{\bff}_2,
\end{align}
the bound on $\|(\Amato)^{-1}\Mmat_n\|_{\Dmat_k}$ in \cref{eq:keybound1} then follows from the definition of $\|\cdot\|_{\Dmat_k}$ in \cref{eq:Dk} and the definition of $C_2$ \cref{eq:C2}.

To prove the bound on $\|\Mmat_n(\Amato)^{-1}\|_{(\Dmat_k)^{-1}}$ in \cref{eq:keybound1}, first observe that the definitions of $\|\cdot\|_{\Dmat_k}$ and $\|\cdot\|_{(\Dmat_k)^{-1}}$ in \cref{eq:Dk} imply that, for any matrix $\Cmat \in \CCNtN$ and for any $\bv\in \CC^N$,
\beq\label{eq:A380-0}
\frac{
\big\|\matrixC \bv \big\|_{(\Dmat_k)^{-1}}
}{
\big\|\bv\|_{(\Dmat_k)^{-1}}
} = 
\frac{
\big\|\matrixC^* \bw \big\|_{\Dmat_k}
}{
\big\|\bw\|_{\Dmat_k}
}
\eeq
where $\bw := (\Dmat_k)^{1/2}\bv$, and where $\matrixC^*$ is the conjugate transpose of $\matrixC$ (i.e.~the adjoint with respect to $(\cdot,\cdot)_2$).
Therefore, since $\Mmat_n$ is a real, symmetric matrix,
\beqs
\frac{
\big\|\Mmat_n (\Amato)^{-1}\bv\big\|_{(\Dmat_k)^{-1}}
}{
\N{\bv}_{(\Dmat_k)^{-1}}
}
=
\frac{\NDk{\mleft(\AmatoI\Mmatn\mright)^* \bw}}{\NDk{\bw}}
= 
\frac{
\big\|((\Amato)^*)^{-1}\Mmat_n\bw\big\|_{\Dmat_k}
}{
\N{\bw}_{\Dmat_k}
},
 \eeqs
 so that 
\beq\label{eq:A380} 
 \big\|\Mmat_n (\Amato)^{-1}\big\|_{(\Dmat_k)^{-1}}=\big\|((\Amato)^*)^{-1}\Mmat_n\big\|_{\Dmat_k}.
 \eeq 
Recall from the text below \cref{eq:A*} that $(\Amato)^*$ is the Galerkin matrix corresponding to the variational problem \cref{eq:adjoint1} -- the adjoint problem. \cref{lem:adjoint} implies that if the EDP %with coefficients $A^{(1)}$ and $n^{(1)}$ 
satisfies \cref{cond:1nbpc,cond:2}, then so does the adjoint problem. Therefore, the argument above leading to the bound on $\|(\Amato)^{-1}\Mmat_n\|_{\Dmat_k}$ under \cref{cond:1nbpc} and Part (i) of \cref{cond:2} proves the same bound on $\|((\Amato)^*)^{-1}\Mmat_n\|_{\Dmat_k}$, and then, using \cref{eq:A380}, also on $\big\|\Mmat_n(\Amato)^{-1}\big\|_{(\Dmat_k)^{-1}}$.

To prove the bound on  $\|(\Amato)^{-1}\Mmat_n\|_{2}$ in \cref{eq:keybound1a}, we use the bounds 
\beqs
m_- h^{d/2} k \N{\tbu}_2 \leq k \N{\widetilde{u}_h}_{\LtDR} \leq \N{\widetilde{u}_h}_{\HokDR}
\,\tand\,
\big\|\widetilde{f}\big\|_{\LtDR} \leq m_+ h^{d/2}\N{\bff}_2,
\eeqs
on either side of the inequality \cref{eq:mainevent1}, with these bounds coming from \cref{eq:normequiv1}. The proof of the bound on 
$\|\Mmat_n((\Amato)^*)^{-1}\|_{2}$ in \cref{eq:keybound1a} follows in a similar way to above, using the fact that 
$\|\Mmat_n (\Amato)^{-1}\|_2=\|((\Amato)^*)^{-1}\Mmat_n\|_2$ (compare to \cref{eq:A380}).
%, namely the variational problem \cref{eq:EDPvar} with the operator $T_R$ in $a^{(1)}(\cdot,\cdot)$ replaced by $\overline{T_R}$ (corresponding to the $-\ri k$ in the radiation condition \cref{eq:src} being changed to $+\ri k$).
%
%Now, if $u$ is the solution of the adjoint problem with data $F(v)$, then $\overline{u}$ is the solution of the original problem with data $\overline{F(\overline{v})}$; 
%
%in particular if $F(v)$ is as in \cref{eq:EDPa}, then the $L^2$ data of the adjoint problem is just $\overline{f}$. Therefore, if the EDP satisfies \cref{cond:1nbpc,cond:2}, then so does its adjoint, and
% the bound in \cref{eq:keybound1} on $\|(\Amato)^{-1}\Mmat_n\|_{2}$ also holds for $\|((\Amato)^*)^{-1}\Mmat_n\|_{2}$.
\epf

The proof of \cref{lem:keylemma2} uses the following \lcnamecref{lem:H1}, which one can prove sing the G\aa rding inequality \cref{eq:gardingbrief}; see \cite[Lemma 5.1]{GrPeSp:19}.

\ble[Bound for data in $\HozDDRs$]\label{lem:H1}
%With the sesquilinear form $a(\cdot,\cdot)$ defined by \cref{eq:EDPa} with $A=\Aso$ and $n=\nso$, 
Given $\widetilde{F}\in \HozDDRs$, let $\widetilde{u}$ be the solution of the variational problem
\beqs
\text{ find } \,\,\widetilde{u} \in H^1_{0,D}(\DR) \,\,\tst \,\,
a^{(1)}(\widetilde{u},v)=\widetilde{F}(v) \,\, \tfa v\in H^1_{0,D}(\DR).
\eeqs
If \cref{cond:1nbpc} holds, then $\widetilde{u}$ exists, is unique, and satisfies the bound
\beq\label{eq:bound2}
\N{\widetilde{u}}_{\HokDR} \leq \frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomax  k\right) \big\|\widetilde{F}\big\|_{(\HokDR)'}
\eeq
for all $k\geq k_0$.
\ele
Observe that, similar to \cref{rem:yesitis}, \cref{eq:bound2} is a $k$-independent bound, due to the norm $\NHokDRs{F}$ on the right-hand side.


\bpf[Proof of \cref{lem:keylemma2}]
In a similar way to the proof of \cref{lem:keylemma1}, given $\bff \in \CC^N$ and a symmetric $A\in L^\infty(\DR, \RR^{d\times d})$, let $\widetilde{f} := \sum_j f_j \phi_j$ and observe that $\widetilde{f} \in \HozDDR$. Define $\widetilde{u}$ to be the solution of the variational problem 
\beq\label{eq:411a}
a^{(1)}(\widetilde{u},v)= \widetilde{F}(v) \quad\text{ for all } v\in H^1(\Omega),
\quad\text{ where } \quad
 \widetilde{F}(v) :=(A\nabla\widetilde{f},\nabla v)_{L^2(\Omega)}.
\eeq
Observe that the definition of the norms $\|\cdot\|_{(\HokDR)'}$ \cref{eq:dualnorm} and $\|\cdot\|_{\HokDR}$ \cref{eq:weightednorm} and the Cauchy-Schwarz inequality imply that
\begin{align}
\big\| \widetilde{F}\big\|_{(\HokDR)'}&\leq \big\|A\nabla \widetilde{f}\big\|_{\LtDR}\nonumber\\
&\leq \NLiDRRRdtd{A} \big\|\nabla \widetilde{f}\big\|_{\LtDR}\label{eq:Fbounda}\\
&\leq \NLiDRRRdtd{A} \big\| \widetilde{f}\big\|_{\HokDR}.\label{eq:Fbound}
\end{align}
Let $\tu_h$ be the solution of the finite element approximation of \cref{eq:411a}, i.e.,
\beq\label{eq:41a}
a^{(1)}(\tu_h,v_h)= \widetilde{F}(v_h) \quad\text{ for all } v_h\in \Vhp,
\eeq
and let $\tbu$ be the vector of nodal values of $\tu_h$. The definition of $\widetilde{f}$ then implies that \cref{eq:41a} is equivalent to $\Amato \tbu = \Smat_A\,\bff$. 

Similar to the proof of \cref{lem:keylemma1},
using the triangle inequality, the bound \cref{eq:bound4} from \cref{cond:2}, the bound \cref{eq:bound2} from \cref{lem:H1}, the bound \cref{eq:Fbound}, and the definition of $C_1$ \cref{eq:C1nbpc},
we find
%Note that the hypotheses imply that the bound on the solution operator 
%\cref{eq:bound_unif} holds (by \cref{cor:uniform}), and also that if $h k\sqrt{|k^2-\eps|} \leq C_1$ then quasi-optimality \cref{eq:qoeps_lemma} holds (by \cref{lem:qo}).
%Starting with \cref{eq:equiv} we then have 
\begin{align}\nonumber 
%s_- h^{(d-2)/2} \N{\tbu}_2 &\leq \N{\nabla \tu_h}_{\LtDR}\leq  
\N{\tu_h}_{\HokDR} &\leq
\N{\tu-\tu_h}_{\HokDR} + \N{\tu}_{\HokDR},\nonumber \\ \nonumber
& \leq \left[ C^{(1)}_{\rm FEM2} k + 
\frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomax k  \right) 
\right]\big\|\widetilde{F}\big\|_{(\HokDR)'},\\
&\leq C_1 \, k\, 
%\left[C^{(1)}_{\rm FEM2} k + \frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomaxk  \right) \right]
\NLiDRRRdtd{A} \big\|\nabla\widetilde{f}\big\|_{\LtDR},\label{eq:mainevent2}\\
&\leq C_1 \, k\, 
%\left[C^{(1)}_{\rm FEM2} k + \frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomaxk  \right) \right]
\NLiDRRRdtd{A} \big\|\widetilde{f}\big\|_{\HokDR},\nonumber
%&\leq \left[ C^{(1)}_{\rm FEM2} k + 
%\frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomaxk  \right) 
%\right]\big\|A\big\|_{L^\infty(\DR)}s_+ h^{(d-2)/2} \N{\bff}_2,
\end{align}
and the bound on $\|(\Amato)^{-1}\Smat_A\|_{\Dmat_k}$ in \cref{eq:keybound2} follows.

The bound on $\|\Smat_A(\Amato)^{-1}\|_{(\Dmat_k)^{-1}}$ follows in a similar way to how we obtained the 
bound on  $\|(\Amato)^{-1}\Mmat_n\|_{(\Dmat_k)^{-1}}$ from the bound on $\|\Mmat_n(\Amato)^{-1}\|_{\Dmat_k}$ in Part (i). Indeed, 
\cref{eq:A380-0} and the fact that $\Smat_A$ is a real, symmetric matrix imply that 
\beq\label{eq:A380-2} 
 \big\|\Smat_A (\Amato)^{-1}\big\|_{(\Dmat_k)^{-1}}=\big\|\big((\Amato)^*\big)^{-1}\Smat_A\big\|_{\Dmat_k}
 \eeq 
%since 
%\beqs
%\big\|\Smat_A(\Amato)^{-1}\big\|_{2}=\big\|(\Smat_A(\Amato)^{-1})^*\big\|_{2}=\big\|((\Amato)^*)^{-1}\Smat_A\big\|_{2},
%\eeqs
(c.f. \cref{eq:A380}),
and then the arguments in the proof of part (i) imply that 
the bound in \cref{eq:keybound2} on $\|(\Amato)^{-1}\Smat_A\|_{\Dmat_k}$ also holds for $\|((\Amato)^*)^{-1}\Smat_A\|_{\Dmat_k}$.

To prove the bound on  $\|(\Amato)^{-1}\Smat_A\|_{2}$ in \cref{eq:keybound2a}, we use the bounds 
\beqs
m_- h^{d/2} k \N{\tbu}_2 \leq k \N{\widetilde{u}_h}_{\LtDR} \leq \N{\widetilde{u}_h}_{\HokDR}
\,\tand\,
\big\|\nabla \widetilde{f}\big\|_{\LtDR} \leq s_+ h^{d/2-1}\N{\bff}_2,
\eeqs
on either side of the inequality \cref{eq:mainevent2}, with these bounds coming from \cref{eq:normequiv1}.and \cref{eq:normequiv2} respectively. The proof of the bound on 
$\|\Smat_A((\Amato)^*)^{-1}\|_{2}$ in \cref{eq:keybound2a} follows in a similar way to above, using \cref{eq:Fbound}.
\epf

\bre[Link to the results of \cite{GaGrSp:15}]
A result analogous to the Euclidean-norm bounds in \cref{thm:1} was proved in \cite{GaGrSp:15} for the case that $\Aso= \Ast= I$, $\nst= 1$, and $\nso = 1 + \ri\eps/k^2$, with the `absorption parameter' or `shift' $\eps$ satisfying $0<\eps\lesssim k^2$. The motivation for proving this result was that the so-called `shifted Laplacian preconditioning' of the Helmholtz equation is based on preconditioning (with these choices of parameters) $\Amatt$ with an approximation of $\Amato$. Similar to \cref{cor:1}, bounds on $\|\Imat -  (\Amato)^{-1}\Amatt \|_2$ and 
$\|\Imat - \Amatt  (\Amato)^{-1}\|_2$
 then give upper bounds on large the `shift' $\eps$ can be for GMRES to converge in a $k$-independent number of iterations in the case when the action of $(\Amato)^{-1}$ is computed exactly.

%\cite[Lemma 4.1]{GaGrSp:15}
The main differences between \cite{GaGrSp:15} and the present paper are that (i)  \cite{GaGrSp:15} focused on the TEDP, not the EDP,
(ii) \cite{GaGrSp:15} focused on the particular case that $\Dm$ is star-shaped with respect to a ball, finding a $k$- and $\eps$-explicit expression for $C^{(1)}_{\rm bound}$ in this case using Morawetz identities, whereas we assume the existence of $\Cboundo,$
(iii) \cite{GaGrSp:15} required a bound on 
$(\Amato)^{-1}\Mmat_{n}$, analogous to the bounds in \cref{lem:keylemma1} along with one on $(\Amato)^{-1}\Nmat$ (in the case that $T_R$ is approximated by $\ri k$), but \emph{not} on 
$(\Amato)^{-1}\Smat_{A}$, and (iv) \cite{GaGrSp:15} only proved bounds in the $\|\cdot\|_2$ norm.
%The result of \cref{thm:1}
\ere

%\bre[Analogue of \cref{thm:1} in a weighted norm]\label{rem:weight1}
%The PDE analysis of the Helmholtz equation naturally takes place in the weighted $H^1$ norm $\|\cdot\|_{\HokDR}$ defined by \cref{eq:1knorm}. The discrete analogue of this norm is the norm $\|\cdot\|_{\Dmat_k}$ defined by 
%\beq\label{eq:Dk}
%\N{\bv}_{\Dmat_k}^2:= \big( (\Smat_I + k^2 \Mmat_1)\bv,\bv\big)_2 = \N{v_h}^2_{\HokDR}
%\eeq
%for
%$v_h =\sum_i v_i \phi_i$. 
%This norm is used, e.g., in recent results about the convergence of domain-decomposition methods %in this norm are proved 
%for the Helmholtz equation \cite{GrSpVa:17}, \cite{GrSpZo:18}, and for the time-harmonic Maxwell equations \cite{BoDoGrSpTo:19}. 
%
%Inspecting the proof of \cref{lem:keylemma}, we see that the bounds \cref{eq:keybound1} and \cref{eq:keybound2} hold with the $\|\cdot\|_2$ norm replaced by the $\|\cdot\|_{\Dmat_k}$ norm and without the terms involving $m_\pm$ and $s_\pm$ on the right-hand side. \cref{thm:1} 
%%(and also \cref{cor:1}) 
%therefore also holds with the $\|\cdot\|_2$ norm replaced by the $\|\cdot\|_{\Dmat_k}$ norm and the constant $C_1$ modified appropriately.
%\ere

\subsection{Proof of \cref{cor:1,cor:1a}}

We first give the set-up for weighted GMRES.
Consider the abstract  linear system 
% \begin{equation*}
$\matrixC \bx = \bd$
%\end{equation*}
in $\mathbb{C}^N$, where $\matrixC \in \CC^{N\times N}$ is invertible.   
Given an initial guess $\bx^0$, we introduce the residual $\br^0 := \bd- C \bx^0$ and 
the usual Krylov spaces:  
\beqs  
\cK^m(C, \br^0) := \mathrm{span}\big\{\matrixC^j \br^0 : j = 0, \ldots, m-1\big\}.
\eeqs
Let $(\cdot , \cdot )_{\Dmat}$ denote the inner product on $\CC^n$ 
induced by some Hermitian positive-definite matrix $\Dmat$, i.e.~
%\begin{equation*}
$(\bv,\bw)_{\Dmat} := (\Dmat \bv, \bw)_2,$
%\end{equation*} 
with induced norm $\Vert \cdot \Vert_\Dmat$. For $m \geq 1$, define   $\bx^m$  to be  the unique element of $\cK^m$ satisfying  the  
 minimal residual  property: 
$$ \ \Vert \br^m \Vert_\Dmat := \Vert \bd - \matrixC \bx^m \Vert_\Dmat \ = \ \min_{\bx \in \cK^m(C, \br^0)} \Vert {\bd} - {\matrixC} {\bx} \Vert_\Dmat. $$
When $\Dmat = \Imat$ this is just the usual GMRES algorithm, but for  more general  $\Dmat$ it 
is the weighted GMRES method \cite{Es:98} in which case  
its implementation requires the application of the weighted Arnoldi process \cite{GuPe:14}.
Let 
\beq\label{eq:fov}
W_\Dmat(\matrixC):= \Big\{ (\matrixC \bx, \bx)_{\Dmat} : \bx \in \CCN, \|\bx\|_\Dmat=1\Big\};
\eeq
$W_\Dmat(\matrixC)$ is called the \emph{numerical range} or \emph{field of values} of $\matrixC$ (in the $(\cdot,\cdot)_\Dmat$ inner product).

%Recall the so-called ``Elman estimate" for GMRES

\begin{theorem}[Elman estimate for weighted GMRES]\label{thm:GMRES1_intro} 
Let $\matrixC$ be a matrix with $0\notin W(\matrixC)$. Let $\beta\in[0,\pi/2)$ be defined such that
\beq\label{eq:cosbeta}
\cos \beta := \frac{\mathrm{dist}\big(0, W(\matrixC)\big)}{\N{\matrixC}_{2}}.
\eeq
If the matrix equation $\matrixC \bx = \by$ is solved using weighted GMRES then, 
for $m\in \mathbb{N}$, the GMRES residual $\br_m$ %:= \matrixC \bx_m - \by$ 
satisfies
\beq\label{eq:Elman}
\frac{\N{\bfr_m}_{\Dmat}}{\N{\bfr_0}_{\Dmat}} \leq \sin^m \beta. %, \quad \text{ where}\quad 
\eeq
\end{theorem}
The bound \cref{eq:Elman} with $\Dmat=\Imat$ was originally proved in \cite{El:82} (see also \cite[Theorem 3.3]{EiElSc:83}) and appears in the form above in \cite[Equation 1.2]{BeGoTy:06}. The bound \cref{eq:Elman} (for arbitrary Hermitian positive-definite $\Dmat$) was stated (without proof) in \cite{CaWi:92} and proved in \cite[Theorem 5.1]{GrSpVa:17}. % (see also \cite[Remark 5.2]{GrSpVa:17}). 



\cref{thm:GMRES1_intro} has the following corollary, and the proofs of \cref{cor:1,cor:1a} follow from combining this with \cref{thm:1}.

\begin{corollary}
\label{cor:GMRES_intro} 
If $\|\Imat - \matrixC \|_\Dmat \leq \alpha < 1$, then, with $\beta$ defined as in \cref{eq:cosbeta},
\beqs
\cos \beta \geq \frac{1-\alpha}{1+\alpha}\eeqs
and
\beq\label{eq:gmressin}
\sin \beta \leq \frac{2 \sqrt{\alpha}}{(1+\alpha)^2}.
\eeq
\end{corollary}

\bpf[Proof of \cref{cor:1}]
This follows from \cref{thm:1} by applying \cref{cor:GMRES_intro} first with $\matrixC= (\Amato)^{-1} \Amatt$, $\Dmat=\Dmat_k$, and $\alpha=1/2$, and then with $\matrixC= \Amatt(\Amato)^{-1} $, $\Dmat=(\Dmat_k)^{-1}$, and $\alpha=1/2$.
\epf

\

\bpf[Proof of \cref{cor:1a}]
This follows from \cref{thm:1} by applying \cref{cor:GMRES_intro} first with $\matrixC= (\Amato)^{-1} \Amatt$, $\Dmat=\Imat$, and $\alpha=1/2$, and then with $\matrixC= \Amatt(\Amato)^{-1} $, $\Dmat=\Imat$, and $\alpha=1/2$.
\epf


\bre[The improvement of the Elman estimate \cref{eq:Elman} in \cite{BeGoTy:06}]
A stronger result than \cref{eq:Elman} is given for standard (unweighted) GMRES in \cite[Theorem 2.1]{BeGoTy:06}, and then converted to a result about weighted GMRES in \cite[Theorem 5.3]{BoDoGrSpTo:19}; indeed, the convergence factor $\sin \beta$ is replaced by a function of $\beta$ strictly less than $\sin\beta$ for $\beta\in (0,\pi/2)$. Using this stronger result, however, does not improve the $k$-dependence of \cref{cor:1}.
\ere


%\section{Proof of }\label{sec:proofPDE}

\subsection{Proofs of \cref{thm:2}, and \cref{lem:sharp}}

\bpf[Proof of \cref{thm:2}]
%We first prove the upper bound \cref{eq:PDEbound}.
By \cref{prob:edp}, $u^{(1)}$ and $u^{(2)}$ satisfy $a^{(1)}(u^{(1)}, v) = F(v)$ and 
$a^{(2)}(u^{(2)}, v) = F(v)$ respectively. Subtracting these equations, we find that $u^{(1)}- u^{(2)}$ satisfies the variational problem
\beq\label{eq:vp1}
a^{(1)}(u^{(1)}-u^{(2)},v) = \widetilde{F}(v) \quad\tfa v\in H^1_{0,D}(\DR)
\eeq
where
\beqs
 \widetilde{F}(v):= \int_{\DR} \left((\Ast-\Aso) \nabla u^{(2)}\right) \cdot\overline{\nabla v} + k^2 (\nso-\nst) u^{(2)}\overline{v}.
\eeqs
Now, by the Cauchy-Schwarz inequality and the definition of the norm $\|\cdot\|_{\HokDR}$ \cref{eq:weightednorm}, we have that
\begin{align*}
| \widetilde{F}(v)| &\leq \big\|\Aso-\Ast\big\|_{L^\infty(\DR)} \big\|\nabla u^{(2)}\big\|_{L^2(\DR)}
\N{\nabla v}_{L^2(\DR)} 
\\& \hspace{5cm}+ k^2 
\big\|\nso-\nst\big\|_{L^\infty(\DR)} \big\| u^{(2)}\big\|_{L^2(\DR)}
\N{v}_{L^2(\DR)}\\
&\leq\max\Big\{\big\|\Aso-\Ast\big\|_{L^\infty(\DR)}\,,\, \big\|\nso-\nst\big\|_{L^\infty(\DR)}\Big\}
\big\| u^{(2)}\big\|_{\HokDR} \N{v}_{\HokDR}.
\end{align*}
and thus, by the definition of the norm $\|\cdot\|_{(\HokDR)'}$ \cref{eq:dualnorm},
\beqs
\big\|\widetilde{F}\big\|_{(\HokDR)'}\leq \max\Big\{\big\|\Aso-\Ast\big\|_{L^\infty(\DR)}\,,\, \big\|\nso-\nst\big\|_{L^\infty(\DR)}\Big\}
\big\| u^{(2)}\big\|_{\HokDR}.
\eeqs
Since \cref{cond:1nbpc} holds, we can then apply the result of \cref{lem:H1}, i.e.~the bound \cref{eq:bound2}, to the solution of the variational problem \cref{eq:vp1}  to find that 
\begin{align*}
\frac{\big\| u^{(1)} - u^{(1)}\big\|_{\HokDR}}
{\big\| u^{(2)}\big\|_{\HokDR}, 
}
 \leq 
\,&\frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomax  k\right)
\\
&\quad\times \left(\max\Big\{\big\|\Aso-\Ast\big\|_{L^\infty(\DR)}\,,\, \big\|\nso-\nst\big\|_{L^\infty(\DR)}\Big\}\right),
\end{align*}
and then the result \cref{eq:PDEbound} follows with 
\beq\label{eq:C3}
C_3:= \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right).
\eeq
\epf

\bpf[Proof of \cref{lem:sharp}]
We actually prove the stronger result that given any function $c(k)$ such that $c(k)>0$ for all $k>0$, there exist 
$f, \nso, \nst$ (with $\nso\not= \nst$), such that, firstly,
\beqs
\big\|\nso-\nst\big\|_{L^\infty(\DR)} \sim c(k)
\eeqs
and, secondly,
the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of the exterior Dirichlet problem with $\Aso = \Ast= I$ exist, are unique, and satisfy \cref{eq:sharp1}. 

The heart of the proof is the equation
\beq\label{eq:obs1}
(\Delta + k^2) \big(\re^{\ri k r}\chi(r)\big) =  \re^{\ri k r}\left(\ri k \frac{d-1}{r} \chi(r) + 2 \ri k \diff{\chi}{r}(r) + \Delta \chi(r)\right)=: -\widetilde{f}(r),
\eeq
where $\chi(r)$ is chosen to have $\supp \chi \subset \DR$.
This equation proves the sharpness of the nontrapping resolvent estimate \cref{eq:bound1}, since both the $L^2(\DR)$ norm of $\widetilde{f}$ and the $\HokDR$ norm of $\re^{\ri kr}\chi(r)$ are proportional to $k$, and hence each to other (see, e.g., \cite[Lemma 3.10]{ChMo:08},  \cite[Lemma 4.12]{Sp:14}).

The overall idea of the proof is to set things up so that $(u^{(1)}-u^{(2)})(\bx) = \re^{\ri k r}\chi(r)$, the rationale being that \cref{eq:obs1} proves the sharpness of \cref{eq:bound1}, and \cref{eq:bound1} and its corollary \cref{eq:bound2} (applied to $u^{(1)}-u^{(2)}$) are the main ingredients in the proof of \cref{thm:2}.
% to prove the sharpness of \cref{thm:2} (at least when $\Aj:=I$, $j=1,2,$), 

Observe that, when $\Aj:=I$, $j=1,2,$ and $\nso:=1$, the variational problem \cref{eq:vp1} implies that 
\beq\label{eq:obs2}
\Delta \big( u^{(1)} - u^{(2)}\big) + k^2 \big( u^{(1)} - u^{(2)}\big) = -k^2 \big(1-\nst\big)u^{(2)}.
\eeq
Let $\nst:= 1 + c(k)\widetilde{\chi}$ with $\widetilde{\chi}= \widetilde{\chi}(r)$, $\widetilde{\chi}\in C^{\infty}(\DR)$, $\widetilde{\chi}\not = 1$ (so that $\nst\not = \nso$), and 
 $\supp \, \widetilde{\chi} \subset\DR$. 
%observe then that $\nst(\bx) >1$ for all $\bx \in \DR$ and thus, in particular, %for some function $c(k)>0$ for all $k>0$ 
%$\nst\not = \nso$. 
As above, let $\chi=\chi(r)$ with $\chi \in C^{\infty}(\DR)$ and
$\supp \,\chi$ a compact subset of $\DR$. Then with $\widetilde{f}$ defined in \cref{eq:obs1}, our goal is to let 
\beq\label{eq:obs3}
u^{(2)}(\bx):= -\frac{1}{k^2 c(k)}\frac{\widetilde{f}(r)}{\widetilde{\chi}(r)} \quad\tand\quad  f(\bx):= -\big(\Delta +k^2 \nst(\bx)\big) u^{(2)}(\bx);
\eeq
however, since $\widetilde{\chi}(r)$ has compact support, we need to tie both the support of $\widetilde{\chi}$ and how fast $\widetilde{\chi}$ vanishes in a neighbourhood of its support to the definition of $\chi$ for both $u^{(2)}$ and $f$ to be well defined.

Setting aside for the moment this need to synchronise the definitions of $\chi$ and $\widetilde{\chi}$, since $\supp \,\widetilde{f}$ is a compact subset of $\DR$, so is 
$\supp \,u^{(2)}$, and so $u^{(2)}$ is then the solution of the exterior Dirichlet problem (in the sense of \cref{prob:edp}) with data $f$ defined in \cref{eq:obs3} and coefficient $\nst:=1 + c(k)\widetilde{\chi}$.
Finally, define $u^{(1)}$ to be the solution of the exterior Dirichlet problem with $f$ defined in \cref{eq:obs3}. The whole point of these definitions is that, combined with \cref{eq:obs1} and \cref{eq:obs2} and the uniqueness of the solution of the exterior Dirichlet problem, they imply that 
\beq\label{eq:obs4}
u^{(1)}(\bx)- u^{(2)}(\bx) = \re^{\ri k x_1}\chi(r),
\eeq
and from this we therefore have that
\beqs
\big\|u^{(1)}-u^{(2)}\big\|_{L^2(\DR)} \sim 1
\quad \tand \quad
\big\|u^{(1)}-u^{(2)}\big\|_{\HokDR} \sim k.
\eeqs
Furthermore, the definitions of $u^{(2)}$ \cref{eq:obs3} and $\widetilde{f}$ \cref{eq:obs1} imply that
\beqs
\big\| u^{(2)}\big\|_{L^2(\DR)} \sim \frac{1}{k\, c(k)} \quad\tand \quad 
\big\| u^{(2)}\big\|_{\HokDR} \sim \frac{1}{c(k)},
\eeqs 
and, since $\|\nso- \nst\|_{L^\infty(\DR)} = c(k)$, \cref{eq:sharp1} holds.

Therefore, to complete the proof, we only need to show that there exists a choice of $\chi$ and $\widetilde{\chi}$ for which $u^{(2)}$ and $f$ defined by \cref{eq:obs3} are 
in $H^{1}(\DR)$ and $\LtDR$ respectively (in fact, we prove that they are in $W^{1,\infty}(\DR)$ and $L^\infty(\DR)$ respectively).
%well-defined. 
Since $\chi$ and $\widetilde{\chi} \in C^\infty(\DR)$, the only issue is what happens at the boundary of the support of $\widetilde{\chi}$, where $u^{(2)}$ has the potential to be singular.
Since $\overline{\Omega_-} \subset \BR$, there exist $0<R_1<R_2<R$ such that $\overline{\Dm} \subset B_{R_2}\setminus B_{R_1} \subset \BR$. Let $\supp \chi = B_{R_2}\setminus B_{R_1}$ and let $\chi$ vanish to order $m$ at $r= R_1$ and $r=R_2$; i.e.~$\chi(r) \sim (r-R_1)^m$ as $r \rightarrow (R_1)^+$ and 
$\chi(r) \sim (R_2-r)^m$ as $r \rightarrow (R_2)^-$. The definition of $\widetilde{f}$ \cref{eq:obs3} then implies that $\widetilde{f}$ vanishes to order $m-2$. Let $\widetilde{\chi}(r)$ vanish to order $n$ at $r= R_1$ and $r=R_2$. 
We now claim that if $m >n+4$, then $u^{(2)}\in W^{1,\infty}(\DR)$ and $f$ $\in L^\infty(\DR)$. Indeed,  
straightforward calculation from \cref{eq:obs3} shows that  $u^{(2)}(r) \sim (r-R_1)^{m-n-2}$, $\nabla u^{(2)}(r) \sim (r-R_1)^{m-n-3}$, and $\Delta u^{(2)}(r) \sim (r-R_1)^{m-n-4}$ as $r \rightarrow (R_1)^+$, with analogous behaviour at $r=R_2$.
%vanishes to order $m-n-2$ and $\Delta u^{(2)}$ vanishes to order $m-n-4$ at $r= R_1$ and $r=R_2$. 
The assumption 
$m >n+4$ therefore implies that $u^{(2)}$, $\nabla u ^{(2)}$, and $\Delta u^{(2)}$ vanish (and hence are finite) at $r=R_1$ and $r=R_2$.
\epf

\bre[Why doesn't \cref{lem:sharp} cover the case $\Aso\neq  \Ast$?]
When $\nj:=1$, $j=1,2,$, $\Aso:=I$, and $\Ast:= I + c(k)\widetilde{\chi}$, the variational problem \cref{eq:vp1} implies that 
\beqs%\label{eq:obs2}
\Delta \big( u^{(1)} - u^{(2)}\big) + k^2 \big( u^{(1)} - u^{(2)}\big) = c(k)\nabla\cdot \big(\widetilde{\chi}\nabla u^{(2)}\big).
\eeqs
It is now much harder than in \cref{eq:obs2} to set things up so that $ u^{(1)}(\bx) - u^{(2)}(\bx)=\re^{\ri kr}\chi(r)$ (so that one can then use \cref{eq:obs1}).
\ere

%\section{Proof of \cref{lem:2}}

\section{Extension of the results to weaker norms and probabilistic convergence results}
\subsection{Analogues of \cref{thm:1,cor:1,cor:1a} in weaker norms}\label{sec:weaknorm}
Recall from \cref{sec:num,sec:main} that GMRES applied to $\AmatoI \Amatt$ converges in a $k$-independent number of iterations if $\NLiDRRR{\nso-\nst} \sim 1/k$ (with an analagous result for $\Aso-\Ast$). This result show that $1/k$ is a sharp threshold when we consider the \emph{magnitude}  of the difference between $\nso$ and $\nst$. However, this result does not say anything about the \emph{spatial} variability between $\nso$ and $\nst$. For example if $\nso$ and $\nst$ (defined on the unit square) are given by
\beq\label{eq:noweak}
\nso(\bx) =
\begin{dcases}
  1 &\tif \bx_1 \leq \half\\
  2  &\tif \bx_1 > \half
  \end{dcases}
\eeq
and
\beq\label{eq:ntweak}
\nst(\bx) =
\begin{dcases}
  1 &\tif \bx_1 \leq \half+\alpha\\
  2  &\tif \bx_1 > \half+\alpha
  \end{dcases}
\eeq
for some $0 < \alpha < 1/2,$ then $\NLiDRRR{\nso-\nst} = 1$ for all $\alpha$, but one would expect that for small $\alpha$ the corresponding solutions of \cref{prob:edp} satisfy $\uso \approx \ust,$ and one might expect that GMRES applied to $\AmatoI\Amatt$ would converge in a $k$-independent number of iterations. Therefore, in this \lcnamecref{sec:weaknorm} we seek to obtain analogues of \cref{thm:1,cor:1,cor:1a} with the difference in $\nso-\nst$ and $\Aso-\Ast$ measured in weaker norms than the $L^\infty$ norm.

The (realistic) best-case result we could obtain would be that GMRES applied to $\AmatoI\Amatt$ converges in a $k$-independent number of iterations if $\NLoDRRR{\nso-\nst} \lesssim 1/k$. This result is `best' in the sense that it depends optimally on $k$; recall the discussion in \cref{rem:physical1k} that the length scale $1/k$ is the length scale governing the behaviour of Helmholtz problems. It is also `best' with regards to the norm used to measure $\nso-\nst$. When we measure $\nso-\nst$ in the $L^\infty$ norm, we are able to control the \emph{magnitude} of $\nso-\nst$, but not the spatial variability; if $\nso-\nst \neq 0$ only on a set of small (but nonzero) measure, where $\nso-\nst=1,$ then $\NLiDRRR{\nso-\nst} = 1$, regardless of the measure of the set. In contrast, the $L^1$ norm allows us to control both the magnitude of $\nso-\nst$ and the measure of the sets on which it is nonzero.

We will give numerical results indicating that this best-case result is sharp (our results actually indicate that we obtain $k$-independent convergence is $\NLqDRRR{\nso-\nst}\sim 1/k$ for any $q>0$). We will also provide theory results that are, to our knowledge, the best one can prove, although they are sub-optimal in both $q$ and the dependence on $k.$


\subsubsection{Theory in weaker norms}\label{sec:weakertheory}
The reason that the terms $\NLiDRRRdtd{\Aso-\Ast}$ and $\NLiDRRR{\nso-\nst}$ appear in \cref{thm:1} is that the terms $\NLiDRRR{n}$ and $\NLiDRRRdtd{A}$ appear in \cref{lem:keylemma1,lem:keylemma2}, respectively. These terms appear because in \cref{eq:mainevent1a,eq:Fbounda} we use the bounds
\beq\label{eq:keynbound}
\NLtDR{n\ftilde} \leq \NLiDRRR{n}\NLtDR{\ftilde}
\eeq
and
\beq\label{eq:keyAbound}
\NLtDR{A \grad \ftilde} \leq \NLiDRRRdtd{A}\NLtDR{\grad \ftilde}
\eeq
respectively, for some function $\ftilde,$ and these bounds are carried through the rest of the proof.

However, we observe that we have the following generalisation of H\"older's inequality: If $\ptilde,\qtilde > 2$ such that $1/2 = 1/\ptilde + /\qtilde,$ then
\beq\label{eq:genholder}
\NLtDR{\vo\vt} \leq \NLptildeDR{\vo}\NLqtildeDR{\vt}.
\eeq

If we instead use \cref{eq:genholder} to bound \cref{eq:keynbound,eq:keyAbound} we instead obtain
\beq\label{eq:keynbound2}
\NLtDR{n\ftilde} \leq \NLqtildeDR{n}\NLptildeDR{\ftilde}
\eeq
and
\beq\label{eq:keyAbound2}
\NLtDR{A\grad\ftilde} \leq \NLqtildeDR{A}\NLptildeDR{\ftilde}.
\eeq

As $\ftilde \in \Vhp$, we can apply an inverse inequality to bound $\NLptildeDR{\ftilde}$ by $\NLtDR{\ftilde}$. The required inverse inequality is (see \cite[Theorem 4.5.11 and Remark 4.5.20]{BrSc:08}
\beq\label{eq:inverseptilde}
\NLptildeDR{\ftilde} \leq \Cinvptilde h^{d\mleft(\frac1{\ptilde} - \half\mright)} \NLtDR{\ftilde}.
\eeq
If we then apply \cref{eq:inverseptilde} to \cref{eq:keynbound2,eq:keyAbound2} we obtain
\beq\label{eq:keynboundfinal}
\NLtDR{n\ftilde} \leq \Cinvptilde \NLqtildeDR{n} h^{d\mleft(\frac1{\ptilde} - \half\mright)} \NLtDR{\ftilde}
\eeq
and
\beq\label{eq:keyAboundfinal}
\NLtDR{A\grad\ftilde} \leq \Cinvptilde \NLqtildeDR{A} h^{d\mleft(\frac1{\ptilde} - \half\mright)} \NLtDR{\grad\ftilde}.
\eeq

Replacing \cref{eq:mainevent1a,eq:Fbounda} with \cref{eq:keynboundfinal,eq:keyAboundfinal}, and proceeding as in the proofs of \cref{lem:keylemma1,lem:keylemma2}, we obtain \cref{lem:keylemma1a,lem:keylemma2a} below, the analogues of \cref{lem:keylemma1,lem:keylemma2}.

\ble[Alternative bounds on $(\Amato)^{-1} \Mmat_{n}$]\label{lem:keylemma1a}
Under the assumptions of \cref{lem:keylemma1}, for $n\in \LiDRRR$ and for any $\ptilde,\qtilde > 2$ such that $1/\ptilde + 1/\qtilde = \half$,
\beq\label{eq:keybound12}
\max\set{\NDk{\AmatoI \Mmatn},\NDkI{\Mmatn\AmatoI}} \leq \Cttilde h^{d\mleft(\frac1{\ptilde}-\half\mright)} \frac{\NLqtildeDR{n}}k
\eeq
and 
\beq\label{eq:keybound1a2}
\max\set{\Nt{\AmatoI \Mmatn},\Nt{\Mmatn\AmatoI}} \leq \Cttilde\mleft(\frac{\mplus}{\mminus}\mright) h^{d\mleft(\frac1{\ptilde}-\half\mright)} \frac{\NLqtildeDR{n}}k
\eeq
for all $k\geq \kz$,
where
\beq\label{eq:C2tilde}
\Cttilde\de%\frac{m_+}{m_-} 
%\left[ 
\Cinvptilde\Ct,
\eeq
where $\Ct$ is defined by \cref{eq:C2}.
\ele

\ble[Alternative bounds on $(\Amato)^{-1} \Smat_A$]\label{lem:keylemma2a}
Under the assumptions of \cref{lem:keylemma2}, for $A\in L^\infty(\DR,\RR^{d\times d})$ and for any $\ptilde,\qtilde > 2$ such that $1/\ptilde + 1/\qtilde = \half$,
\beq\label{eq:keybound22}
\max\set{\NDk{\AmatoI \SmatA},\NDkI{\SmatA\AmatoI}} \leq \Cotilde h^{d\mleft(\frac1{\ptilde}-\half\mright)}k \NLqtildeDR{A}
\eeq
and
\beq\label{eq:keybound2a2}
\max\set{\Nt{\AmatoI \SmatA},\Nt{\SmatA\AmatoI}} \leq \Cotilde\mleft(\frac{\splus}{\mminus}\mright) h^{d\mleft(\frac1{\ptilde}-\half\mright)-1} \NLqtildeDR{A}
\eeq
%\begin{align}\nonumber
%&\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_2, \,\,
%\big\| \Smat_A (\Amato)^{-1} \big\|_2\Big\}\nonumber \\
%&\hspace{2cm}
% \leq \frac{s_+}{s_-} \left[ C_{\rm FEM2}^{(1)} + 
% \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right]k\N{A}_{L^\infty(\DR)}\label{eq:keybound2}
%% + C_{\rm bound}^{(1)}\right) \frac{\N{n}_{L^\infty(\DR)}}{k}.
%\end{align}
for all $k\geq k_0$, where
\beq\label{eq:C1tildenbpc}
\Cotilde \de \Cinvptilde\Co,
\eeq
where $\Co$ is given by \cref{eq:C1nbpc}.
\ele


\begin{theorem}[Alternative main ingredient to answer to \cref{it:nbpcq1}]\label{thm:1alt}
Let the assumptions of \cref{thm:1} hold.   Then, given $\kz>0$ and $\ptilde,\qtilde >2$ such that $1/\ptilde + 1/\qtilde = 1/2$, there exist $\Cotilde, \Cttilde>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, $\ptilde$, and $\kz$) such that
\begin{align}\nonumber
&\max\set{\NDk{\Imat - \AmatoI\Amatt},\NDkI{\Imat -\Amatt\AmatoI}}\\
&\hspace{3cm} 
\leq \Cotilde kh^{d\mleft(\frac1{\ptilde}-\half\mright)} \NLqtildeDR{\Aso-\Ast} + \Cttilde  kh^{d\mleft(\frac1{\ptilde}-\half\mright)}  \NLqtildeDR{\nso-\nst}
\label{eq:main1alt}
\end{align}
and 
\begin{align}\nonumber
&\max\set{\Nt{\Imat - \AmatoI\Amatt}, \Nt{\Imat -\Amatt\AmatoI}}\\
&\hspace{0cm}
\leq \Cotilde \mleft(\frac{\splus}{\mminus}\mright) h^{d\mleft(\frac1{\ptilde}-\half\mright)-1}\NLqtildeDR{\Aso-\Ast} + \Cttilde \mleft(\frac{\mplus}{\mminus}\mright) kh^{d\mleft(\frac1{\ptilde}-\half\mright)}\NLqtildeDR{\nso-\nst}
\label{eq:main1aalt}
\end{align}
for all $k\geq k_0$. 
\end{theorem}

\begin{corollary}[Alternative answer to \cref{it:nbpcq1}: $k$-independent weighted GMRES iterations]\label{cor:1alt}
Let the assumptions of \cref{cor:1a} hold.  Given $k_0>0$ and $\qtilde >2$, let $\Cotilde$ and $\Cttilde$ be as in \cref{thm:1alt}. Then if 
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:condalt}
\Cotilde kh^{-\frac{d}{\qtilde}} \NLqtildeDR{\Aso-\Ast} +\Cttilde  kh^{-\frac{d}{\qtilde}} \NLqtildeDR{\nso-\nst}
\leq \frac{1}{2}
\eeq
for all $k\geq k_0$, then \emph{both} weighted GMRES working in $\|\cdot\|_{\Dmat_k}$ (and the associated inner product) applied to \cref{eq:pcsystem1} \emph{and} weighted GMRES working in $\|\cdot\|_{(\Dmat_k)^{-1}}$ (and the associated inner product) applied to \cref{eq:pcsystem2}  converge in a $k$-independent number of iterations for all $k\geq k_0$.
\end{corollary}

\begin{corollary}[Alternative answer to \cref{it:nbpcq1}: $k$-independent (unweighted) GMRES iterations]\label{cor:1aalt}
Let the assumptions of \cref{cor:1a} hold.  Given $k_0>0$, and $\qtilde >2$, let $\Cotilde$ and $\Cttilde$ be as in \cref{thm:1alt}. Then if 
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:condaalt}
\Cotilde \mleft(\frac{\splus}{\mminus}\mright) h^{-\frac{d}{\qtilde}-1} \NLqtildeDR{\Aso-\Ast} + \Cttilde \mleft(\frac{\mplus}{\mminus}\mright) kh^{-\frac{d}{\qtilde}} \NLqtildeDR{\nso-\nst} \leq \half
\eeq
for all $k\geq k_0$, then standard GMRES (working in the Euclidean norm and inner product) applied to either of the equations \cref{eq:pcsystem1} or \cref{eq:pcsystem2}
%\beqs
%(\Amat^{(1)})^{-1}\Amat^{(2)}\bu = \bff\quad\text{ or } \quad\Amat^{(2)}(\Amat^{(1)})^{-1}\bv = \bff
%\eeqs
 converges in a $k$-independent number of iterations for all $k\geq k_0$.
\end{corollary}

Observe that in \cref{cor:1alt,cor:1aalt} there is a trade-off between the norm that one uses to measure $\no-\nt$ and the restriction on the magnitude of this norm. E.g., the condition on $\no-\nt$ in both \cref{cor:1alt,cor:1aalt} can be summarised as
\beq\label{eq:altsufficientlysmall}
\NLqtildeDRRR{\no-\nt} k h^{-\frac{d}{\qtilde}} \text{ is sufficiently small}.
\eeq
Observe that as $\qtilde \downarrow 2,$ we measure $\no-\nt$ in a weaker norm, but the condition \cref{eq:altsufficientlysmall} becomes more restrictive; the power of $h$ increases. Conversely, as $\qtilde \uparrow \infty,$ we measure $\no-\nt$ in a stronger norm, but the condition \cref{eq:altsufficientlysmall} becomes less restrictive; the power of $h$ decreases. (Also observe that in the $\qtilde=\infty$ limit we recover the condition \cref{eq:sufficientlysmall} we previously proved for $\NLiDRRR{\no-\nt}.$

However, the numerics below suggest that in certain cases, a sufficient condition for nearby preconditioning to be effective is
\beq\label{eq:experimentalsufficientlysmall}
\NLqtildeDRRR{\no-\nt} k \text{ is sufficiently small},
\eeq
for \emph{any} $\qtilde \geq 1$, and moreover \cref{eq:experimentalsufficientlysmall} may be sharp in its $k$-dependence. (This requirement would fit with our previous observation about $1/k$ being the length scale below which perturbations cannot be seen---see \cref{rem:physical1k} above.) However, we do not saythat \cref{eq:experimentalsufficientlysmall} is sufficient for all cases; recall that for transmission problems, very small perturbations in $n$ can lead to very different behaviour in the solution $u$ if $k$ is a quasi-resonance for $\no$ or $\nt$; see the discussion at the end of \cref{sec:wpdisc} above.


\subsubsection{Numerics in weaker norms}\label{sec:weakernumerics}
For our computations, we use the computational setup as in \cref{app:compsetup}, with $f$ and $\gI$ corresponding to a plane wave passing through homogeneous media. We let $\Aso=\Ast=I,$ and we define $\nso$ and $\nst$ by \cref{eq:noweak,eq:ntweak}. For $\alpha = 0.2,0.2/k^{0.1},0.2/k^{0.2},\ldots,0.2/k$ and for $k=10,20,\ldots,100$ we use GMRES to solve $\AmatoI\Amatt = \AmatoI \bff$ (for $\bff$ given by the Helmholtz problem), and we record the number of GMRES iterations taken to achieve convergence.

Our results in \cref{fig:l1low,fig:l1med,fig:l1high} indicate the following conclusions for $\NLqDRR{\nso-\nst} \sim 0.1/k^{\beta}$ :
\bit
\item For $\beta \in (0,0.6)$ there is clear growth of the number of GMRES iterations with $k$,
\item For $\beta = 1$ there is clear boundedness of the number of GMRES iterations with $k$, and
  \item for $\beta \in (0.7,0.9)$ it is unclear if the number of GMRES iterations grows with $k.$
\eit

If we compare our numerical results with the theory results in \cref{cor:aalt}, we see that the theory (with $h \sim k^{-3/2}$ and $d=2$) predicts that the number of iterations will remain bounded if $\NLqDRR{\nso-\nst} k^{1+3/q}$ is sufficiently small, for any $q \in (2,\infty).$ Our computed results indicate that this result is not sharp. The computed results indicate that if $\NLqDRR{\nso-\nst} \sim k^{-1}$ for any $q \geq 1,$ then the number of GMRES iterations is bounded as $k$ increases. Observe that the `best case' $1/k$ condition is only predicted by the theory in the $q\rightarrow \infty$ limit.

\begin{figure}
\input{l1-low.pgf}
  \caption{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2/k^\beta,$ for $\beta = 0,0.1,0.2,0.3.$}\label{fig:l1low}
\end{figure}

\begin{figure}
  \input{l1-med.pgf}
    \caption{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2/k^\beta,$ for $\beta = 0.4,0.5,0.6,0.7.$}\label{fig:l1med}
\end{figure}
    
    \begin{figure}
    \input{l1-high.pgf}
      \caption{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2/k^\beta,$ for $\beta = 0.8,0.9,1.$}\label{fig:l1high}
\end{figure}
      
\begin{table}
  \centering
  \input{l1-table}
  \caption{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2/k^\beta.$}\label{tab:l1}
  \end{table}

\section{Applying nearby preconditioning to stochastic problems}\label{sec:nbpcstochastic}

We now apply nearby preconditioning to \emph{stochastic} Helmholtz problems, i.e., the Helmholtz problems defined and described in \cref{chap:stochastic}. We have two applications. In \cref{sec:probnbpc} we develop a probabalistic analogue of \cref{cor:1} above, and in \cref{sec:qmcnbpc} we apply nearby preconditioning to a Quasi-Monte-Carlo (QMC) method, showing that we have significant computational gains by using nearby preconditioning.

Throughout this \cref{sec:nbpcstochastic} we consider \cref{prob:msedp} from \cref{chap:stochastic} but with $A=I$. I.e., for simplicity's sake we only consider the case of random $n$, although everything we sya could be easily extended to random $A$. To maintain consistent notation with the rest of this \cref{chap:nbpc} we will use a superscript ${}^{(2)}$ to refer to the stochastic problem (e.g., the random coefficient will be $\nst(\omega)$, the solution will be $\ust(\omega)$, the matrices arising from the finite-element discretiation will be $\Amatt(\omega),$ etc.. We also let $\nso \in \LiDRRR$ define a \emph{deterministic} Helmholtz problem, as at the beginning of this \cref{sec:nbpc}; we will use this deterministic Helmholtz problem to precondition the (discretisations of) the realisations of the stochastic Helmholtz problem. I.e., we will consider the performance of GMRES applied to
\beq\label{eq:stopc}
\AmatoI\Amatt(\omega)\bu = \AmatoI \bff.
\eeq
For simplicity's sake, in all that follows we will measure $\no-\nt$ in the $L^{\infty}$ norm, or though one could use any of the weaker norms discussed above, and obtain analogous results.

\subsection{Probabilistic analogue of \cref{cor:1a}}
If we apply \cref{cor:1a} to the above set-up we can straightforwardly conclude the following \cref{cor:stonbpcas}

\bco[Almost-sure nearby preconditioning]\label{cor:stonbpcas}
Let the assumptions of \cref{cor:1a} hold, and let $\nt$ satisfy the assumptions at the start of \cref{sec:hh-results}. Then the number of GMRES iterations needed to solve \cref{eq:stopc} is bounded independently of $k$ almost surely if
\beq\label{eq:nbpcas}
\NLiDRRR{\nso-\nst(\omega)} \leq \frac1{2\Ct k}
\eeq
almost surely.
\eco
The numerical results in \cref{sec:num} above can be seen as confirming this result.

\bre[\Cref{cor:stonbpcas} is not ideal]\label{rem:notideal}
\Cref{cor:stonbpcas} is not ideal for two main reasons:
\ben
\item\label[itemreason]{it:notideal1} The condition \cref{eq:nbpcas} must hold almost surely, and
  \item\label[itemreason]{it:notideal2} \Cref{cor:stonbpcas} does not give any explicit information on how the number of GMRES iterations depends on $\NLiDRRR{\nso-\nst(\omega)}.$
    \een
    \Cref{it:notideal1} is not ideal because in many physically realistic problems $\NLiDRRR{\no-\nt(\omega)}$ may be unbounded (e.g., if $\nt$ is a lognormal random field) or even if bounded may not satisfy the condition \cref{eq:nbpcas} almost surely. \Cref{it:notideal2} is not ideal because it means one cannot infer information about the distribution of the number of GMRES iterations from the distribution of$\NLiDRRR{\nso-\nst(\omega)}.$
    \ere
    In order to correct the deficiencies described in \cref{rem:notideal} we will first prove a bound on the number of GMRES iterations depending explicitly on $\NLiDRRR{\nso-\nst(\omega)}$, and then use this bound to prove a probabalistic estimate on the number of GMRES iterations.

The following \lcnamecref{lem:probgmres1} is a \emph{deterministic} result, i.e., it does not require $\nt$ to be a random field. Therefore for this \lcnamecref{lem:probgmres1} only, we assume $\nst$ is as given at the beginning of this \lcnamecref{chap:nbpc}.
\ble[Maximum number of GMRES iterations]\label{lem:probgmres1}
Let $0 < \eps < 1,$ and let $\dofs$ denote the number of degrees of freedom, i.e. the size of the matrices $\Amato$ and $\Amatt$.

Let $\GMRES{\eps}{\nso}{\nst}$ denote the number of iterations required for GMRES in the unweighted norm $\Nt{\cdot}$ with $\Nt{\brz} = 1,$ applied to
\beqs
\AmatoI\Amatt  \bu = \AmatoI \bff
\eeqs
to converge to within a tolerance $\eps,$ i.e., to achieve
\beqs
\frac{\Nt{\bxm}}{\Nt{\bff}} < \eps.
\eeqs

Then there exists a function $\Gfnname:\RRp\rightarrow [0,\dofs]$ such that
\beqs
\GMRES{\eps}{\nso}{\nst} \leq \Gfn{\nso-\nst}.
\eeqs

Moreover, $\Gfnname$ is given by
\beq\label{eq:gdef}
\Gfn{\NLiDRRR{\nso-\nst}} =
\begin{dcases}
\min\set{N,\frac{\log{\eps}}{\log\mleft(\frac{2\alpha^{1/2}}{\mleft(1+\alpha\mright)^2}\mright)}+1} & \tif \alpha < 1\\
N & \tif \alpha \geq 1,
\end{dcases}
\eeq

where $\alpha = \Ct k \NLiDRRR{\nso-\nst},$ where $\Ct$ is given by \cref{eq:C2}.
\ele

See \cref{fig:G} for some examples of the function $\Gfnname$.

The proof of \cref{lem:probgmres1} uses the following corollary \cite[Corollary 3]{SaSc:86} of \cite[Proposition 2]{SaSc:86} on the `lucky breakdown' of GMRES.
\bco[Guaranteed convergence of GMRES]\label{cor:gmresguaranteed}
For an $N \times N$ problem GMRES converges in at most $N$ iterations.
\eco

\bpf[Proof of \cref{lem:probgmres1}]
For $\alpha \geq 1$, the result is immediate from \cref{cor:gmresguaranteed}. For $\alpha < 1,$ if we insert \cref{eq:gmressin} into \cref{eq:Elman} (with $\Dmat=\Imat,$ so $\NDmat{\cdot} = \Nt{\cdot}$), we obtain, for $m \in \NN$
\beq\label{eq:gmressub}
\frac{\Nt{\brm}}{\Nt{\brz}} \leq \mleft(\frac{2\sqrt{\alpha}}{\mleft(1+\alpha\mright)^2}\mright)^m.
\eeq
To obtain a lower bound on the number of iterations needed to obtain the solution to within a tolerance $\eps,$ we set the right-hand side of \cref{eq:gmressub} to be less than $\eps$ and solve for $m$ to obtain that the GMRES residual is less than $\eps$ (recall we assume $\Nt{\brz} = 1$) if
\beq\label{eq:mlower}
m \geq \frac{\log{\eps}}{\log\mleft(\frac{2\alpha^{1/2}}{\mleft(1+\alpha\mright)^2}\mright)}.
\eeq
Hence, if $m$ is the smallest integer satisfying \cref{eq:mlower}, then
\beq\label{eq:gmressub2}
m  \leq\frac{\log{\eps}}{\log\mleft(\frac{2\alpha^{1/2}}{\mleft(1+\alpha\mright)^2}\mright)}+1.
\eeq
The result for $\alpha < 1$ therefore follows from \cref{eq:gmressub2,cor:gmresguaranteed}.
\epf

\bre[Why not use the ceiling in \cref{eq:gmressub2}?]
One could replace the bound \cref{eq:gmressub2} on $m$ by the equality
\beqs
m  =\ceil{\frac{\log{\eps}}{\log\mleft(\frac{2\alpha^{1/2}}{\mleft(1+\alpha\mright)^2}\mright)}}.
\eeqs
However, the change in the definition of $\Gfnname$ \cref{eq:gdef} would mean $\Gfnname$ would no longer be continuous. As we must use numberical methods to calculate probabilities associated with $\Gfnname$ (see \cref{thm:probgmres,rem:gcomputable} below), it is much simpler if $\Gfnname$ is continuous, and so we use \cref{eq:gmressub2}.
\ere

\bre[Why the dependence on $\alpha$ in \cref{lem:probgmres1}?]
The reason that \cref{eq:gdef} has two cases depending on $\alpha = \Ct k \NLiDRRR{\nso-\nst}$ is because the \lcnamecref{cor:GMRES_intro} only holds if $\alpha < 1$. Therefore if $\alpha \geq 1$ the only result available for us to apply is \cref{cor:gmresguaranteed}, stating that GMRES converges in at most $N$ iterations.
\ere

\begin{figure}
  \centering
  \input{G.pgf}
  \caption{Plot of $\Gfnname$ for $\NLiDRRR{\no-\nt} \in \mleft(0.01,1.0\mright)$, for $k=20,40,100$ $\Ct = 0.1,$ $N=\ceil{k^3},$ and $\eps = 10^{-5}$.\label{fig:G}}
    \end{figure}


\Cref{lem:probgmres1} gives us the relationship between the (bound on the) number of GMRES iterations and $\NLiDRRR{\nso-\nst(\omega)}.$ We can therefore infer probabalistic properties of the number of GMRES iterations from the probability distribution of $\NLiDRRR{\nso-\nst(\omega)}.$ (For probabalistic notation, we refer the reader to \cref{chap:stochastic}.)

\bth[Probabilistic GMRES convergence]\label{thm:probgmres}
Let $\nso \in \LiDRRR$ be fixed, and let $\nst:\Omega\rightarrow\LiDRRR$ be a random field. Let $\eps$ and $\dofs$ be as in \cref{lem:probgmres1}, and let $\Aso = \Ast = I.$ Fix $ R \in \NN.$ Then
\beq\label{eq:GMRESprob}
\PP\mleft(\Gfn{\NLiDRRR{\nso-\nst}} \leq R\mright)\leq\PP\mleft(\GMRES{\eps}{\nso}{\nst} \leq R\mright) .
\eeq
\enth

\bpf[Proof of \cref{thm:probgmres}]
By \cref{lem:probgmres1} we have the implication: if $\Gfn{\nso-\nst(\omega)} \leq R$, then $\GMRES{\eps}{\nso}{\nst(\omega)} \leq R.$ Therefore we have the set inclusion
\beqs
\set{\omega \in \Omega \st \Gfn{\nso-\nst(\omega)} \leq R} \subseteq \set{\omega \in \Omega \st \GMRES{\eps}{\nso}{\nst(\omega)} \leq R}.
\eeqs
The result immediately follows.
\epf

\bre[$\GMRES{\eps}{\no}{\nt}$ is a random variable]
All of the operations used in constructing the vectors $\bxm$ in the GMRES algorithm are measurable functions of $\bxmmo$ and $\AmatoI\Amatt$ (see, e.g., \cite[Algorithms 11.4.2 and 5.1.3]{GoVa:13}), therefore $\mleft(\bxm\mright)_{m=1}^N$ is a sequence of random variables, a stochastic process (see, e.g., \cite[Definition 2.1.4]{Ok:13}). The stopping criterion $\Nt{\bxm} < \eps$ is an \emph{exit time} from the set $\mleft[\eps,\infty\mright)$, and therefore, because we assume $\OFP$ is a complete probability space, it follows from, e.g.,  \cite[Example 7.2.2]{Ok:13} that $\GMRES{\eps}{\no}{\nt}$ is a stopping time. See \cite[Definition 7.2.1]{Ok:13} for the probabalistic definition of a stopping time. Informally, a random time is a stopping time, if one can determine whether it has occured based only on past knowledge. For example, the \emph{first} time a stochastic process exits a set is a stopping time, the \emph{last} time a stochastic process exits a set is not. Because $\GMRES{\eps}{\no}{\nt}$ is a stopping time, it is measurable with respect to the associated filtration (see, e.g., \cite[Definition 3.2.2]{Ok:13}), and so is measurable with respect to $\cF$. I.e., $\GMRES{\eps}{\no}{\nt}$ is a random variable.
\ere

\bre[The expression \cref{eq:GMRESprob} is computable]\label{rem:computable}
Because the function $\Gfnname$ is not invertible (as is clear from \cref{fig:G} above), one cannot write the left-hand side of \cref{eq:GMRESprob} as
\beqs
\PP\mleft(\NLiDRRR{\nso-\nst} \leq \GfnnameI\mleft(\mleft[0,R\mright]\mright)\mright).
\eeqs
However, one can still compute the set
\beqs
\GfnnameI\mleft(\mleft[0,R\mright]\mright) = \set{\alpha \st \Gfn{\alpha} \in \mleft[0,R\mright]}
\eeqs
(where $\GfnnameI$ here denotes the pullback), and therefore one can compute the probabilities in \cref{eq:GMRESprob}. The main effort in computing $\GfnI{\mleft[0,R\mright]}$ is finding if there are any values of $\alpha < 1$ such that $\Gfn{alpha} = R$. However, these values can be computed numerically using standard root-finding software.
\ere

\bre[\Cref{thm:probgmres} is pessimistic]\label{rem:pessimistic}
Observe that we expect the bound in \cref{thm:probgmres} to be pessimistic, i.e., we expect that \cref{eq:GMRESprob} is not sharp. We especially expect \cref{eq:GMRESprob} is not sharp for large values of $R:$

One can show via elementary calculus that (assuming that for $\alpha < 1$ the expression involving $\alpha$ in \cref{eq:gdef} is always $\leq N$) for $\alpha < 1$ $\Gfnname$ achieves its maximum when $\alpha = 1/3$. Also observe that $\Gfnname$ over the range $\alpha < 1$ only depends on $k$ through the dependence of $\alpha$ on $k.$ Therefore, the maximum of $\Gfnname$ over $\alpha \in (0,1)$ is independent of $k.$ Let $\Gfnmaxlo$ denote the value of this maximum. Then, for any $R \in \mleft(\Gfnmaxlo,N\mright)$, the estimate $\PP\mleft(\Gfn{\nso-\nst} \leq R\mright)$ is equal to $\PP\mleft(\NLiDRRR{\nso-\nst} \leq 1/\mleft(\Ct k\mright)\mright),$ i.e., the lower-bound estimate is \emph{independent} of $R$. This is almost certainly not sharp - we would expect $\PP\mleft(\GMRES{\eps}{\nso}{\nst} \leq R\mright)$ to increase with $R$. However, because the only rigorous result we have available if $\alpha \geq 1$ is \cref{cor:gmresguaranteed}, we cannot prove a better bound.
\ere

\subsubsection{Qualitative predictions from \cref{thm:probgmres}}\label{sec:qualgmres}

Notwithstanding the comments in \cref{rem:pessimistic}, we wil show in this \lcnamecref{sec:qualgmres} that \cref{thm:probgmres} gives \emph{qualitatively} correct predictions. To illustrate this correctness, we take $\Ao=\At=I,$ $\no =1$, and $\nt = \no + \eta,$ where $\eta$ is an $\Exp{\sigma}$ random variable, where $\sigma$ is the shape parameter, that we will vary. By construction, $\NLiDRRR{\no-\nt} = \eta$. Recall that the standard deviation of $\eta$ is $\sigma$, and therefore one might expect (based on \cref{cor:1,cor:1a}) that if $\sigma \sim 1/k,$ the number of GMRES iterations may be controllable (in some sense) as $k \rightarrow \infty.$

In our numerical experiments we use the computational setup described in \cref{app:compsetup}, with $f=1$ and $\gI=0.$ we consider three cases:
\ben
\item\label{it:sigma1} $\displaystyle \sigma  = 1,$
\item\label{it:sigma2} $\displaystyle \sigma  = \frac{1}k,$ and
  \item\label{it:sigma3} $\displaystyle \sigma  = \frac{1}{k^2}.$
    \een

    For each of these cases we calculate
    \beq\label{eq:gmresprob}
    \PP\mleft(\GMRES{\eps}{\no}{\nt} \leq 12\mright),
    \eeq
     and compare these experimental results to the lower bounds given by \cref{thm:probgmres}.

    Qualitatively, from \cref{fig:prob-theory-plot-0.0,fig:prob-theory-plot-1.0,fig:prob-theory-plot-2.0} we expect that in \cref{it:sigma1} the probability \cref{eq:gmresprob} \emph{decreases} as $k$ increases, in \cref{it:sigma2} the probability \cref{eq:gmresprob} \emph{is constant} as $k$ increases, and in \cref{it:sigma3} the probability \cref{eq:gmresprob} \emph{increases} as $k$ increases. This qualitative behaviour is what we see experimentally in \cref{fig:prob-plot-0.0,fig:prob-plot-1.0,fig:prob-plot-2.0}, although the values for the probabilities are higher, as is expected, because \cref{thm:probgmres} is a lower bound.

\begin{figure}[h]
    \centering
\input{prob-gmres-theory-0.0.pgf}
\caption{The lower bound in \cref{eq:GMRESprob} for $R=12$, $\eps = 10^{-5}$, $N = \ceil{k^{3}}$, and $\Ct=0.1,$ with $\NLiDRR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1.$\label{fig:prob-theory-plot-0.0}}
\end{figure}

\begin{figure}[h]
    \centering
\input{prob-gmres-theory-1.0.pgf}
\caption{The lower bound in \cref{eq:GMRESprob} for $R=12$, $\eps = 10^{-5}$, $N = \ceil{k^{3}}$, and $\Ct=0.1,$ with $\NLiDRR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1/k.$\label{fig:prob-theory-plot-1.0}}
\end{figure}

\begin{figure}[h]
    \centering
    \input{prob-gmres-theory-2.0.pgf}
    \caption{The lower bound in \cref{eq:GMRESprob} for $R=12$, $\eps = 10^{-5}$, $N = \ceil{k^{3}}$, and $\Ct=0.1,$ with $\NLiDRR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1/k^2.$\label{fig:prob-theory-plot-2.0}}
\end{figure}
\begin{figure}[h]
    \centering
\input{prob-plot-rate-0.0.pgf}
\caption{Empricial probability (calculated from 1000 realisations) that $\GMRES{\eps}{\no}{\nt}\leq 12$ for $k = 10, 20, 30, 40,$ where $R=12$, $\eps = 10^{-5}$, $N = \ceil{k^{3}}$, and $\Ct=0.1,$ with $\NLiDRR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1.$\label{fig:prob-plot-0.0}}
\end{figure}

\begin{figure}[h]
    \centering
\input{prob-plot-rate-1.0.pgf}
\caption{Empricial probability (calculated from 1000 realisations) that $\GMRES{\eps}{\no}{\nt}\leq 12$ for $k = 10, 20, 30, 40,$ where $R=12$, $\eps = 10^{-5}$, $N = \ceil{k^{3}}$, and $\Ct=0.1,$ with $\NLiDRR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1/k$\label{fig:prob-plot-1.0}}
\end{figure}

\begin{figure}[h]
    \centering
\input{prob-plot-rate-2.0.pgf}
\caption{Empricial probability (calculated from 1000 realisations) that $\GMRES{\eps}{\no}{\nt}\leq 12$ for $k = 10, 20, 30, 40,$ where $R=12$, $\eps = 10^{-5}$, $N = \ceil{k^{3}}$, and $\Ct=0.1,$ with $\NLiDRR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1.$\label{fig:prob-plot-2.0}}
\end{figure}


\subsection{Application to Quasi-Monte-Carlo methods for the Helmholtz equation}\label{sec:nbpcqmc}

We now apply nearby preconditioning to a Quasi-Monte-Carlo (QMC) method for the Helmholtz equation. We begin with a brief description of QMC methods, before detailing two ways we apply nearby preconditioning to these methods. Finally, we give computational results for applying nearby preconditioning to QMC methods for the Helmholtz equation.

\subsubsection{Brief description of QMC}

QMC methods (or rules) are high-dimensional integration rules, designed to yield superior rates of convergence (with respect to the number of integration points) compared to Monte-Carlo methods. Suppose one wants to approximate $\EXP{Q},$ where $Q$ is some random variable (later in this \lcnamecref{sec:nbpcqmc}, $Q$ will be a function of a stochastic PDE). By definition, the expectation is
\beq\label{eq:qmcexpdef}
\EXP{Q} = \int_\Omega Q(\omega) \ddPPomega.
\eeq

If we now suppose $Q$ depends on the sample space via a finite set of random variables $\Uo,\ldots,\UJ$, then we can rewrite \cref{eq:qmcexpdef} as
\beq\label{eq:qmcexp2}
\EXP{Q} = \int_\Omega Q\mleft((\Uo(\omega),\ldots,\UJ(\omega)\mright) \ddPPomega.
\eeq
If, for example, the $\Uj$ are all Uniform random variables on $\mleft[-1/2,1/2\mright]$, then \cref{eq:qmcexp2} can be rewritten as
\beq\label{eq:qmcexp3}
\EXP{Q} = \int_{\cube{J}} Q\mleft(\by\mright) \dd\lambda,
\eeq
where $\by \in \cube{J}$ and $\lambda$ denotes Lebesgue measure.

Any integration rule, or method for approximating $\EXP{Q}$, can then be seen as a method for approximating the $J$-dimensional integral on the right-hand side of \cref{eq:qmcexp3}. Sampling-based rules will choose points $\byo,\ldots,\byNpoints \in \cube{J}$ and use the approximation
\beqs
\EXP{Q} \approx \frac1{\Npoints}\sum_{l=1}^{\Npoints} Q\mleft(\byl\mright).
\eeqs
Monte-Carlo and Quasi-Monte-Carlo rules are methods for choosing the points $\byl$. In a Monte-Carlo rule the points are chosen at random in accordance with the associated probability distribution. For example, in this case, the points are chosen according to the Uniform distribution on $\cube{J}$. Observe that Monte-Carlo rules do not need the dependence of $Q$ on $\omega$ to take the form prescribed above, they apply to any random variable.

Quasi-Monte-Carlo rules, in contrast to Monte-Carlo rules, do require the dependence on $\omega$ to take the form prescribed above, as QMC rules are high-dimensional integration rules (where we are integrating across the high-dimensional cube $\cube{J}).$ In QMC rules the points $\byl$ are not chosen completely at random, unlike Monte-Carlo rules.

The advantages of QMC rules is that they can exhibit higher rates of convergence compared to Monte-Carlo rules; Monte Carlo rules typically converge with rate $1/\Npoints^{1/2}$ (see\ednote{I'll sketch this in the MLMC chapter, and add in a reference once it's done.}), whereas QMC rules can converge with rates up to $1/\Npoints$  or with even higher rates for higher-order QMC rules, see, e.g., \cite[Penultimate paragraph of Section 1.2]{KuNu:16}.

In applying QMC rules to stochastic PDEs, we assume that the random coefficient is dependent on fintely many (or countably many) random variables, as in \cref{eq:qmcexp2} above, and we then use QMC rules to estimate expectations of quantities of interest for the stochastic PDE. Seee the next \lcnamecref{sec:nbpcqmcnum} for this setup worked out in more detail for the Helmholtz equation. Applying QMC rules to stochastic PDEs is a vibrant and active research area. For recent overviews of this field, see \cite{KuNu:16} (and the associated tutorial \cite{KuNU:18a}) and \cite{KuNu:18b}. We observe that there is currently no rigorous study of how QMC methods behave for the Helmholtz equation, although we undertstand such work is underway in \cite{GaKuSl}.

The key idea in applying nearby preconditioning to QMC methods for the Helmholtz equation is that we do not just choose one realisation $\no$ of the coefficient for which to calculate the preconditioner, rather, we choose as many as a are necessary to ensure the number of GMRES iterations across all samples remains bounded. The key questions the algorithms in this \lcnamecref{sec:nbpcqmc} answer are therefore: How many preconditioners should one calculate, and for which realisations of the coefficient should they be calculated?

\subsubsection{Methods for applying nearby preconditioning to QMC}\label{sec:nbpcqmcnum}
We now detail two methods for using nearby preconditioning to speed up QMC methods for the Helmholtz equation. To apply these methods, we use the following model problem: We consider the Interior Impedance Problem in 2-d with $f=1$ and $\gI=0$, $A = I$ and $n$ given by
\beq\label{eq:artificialkl}
n(\omega,\bx) = 1 + \sum_{j=1}^{10} \Uj(omega) \sqrt{\lambdaj} \psij(\bx),
\eeq
where
\beqs
\sqrt{\lambdaj} = j^{-2}
\eeqs
and
\beqs
\psij(\bx) = \cos\mleft(\frac{j\pi}4 x\mright)\cos\mleft(\frac{\mleft(j+1\mright)\pi}4 y\mright).
\eeqs
Observe that $\NLiDRR{\psij}=1$ for all $j,$ and $\sqrt{\lambdaj} \rightarrow 0$ as $j \rightarrow \infty.$ This expansion is based on the random-field expansion in \cite[Section 5.1]{GiGrKuScSl:19}. Expansions similar to \cref{eq:artificalkl} are often decribed as `artificial Karhunen--Lo\`eve expansions' due to their similarity with the Karhunen--Loe\`eve expansion of a random field, where the $\Uj$ are independent random variables, and the $\lambdaj$ and $\psij$ are the eigenvalues and eigenvectors of the covariance operator, see, e.g., \cite[Section 7.4]{LoPoSh:14}. In view of the fact that we will be using QMC methods to approximate $\EXP{Q(u)}$ (for some quantities of interest $Q$) we will sometimes instead write $n(\by)$ for $\by \in \cube{10}$, by which we mean
\beqs
n(\by) = 1 + 1 + \sum_{j=1}^{10} \by_{j} \sqrt{\lambdaj} \psij.
\eeqs
There is no a priori reason that one must have such an affine dependence of the random field on the randomness in order to apply nearby preconditioning to QMC methods (one could, for example, take $n$ to be a lognormal random field). However, affine dependence will allow us to easily define the so-called `paralleisable' nearby-preconditioning-QMC algorithm below.

We stress that the results in this \lcnamecref{sec:nbpcqmcnum} are strictly numerical; there is no current theory to support these calculations. In particular, we show in \cref{sec:nbpcqmcnumerics} below that it appears that for the QMC error for Helmholtz problems to remain bounded as $k$ increases, one must increase the number of QMC points with $k.$ We again remark that there is currently no theoretical justification for this behaviour.


\paragraph{Terminology} Before we describe the nearby-preconditioning-QMC algorithms in detail we establish two pieces of terminology that will be of use in describing these algorithms. Firstly, we will use the word `point' to refer to a point in the parameter space $\cube{J}$, and use phrases such as `calculate a preconditioner at the point $\by$' as shorthand for `calculate the preconditioner corresponding to the finite-element discretisation of the Helmholtz IIP (as described above) with coefficient $n(\by)$'.

We we alse use the words `nearby' and `nearest' (when referring to QMC points) to mean: nearest in the metric
\beqs
\dQMC(\byo,\byt) = \NLiDRRR{n(\byo)-n(\byt)}.
\eeqs
We use this metric to describe the geometry of our QMC points as our results in \cref{sec:intronbpc} above indicate that it is the $L^\infty$-norm of the different in the coefficients that dictates the effectiveness of nearby preconditioning. Therefore, when considering which QMC points will yield preconditioners suitable for use with other QMC points, this metric is a natural metric to use.


\paragraph{A sequential algorithm} We first describe a straightforward algorithm that uses nearby preconditioning to speed up QMC calculation for the Helmholtz equation. We call this a `sequential' algorithm because, unlike the `parallel' algorithm we describe below, it is intrinsically sequential and cannot be parallelised. I.e., finite-element solves for different realisations of the random field $n$ cannot be treated in parallel. One can, of course, use parallelisation for inidividual finite-element solves.

An overview of the algorithm is:
\ben
\item Choose a point $\by$ for which to calculate a preconditioner
\item\label{it:nearest} Find the nearest non-computed point and attempt a preconditioned GMRES solve at that point.
    \item If GMRES does not take too long to converge, return to \cref{it:nearest}.
\item If GMRES takes too long to converge, recalculate the preconditioner at the current point, and return to \cref{it:nearest}.
  \een
  The algorithm is written in more formal pseudocode in \cref{alg:seq}.
\begin{algorithm}[h]
\DontPrintSemicolon
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{Nearest}{nearest}

\Input{$\maxGMRES$,$\SQMC$}
\BlankLine
Choose starting point $\bystart$\;
$\bypre \defined \bystart$\;
$\Sremaining \defined \SQMC\setminus\set{\bypre}$\;
Calculate preconditioner $\Pmat$ at $\bypre$\;
$\bycurrent \defined$ \Nearest{$\bypre,\Sremaining$}\;
\While{$\Sremaining \neq \emptyset$}{
\lIf{GMRES applied to $\Pmat \Amat(\bycurrent) = \Pmat \bb$ converges in fewer than $\maxGMRES$ iterations}{
$\Sremaining \defined \Sremaining\setminus\set{\bycurrent}$\;
$\bycurrent \defined$ \Nearest{$\bypre,\Sremaining$}\;
}
\Else{
$\bypre \defined \bycurrent$\;
Calculate preconditioner $\Pmat$ at $\bypre$\;
}
}
\caption{Algorithm to perform all solves in a QMC method using nearby preconditioning\label{alg:seq}}
\end{algorithm}
\paragraph{Parallelisable algorithm} The main disadvantage of the `sequential' algorithm described above is that the points at which preconditioners are calculated are identified as the algorithm progresses. Therefore the algorithm cannot be parallelised by sending different collections of QMC points to different processors (as one would need to know which preconditioner to use for each point at the start of the algorithm). Therefore, we now suggest an alternative algorithm that allows one to specify the number of preconditioning points before the algorithm commences, and then calculates which points to use as preconditioning points, and then performs the all of the QMC solves, potentially in parallel if required. The most complicated part of the algorithm is deciding at which points to calculate the preconditioners, and so we describe this part of the algorithm in more detail here. A more formal description of the algorithm is available in \cref{alg:par}.

Suppose we are given a set $\set{\byo,\ldots,\byNQMC}$ of QMC points and a number $\Npretarget$; the target number of preconditioners to compute. The aim of this algorithm is to select (approximately) $\Npretarget$ QMC points that are (approximately) equally spaced with respect to the $\dQMC$ metric defined above. If such a goal is acheived, then one expects that the preconditioning points are best located to minimise the total number of GMRES iterations across all solves across all of the QMC points.

The algorithm contains two key ideas:
\ben
  \item Use a surrogate metric in place of $\dQMC$, and
\item Locate the preconditioning points according to a tensor-product rule.
  \een
  We now describe each of these two ideas in turn.

  Whilst the metric $\dQMC$ is the metric related to the performance of nearby preconditioning (as described in \cref{sec:intronbpc} above), in practice $\dQMC$ is difficult to work with; it is not obvious what geometry it induces on $\cube{J}$, nor how to theoretically work with it (we ignore the issue of \emph{computing} $\dQMC$ here). Therefore, we work in an alternative, although related metric
  \beqs
\dapprox(\byo,\by) = \sum_{j=1}^{J} \sqrt{\lambdaj} \abs{{\byo}_{j} - {\byt}_{j}}.
\eeqs
Observe that $\dapprox$ is a weighted $L^1$ metric, with the weights corresponding to the terms in \cref{eq:artificalkl}. Recall that $\sqrt{\lambdaj} \rightarrow 0$ as $j \rightarrow \infty$; therefore the higher dimensions contribute less to the value of $\dapprox$ (or, informally, points are `closer' in higher dimensions, or higher dimensions are `smaller' than lower dimensions). However, it is easy to compute with $\dapprox,$ and it is obvious that it enables one to think of $\cube{J}$ as the high-dimensional rectangle $\mleft[0,\sqrt{\lambdao}\mright]\times\cdots\times\mleft[0,\sqrt{\lambdaJ}\mright]$ equipped with the standard $L^1$ metric.

To understand why we use locate the preconditioning points using a tensor-product rule, we first decribe the heuristic we use. Let us assume we want to cover $\cube{J}$ with balls of radius $r$ (where these balls are measured in the $\dapprox$ metric). Therefore, given the centres of two of these balls $\bcone$ and $\bct$ (if we suppose these are the centres of `adjacent' balls), then we will have
\beq\label{eq:centres2r}
\dapprox(\bcone,\bct) = 2r.
\eeq
The question now arises of how we choose $\bcone$ and $\bct$ so that \cref{eq:centres2r} holds. We observe that, by the definition of $\dapprox$, if we choose $\bcone$ and $\bct$ such that
\beqs
\sqrt{\lambdaj}\abs{{\bcone}_{j}-{\bct}_j} = \frac{2r}{J},
\eeqs
then we will have \cref{eq:centres2r} by construction, because
\beqs
\dapprox(\bcone,\bct) = \sum_{j=1}^J \frac{2r}J = 2r.
\eeqs
Therefore, in dimension $j$ we choose the centres of the balls to be spaced
\beqs
\min\set{\frac{2r}{J\sqrt{\lambdaj}},1}
\eeqs
apart (where we include the minimum so that, for high dimensions, we include at least one centre). That is, in dimension $j$, we take
\beqs
\Nj \de \max\set{1,\frac{J\sqrt{\lambdaj}}{2r}}
\eeqs
equally spaced points $\centresj = \set{c_{j,1},\ldots,c_{j,\Nj}},$ and then we form the centres $\bcone,\ldots,\bcNpre$ by taking tensor products of the points in $\centreso,\ldots,\centresJ,$ giving a total of $\Npre = \No \times \cdots \times \NJ$ preconditioning points.

However, there are three immediate objections to the above approach:
\ben
\item The above procedure assumes we know the radius of the balls we wish to construct, and then returns the total number of preconditioning points, and their locations, but we only know in advance the ideal total number of preconditioning points.
\item There is no guarantee that the numbers of points $\Nj$ calculated above are integers.
  \item There is no guarantee the preconditioning points given by the above procedure are QMC points.
    \een
    These questions are all completely valid, and so we slightly modify the above procedure to deal with them.

    Recall that we assume that we are given a target number of preconditioners $\Npretarget$. The above procedure (amongst other things) defines a map $\Npreideal:\RRp \rightarrow \RRp$ given by $r \mapsto \Npre.$ Therefore we can calculate numerically the value $\rideal$ such that $\Npreideal(\rideal) = \Npretarget.$ (In our computations, we do this via interval bisection.)

    Once we know $\rideal,$ we can calculate the numbers of centres in each dimension $\Npreidealo,\ldots,\NpreidealJ$ as above (recalling that the $\Npreidealj$ are not necessarily integers). We then obtain integers $\Npreactualj = \round{\Npreidealj}$, where $\round{\cdot}$ denotes rounding to the nearest integer. (Recall $\Npreidealj \geq 1$ for all $j$ by construction, so $\Npreactualj$ will be a positive integer for all $j.$)

We then take $\Npreactualj$ centres in each dimension, as described above. We then obtain a total of $\Npreactual = \Npreactualo \times \cdots \times \NpreactualJ$ peconditioning points.

These points may not be QMC points. We could simply calculate the preconditioners at these non-QMC points. However we instead calculate the preconditioners at QMC points, and so we simply replace each calculated centre with its nearest QMC point.

    This algorithm is summarised more formally in \cref{alg:par}.
    
    

%% Define
%% \beqs
%% \Npreidealj(r) = \max\set{\frac{J \sqrt{\lambdaj}}{2r},1}.
%% \eeqs
%% The `ideal' total number of QMC points is
%% \beqs
%% \Npreideal(r)=\prod_{j=1}^J  \Npreidealj(r)
%% \eeqs

%% Want to calculate the number of preconditioners $\Npre$, the set
%% \beqs
%% \Spre=\set{\ypreo,\ldots,\ypreNpre}
%% \eeqs
%% of QMC points at which to calculate the preconditioner and the map
%% \beqs
%% \nearestpre:\SQMC\rightarrow\Spre
%% \eeqs
%% taking each QMC point to its nearest (in the induced spatial $L^\infty$ norm) preconditioner, where $\SQMC$ is the set of QMC points.

\begin{algorithm}[h]
\DontPrintSemicolon
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{Round}{round}

\Input{$\Npretarget \in \NN$}
\Output{$\Spre$, $\nearestpre$}
\BlankLine
Solve (numerically) $\Npreideal(\rideal) = \Npretarget$ for $\rideal$\;
\For{j $= 1$ \KwTo $J$}{
Calculate $\Npreactualj =$ \Round{$\Npreidealj(\rideal)$}\;
Define $\Sprej$ to be set of $\Npreactualj$ equally spaced points in $\mleft[-1/2,1/2\mright]$\;
}
Define $\displaystyle\Npre = \prod_{j=1}^J \Npreactualj$\;
Define $\Spre$ by taking all possible tensor products of points in $\Sprej$\;
\For{l $=1$ \KwTo $\NQMC$}{
Calculate $\nearestpre\mleft(\by^{(l)}\mright)$ by brute force\;
}
\caption{Algorithm to determine $\Spre$ and $\nearestpre$\label{alg:par}}
\end{algorithm}

\paragraph{Advantages and disadvantages of each method} The advantages of the sequential algorithm are:
\bit
\item Its simplicity; the algorithm is simple and intuitive to describe, and
\item Its lack of heuristics - one only needs to specify the maximum number of GMRES iterations; this could be determined, for example by the memory constraints of the machine one is using.
    \eit
    However, the disadvantages of the sequential algorithm re:
    \bit
  \item The algorithm is inherently serial; one must see whether a given solve converges in the required number of GMRES iterations before performing subsequent solves. (In principle one could parallelise the algorithm by splitting the QMC points up onto different groups of processors, and then using the sequential algorithm on each group of processors. However, there is no guarantee one would split the QMC points up in a way that groups nearby points, which could lead to a substantial increase in computational work.)
    \item There is no guarantee that this method for exploring the sample space and choosing the preconditioning points will yield an optimal collection of preconditioning points (optimal in the sense of the minimal number of preconditioning points needed).
    \eit

    The advantages and disadvantages of the parallel algorithm are, by and large, the reverse of those for the sequential algorithm. The advantages of the parallel algorithm are:
    \bit
  \item The algorithm is fully parallelisable; once the preconditioning points and the map from QMC points to preconditioning points have been calculated, one can send different linear solves to different groups of processors as one chooses. (Although note that, unless one sends all of the QMC points corresponding to a single preconditioner to the \emph{same} group of processors, one may need to calculate the same preconditioner multiple times, on different groups of processes\footnote{In our code, we split up the points with respect to the order they are generated by the QMC code. This was purely to make the code simpler.} However, the decrease in computational time gained from parallelisation should more than offset this increase in computational effort.)
    \item The preconditioning points should fill the parameter space `well'. Given the points are chosen a priori to be well spaced according to the $\dapprox$ metric, one expects they will be a close to optimal collection (in the sense described above).
      \eit
      The disadvantages of the parallel algorithm are:
      \bit
    \item One needs a heuristic for how many preconditioning points to choose, as this is not given by the algorithm. (In our numerical experiments below, we obtain this heuristic by using the sequential algorithm for low $k$, and then extrapolating the proportion of preconditioning points used to larger values of $k.$
      \item The number of preconditioning points generated is not exactly $\Npretarget$ due to rounding the `ideal' number of centres in each dimension to the nearest integer. However, we expect the number of generated points will be close $\Npretarget$.
      \eit


\subsubsection{Numerical Experiments}\label{sec:nbpcqmcnumerics}
We now describe numerical experiments demonstrating the effectiveness of nearby preconditioning for tackling Helmholtz problems.

As described above, before we perform our numerical experiments, we need to determine:
\bit
\item How the number of QMC points should scale with $k$, and
  \item How many preconditioners we should choose.
    \eit
    We tackle each of these in turn, before using the parallelisable algorithm detail above to apply nearby preconditioning to QMC methods for the Helmholtz equation. Throughout this \lcnamecref{sec:nbpcqmcnumerics} we use the model problem detailed above.

    To determine how the number of QMC points should scale with $k$, we first estimate the QMC error for increasing $k.$ To estimate the QMC error we use a randomly shifted QMC rule; our exposition here follows \cite[Section 4.2]{GrKuNuScSl:11}.

    Suppose our QMC points are $\byo,\ldots,\byNQMC$, and the resulting QMC rule is
    \beqs
\QMC{\NQMC}{Q} = \frac1{\NQMC}\sum_{l=1}^{\NQMC} Q\mleft(u\mleft(\byl\mright)\mright).
\eeqs
For a `shift' $\shift \in \cube{J}$ we define the shifted QMC rule
\beqs
\QMCshift{\NQMC}{Q}{\shift} = \frac1{\NQMC}\sum_{l=1}^{\NQMC} Q\mleft(u\mleft(\byl\oplus\shift\mright)\mright),
\eeqs
where $\by \oplus \shift$ denotes $\by + \shift$ `wrapped around' onto the hypercube $\cube{J}$. (Formally $\by \oplus \shift = \fracoperator{\mleft(\by + \bhalf\mright)+\shift} - \bhalf,$ where $\fracoperator{\cdot}$ denotes the fractional part.)

We then define the randomly shifted QMC rule (with multiple randomly chosen shifts $\shifto,\ldots,\shiftNshifts$)
\beqs
\QMCrandshift{Q}{\Nshifts} = \frac1{\Nshifts}\sum_{s=1}^{\Nshifts} \QMCshift{Q}{\shifts} = \frac1{\NQMC\Nshifts}\sum_{s=1}^{\Nshifts}\sum_{l=1}^{\NQMC} Q\mleft(u\mleft(\byl\oplus \shifts\mright)\mright).
\eeqs

Having defined the randomly shifted QMC rule, one can the use the standard statistical estimator of the standard deviation of the error in $\QMCrandshift{Q}{\Nshifts}$ \cite[Equation (4.6)]{GrKuNuScSl:11}
\beqs
\QMCerror{\NQMC}{\Nshifts} = \mleft(\frac1{\Nshifts\mleft(\Nshifts-1\mright)}\sum_{s=1}^{\Nshifts} \mleft(\QMCshift{Q}{\shifts} - \QMCrandshift{Q}{\Nshifts}\mright)^2\mright)^{\half}.
\eeqs
(See \cref{app:complexerror} for proof that $\QMCerror{\NQMC}{\Nshifts}^2$ is an unbiased estimator of the variance of $\QMCrandshift{Q}{\Nshifts}$; recall that it does \emph{not} follow that $\QMCerror{\NQMC}{\Nshifts}$ is an \emph{unbiased} estimator of the standard deviation of $\QMCrandshift{Q}{\Nshifts}$.)

We estimated $\QMCerror{\NQMC}{\Nshifts}$ for the setup described in \cref{app:compsetup}with $2048$ QMC points and $\Nshifts=20$ (i.e., 40,960 PDE solves in total) for $k = 10,20,30,40,50,60$. We set $h = 0.002$ for all of the computations (as $0.002 \approx 60^{-3/2}$) to avoid having to consider the effect of finite-element error. The quantities of interest we considered were:
\bit
\item The integral of the solution over the whole domain,
\item The value of the solution at the origin,
\item The value of the solution at the top-right corner of the domain, and
\item The $x$-component of the gradient of the solution at the top-right corner of the domain.
  \eit
  Observe that these QOIs require increasing regularity of the solution (the integral is defined for functions in $\LoD$, point evaluation for functions in $\Hfn{}{3/2 + \eps}{D}$ and point evaluation of the gradient for functions in $\Hfn{}{5/2+\eps}{D}$ (in 3-d - the corresponding function spaces are $\Hfn{}{1+\eps}{D}$ and $\Hfn{}{2+\eps}{D}$ in 2-d) for any $\eps > 0.$), and so computing for this range of quantities of interest will give a good insight into the behaviour of QMC applied to the Helmholtz equation for a wide range of QoIs\footnote{We can evaluate point values of $\uh$ because $\uh$ is continuous, and we evaluate $\grad\uh((1,1))$ as the value of $\grad \uh$ on the upper-rightmost mesh element; such an evaluation is possible due the structure of our mesh; see \cref{fig:grid}.}.
%%     That is, we randomly choose $\shifto,\ldots,\shiftNshifts$ points in $\cube{J}$ (the `shifts')rause the standard error estimator
%%     \beqs
%%     \mleft(\frac1{\nu\mleft(\nu-1\mright)} \sum_{s=1}^{\Nshifts} 
%%     \eeqs$h = 0.002$ (relation to $k=60$ - maximum?)

In \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha} we plot how $C$ and $\alpha$ depend on $k$, for the plots of the QMC error with increasing $\NQMC$ for each value of $k,$ see \cref{app:hhqmcconv}.

\begin{figure}[h]
    \centering
  \begin{subfigure}{\textwidth}
\input{integral-C-plot.pgf}
  \end{subfigure}
    \begin{subfigure}{\textwidth}
\input{integral-alpha-plot.pgf}
    \end{subfigure}
\caption{Plots of the dependence of $C$ and $\alpha$ on $k$ in \cref{eq:qmcerrorform} for $Q(u) = \int_D u$. Observe the $x$-axis is on a $\log_{10}$ scale, but $\log$ is the natural logarithm. \label{fig:integralCalpha}}
\end{figure}

\begin{figure}[h]
    \centering
  \begin{subfigure}{\textwidth}
\input{origin-C-plot.pgf}
  \end{subfigure}
    \begin{subfigure}{\textwidth}
\input{origin-alpha-plot.pgf}
    \end{subfigure}
\caption{Plots of the dependence of $C$ and $\alpha$ on $k$ in \cref{eq:qmcerrorform} for $Q(u) =  u(\bzero)$. Observe the $x$-axis is on a $\log_{10}$ scale, but $\log$ is the natural logarithm. \label{fig:originCalpha}}
\end{figure}

\begin{figure}[h]
    \centering
  \begin{subfigure}{\textwidth}
\input{top_right-C-plot.pgf}
  \end{subfigure}
    \begin{subfigure}{\textwidth}
\input{top_right-alpha-plot.pgf}
    \end{subfigure}
\caption{Plots of the dependence of $C$ and $\alpha$ on $k$ in \cref{eq:qmcerrorform} for $Q(u) = u((1,1))$. Observe the $x$-axis is on a $\log_{10}$ scale, but $\log$ is the natural logarithm.  \label{fig:toprightCalpha}}
\end{figure}

\begin{figure}[h]
    \centering
  \begin{subfigure}{\textwidth}
\input{gradient_top_right-C-plot.pgf}
  \end{subfigure}
    \begin{subfigure}{\textwidth}
\input{gradient_top_right-alpha-plot.pgf}
    \end{subfigure}
\caption{Plots of the dependence of $C$ and $\alpha$ on $k$ in \cref{eq:qmcerrorform} for $Q(u) = \gradu((1,1))$. Observe the $x$-axis is on a $\log_{10}$ scale, but $\log$ is the natural logarithm.  \label{fig:gradienttoprightCalpha}}
\end{figure}


\begin{table}[h]
  \centering
  \input{qmc-alpha-table}
  \caption{The quantities $\alphaz$ and $\alphao$ for differents QoIs, where the QMC error $Err \approx C \NQMC^{\alphaz - \alphao\log(k)}$.}\label{tab:qmcalpha}
  \end{table}

We use these computational results to determine how $\NQMC$ should increase with $k$, in order to keep the QMC error bounded. Based on QMC theory results for the stationary diffusion equation, e.g., \cite[Equation 4.2]{GrKuNuScSl:11}, we make the assumption that the QMC error (with one shift) satisfies
\beq\label{eq:qmcerrorform}
\QMCerror{Q}{1} = C \NQMC^{-\alpha},
\eeq
for some $C, \alpha > 0.$ Based on this assumption, \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha} plot $C$ and $\alpha$ for increasing $k$. (In \cref{app:hhqmcconv}, we plot the QMC error for increasing $\NQMC$ for each $k \in \set{10,20,30,40,50,60}$ and for each QoI---these plots allow us to determine the values of $C$ and $\alpha$ for each value of $k.$) For the QoIs that are point evaluations (\cref{fig:originCalpha,fig:toprightCalpha}), $C$ appears to be constant; we assume $C$ is constant in all of the following calculations.

Based on the evidence in \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha}, we conjecture
\beq\label{eq:alphaform}
\alpha(k) = \alphaz - \alphao\log(k),
\eeq
for some constants $\alphaz,\alphao > 0.$ (Throughout this \lcnamecref{sec:nbpcqmcnumerics}, $\log$ denotes the natural logarithm.) We fitted $\alphaz$ and $\alphao$, and have plotted the resulting line of best fit on \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha}. (Observe that the conjectured form \cref{eq:alphaform} cannot hold for $k$ very large, as then $\alpha(k)$ would be negative. Nevertheless, for the range of $k$ we consider in these numerical experiments, the form \cref{eq:alphaform} seems to give a good fit with the data.)

Having understood how the QMC error increases with $k$ for fixed $\NQMC$, we now use this knowledge to determine how one should increase $\NQMC$ with $k$ in order to keep the QMC error bounded. Recalling that we assume $C$ in \cref{eq:qmcerrorform} is constant, if we take
\beq\label{eq:Nform}
\NQMC(k) = \exp\mleft(\Ctilde \alpha(k)^{-1}\mright),
\eeq
for some constant $\Ctilde > 0$, then substituting \cref{eq:Nform} into \cref{eq:qmcerrorform}, we see that the QMC error should remain bounded, with
\beqs
\QMCerror{Q}{1} = C \exp\mleft(-\Ctilde\mright).
\eeqs

In our numerical experiments with increasing $\NQMC(k)$ below, we choose $\Ctilde$ so that $\NQMC(10) = 2048,$ as in our numerical experiments to determine the behaviour of the QMC error, we used $\NQMC = 2048$ (with 20 shifts). Also, for ease of computational set-up, in our numerical experiments we take the number of QMC points to be a power of 2, chosen so that $\NQMCactual(k) = 2^{M(k)},$ where
\beqs
M(k) = \round{\logtwo\mleft(\NQMC(k)\mright)}.
\eeqs

Based on the results for the QoIs in \cref{tab:qmcalpha} (excluding the results for the QoI being the integral of the solution, as this seems to display slightly different convergence characteristics), in our numerical experiments we take $\alpha(k) = 1.4 - 0.18 \log(k).$ The resulting values of $\NQMC$ are summarised in \cref{tab:nqmc}.

\begin{table}[h]
  \centering
  \input{num-qmc-points-table}
  \caption{The ideal and actual number of QMC points $\NQMC$, chosen so that the QMC error is empirically bounded for all $k$.}\label{tab:qmcpoints}
  \end{table}

Now that we understand how the number of QMC points should scale with $k$ in order to keep the QMC error bounded, we apply nearby preconditioning to QMC (with the number of points scaling as above) and observe how the computational work of this nearby-preconditioning-QMC (NP-QMC) algorithm scales with $k.$\ednote{Should I do the QMC calculations with multiple shifts, but this point scaling, again, and check that the estimate of the QMC error is roughly constant? It could take a while, and be rather expensive....}

As outlined above, we combine our sequential- and parallel-NPQMC algorithms:
\bit
\item We first use the sequential algorithm for low wave numbers (fixing the maximum number of GMRES iterations) and observe how the number of preconditioners (as a proportion of the number of QMC points) changes with $k$.
  \item We then use the parallel algorithm (with the above proportion of preconditioners) for higher values of $k.$
    \eit
    We remark that, in principle, one could use the sequential algorithm for all values of $k$, however, this would take an incredibly long time--- we see below that for $k=60$ we must perform $2^{20}$ Helmholtz solves; if we performed these solves sequentially, and each solve took 5 seconds, this computation would take on the order of 60 days to complete.

    The results for the sequential algorithm are summarised in \cref{tab:seq}, for $k$ up to 30. These results show that nearby preconditioning is very effective, with the number of preconditioners remaining (approximately) constant as a percentage of the total number of solves. This is the best result one could realistically hope for; in this case the decreasing (like $1/k$) radius over which nearby preconditioning is effective is offset by the growing number of QMC points, meaning the number of preconditioners is constant as a fraction of the total number of solves.

    Based on these sequential results, we then used the parallel algorithm with a target proportion of preconditioners of 0.2\%. (Although recall from our discussion above that the actual proportion of preconditioners used can vary due to rounding in the algorithm.) The results of these computations are summarised in \cref{tab:par}. We observe that the fraction of preconditioners is approximate 0.2\%, but the maximum (and average) number of GMRES iterations appears to grow slowly with $k.$ This may be because the placement of the preconditioning points is not optimal with respect to the $\dQMC$ metric; we conjecture that oversampling the number of preconditioners needed (for example, taking a proportion of 0.5\%) may result in a bounded number of GMRES iterations\ednote{Both---Should I run these computations?} Nevertheless, we see that nearby preconditioning gives considerable speedup, drastically reducing the number of preconditinoers that must be calculated.

    \begin{table}
  \centering
  \input{nbpc-qmc-sequential-table}
  \caption{Results applying our sequential nearby-preconditioning-Quasi-Monte-Carlo algorithm, with the maximum number of GMRES iterations $=10$.}\label{tab:nbpcqmcseq}
\end{table}

\begin{table}
  \centering
  \input{nbpc-qmc-parallel-table}
  \caption{Results applying our parallel nearby-preconditioning-Quasi-Monte-Carlo algorithm with the target proportion of preconditioners as $0.2$\%.}\label{tab:nbpcqmcspar}
  \end{table}


    In conclusion, we see that nearby preconditioning gives very significant speedup when applied to a QMC model problem. We therefore expect that this technique will give significant speed up when applied to other, more realistic problems.
    

    
\section{Extension of the results to the truncated exterior Dirichlet problem}\label{sec:TEDP}

%\subsection{Definition of the TEDP and analogues of the results in \cref{sec:3}}

\paragraph{The impedance boundary $\Gamma_I$.} By comparing \cref{eq:src,eq:ibc}, we see that, in the case $g_I=0$, the TEDP approximates the DtN operator $T_R$ by $\ri k$. Indeed, by using Green's first identity and the definition of the normal derivative (see, e.g., \cite[Lemma 4.3]{Mc:00}), show that the boundary condition on $\Gamma_I$ imposed in the variational problem \cref{prob:vtedp} is 
%In this BVP, the DtN operator $T_R$ Sommerfeld radiation condition 
\beq\label{eq:imp}
\dudnu - \ri k\gamma u = g_I \ton \Gamma_I.
\eeq
where $\nu$ is the unit outward-pointing normal vector to $\Omega$ on $\Gamma_I$.

\paragraph{Existence and uniqueness of a solution to the TEDP.} The sesquilinear form $\aT(\cdot,\cdot)$ defined in \cref{eq:aT} satisfies the G\aa rding inequality \cref{eq:gardingbrief}, and existence and uniqueness of a solution to the TEDP follow under the same condition on $A$ (piecewise-Lipschitz) as for the EDP, as discussed in \cref{sec:vpGm}; in the case of Lipschitz scalar $A$, these unique-continuation arguments are summarised in \cite[\S2]{GrSa:18}.

\paragraph{Finite-element/Galerkin solution.}
The Galerkin matrix is defined exactly as in \cref{eq:matrixAdef}, except that 
\beq\label{eq:NTEDP}
\big(\Nmat\big)_{ij}:= \ri k\int_{\Gamma_I}  (\gamma\phi_i) \,\gamma \phi_j.
\eeq

\paragraph{The adjoint sesquilinear form.} For the TEDP, the adjoint sesquilinear form is given by 
\beq\label{eq:TEDPadjoint}
a^*(u,v) := \int_{\DR} 
\Big((A \grad u)\cdot\grad \vb
 - k^2 n u\vb\Big) +\ri k\int_{\Gamma_I} \gamma u\, \overline{\gamma v};
\eeq
then \cref{eq:A*} holds (with $\Nmat$ now given by \cref{eq:NTEDP}, and the analogue of \cref{lem:adjoint} follows in a straightforward way.


\paragraph{The analogues of \cref{cond:1nbpc,cond:2}.}
The statement of the TEDP analogues of \cref{cond:1nbpc,cond:2} are the same as for the EDP, apart from the following.
\ben
\item
$\supp \,f$ need not be a subset of $\widetilde{\Omega}$ (i.e.~the support of $f$ can go up to the impedance boundary $\Gamma_I$), and
\item the assumption $g_I= 0$ needs to be added to \cref{cond:1nbpc} and Part (i) of \cref{cond:2}.
\een
 Note that, since $a(\cdot,\cdot)$ for the TEDP satisfies the same G\aa rding inequality \cref{eq:gardingbrief} as the $a(\cdot,\cdot)$ for the EDP, \cref{lem:H1} holds for the TEDP under the TEDP-analogue of \cref{cond:1nbpc}.

\paragraph{The main results \cref{thm:1,cor:1}.}
Since \cref{cond:1nbpc,cond:2} are essentially unchanged from the EDP case, \cref{lem:keylemma1,lem:keylemma2} hold for the TEDP, and thus so do \cref{thm:1,cor:1,cor:1a}.

\paragraph{The PDE results \cref{thm:2} and \cref{lem:sharp}.}

The PDE bound \cref{thm:2} relies only on \cref{lem:H1}, which, as stated above, also holds for the TEDP. Therefore \cref{thm:2} holds for the TEDP under the TEDP-analogue of \cref{cond:1nbpc} described above. The construction in \cref{lem:sharp} to show sharpness of the bound in \cref{thm:1} (at least when $\Aso= \Ast= I$) also holds for the TEDP; this is because one can choose the supports of $\chi$ and $\widetilde{\chi}$ to be contained inside $\widetilde{\Omega}$, and then $u^{(1)}$ and $u^{(2)}$ defined in \cref{lem:sharp} satisfy the impedance boundary condition \cref{eq:imp} on $\Gamma_I$.

%% \paragraph{When the TEDP-analogue of \cref{cond:1nbpc} holds.}

%% In \cref{sec:cond1hold} we discussed 4 situations (Cases 1-4) where \cref{cond:1nbpc} is proved to hold for the EDP. We now discuss the TEDP-analogues of these.
%% %Cases 1, 3, and 4 (there is no proof yet for the TEDP-analogue of Case 2).

%% \emph{Cases 1 and 2: $\Aso$, $\nso$, and $\Gamma_I$  are $C^\infty$.} 
%% With the rays defined as in the EDP case (by the Melrose--Sj{\"o}strand generalized bicharacteristic flow 
%% \cite[\S24.3]{Ho:85}), the TEDP-analogue of nontrapping for the EDP is the assumption that 
%% every ray eventually hits the boundary at a \emph{non-diffractive point} (defined in \cite[Page 1037]{BaLeRa:92}). Note that, in the case $\Dm=\emptyset$ $\Aso= I$, and $\nso=1$, every ray eventually hits the boundary at a non-diffractive point by \cite[Lemma 5.3]{BaSpWu:16}.
%% Under the additional assumption that $\nso= 1$, \cref{cond:1nbpc} follows from the results of \cite{BaLeRa:92} by combining \cite[Theorem 1.8]{BaSpWu:16} and \cite[Remark 5.6]{BaSpWu:16}, but $C^{(1)}_{\rm bound}$ is not given explicitly.

%% \emph{Case 3: $\Dm$ is starshaped with respect to the origin, $\Aso$ and $\nso$ are Lipschitz and satisfy radial monotonicity-like conditions.}
%% When $\Gamma_I$ is also starshaped with respect to the origin and $A$ and $n$ satisfy \cref{eq:A1nbpc} and \cref{eq:n1nbpc} respectively (with $\Dp$ replaced by $\Omega$), 
%% \cite[Theorem A.6(i)]{GrPeSp:19} proves that
%% \cref{cond:1nbpc} holds, with an explicit expression for $C^{(1)}_{\rm bound}$. Analogous results when (a) $2\Aso - (\bx\cdot\nabla)\Aso \geq \mu_1$ and $\nso= 1$,
%% and  (b) $\Aso= I$ and  $2\nso + \bx \cdot \nabla \nso \geq \mu_2$, 
%% are contained in \cite[Theorem A.6(ii)]{GrPeSp:19} and \cite[Theorem A.6(iii)]{GrPeSp:19} respectively.
%% When $A$ is scalar, these results were also proved in \cite[Theorem 1]{BrGaPe:17} and, when $\Aso= I$ and $\Dm=\emptyset$, also in \cite[Theorem 3.2]{GrSa:18}.

%% \emph{Case 4: %\item[Case 4:]
%%  $\Aso$ and $\nso$ are allowed to be discontinuous.}
%% %\een
%% \cref{cond:1nbpc} is proved in \cite{CaVo:10} (without an explicit expression for $C^{(1)}_{\rm bound}$) when $\Dm$ is $C^\infty$ and nontrapping, $\Gamma_I$ is $C^\infty$, $\Aso= I $, and $\nso$ is a piecewise-constant, monotonically non-decreasing function, jumping on interfaces that are $C^\infty$ with strictly positive curvature.
%% Recall from \cref{cond:1nbpc} that \cite[Theorem 2.7]{GrPeSp:19} proves that \cref{cond:1nbpc} holds for the EDP (with an explicit expression for $C^{(1)}_{\rm bound}$) when $\Dm$ is starshaped with respect to the origin, $A$ and $n$ are $L^\infty$, with $A$ monotonically \emph{non-increasing} in the radial direction, and $n$ monotonically \emph{non-decreasing}. This proof can be extended to the TEDP, with the additional assumption that $\Gamma_I$ is star-shaped with respect to the origin; see the discussion in \cite[Section A.2]{GrPeSp:19}.

%\cref{cond:1nbpc} is proved, with an explicit expression for $C^{(1)}_{\rm bound}$, when 

%\newpage
%
%\section*{Questions for Th\'eo}
%
%\ben
%\item At the place marked A on the scanned pages, you seem to use the inequality 
%\beq\label{eq:Theo1}
%\vert\vert\vert \xi - \cP_h \xi\vert\vert\vert \lesssim h^\alpha \N{u_\phi- \cP_h u_\phi}_{0,\Omega}.
%\eeq
%\een
%
%\newpag

\section*{Owen to do list}
\ben
\item Varying  $\|\Aso-\Ast\|_{L^\infty}$ and $\|\nso-\nst\|_{L^\infty}$ in standard GMRES.
\item Computations where $\|\Aso-\Ast\|_{L^\infty}$ and $\|\nso-\nst\|_{L^\infty}$ are sometimes large; is having the standard deviations of these $\sim 1/k$ good enough for $k$-independent GMRES iterations?
\item ***on backburner*** Checking under what conditions (if any) Part (ii) \cref{cond:2} holds by running the following experiment:
%\item Exciting experiments for random $n$ that you told us about last week.
%\item In the weighted norm, the condition on $A$ is ``$k \|\Aso-\Ast\|_{L^\infty}$ sufficiently small" but in the Euclidean norm the best we have so far is ``$h^{-1} \|\Aso-\Ast\|_{L^\infty}$ sufficiently small". You indicated before that experiments seemed to indicate that ``$k \|\Aso-\Ast\|_{L^\infty}$ sufficiently small" seemed correct for the Euclidean norm too. The next time we meet, can you show me these results please?
%\item Please run the following numerical experiment.
\bit
\item TEDP with $\Omega$ a square/rectangle.
\item $\Aso$ being at least Lipschitz (but smooth is fine). To keep things simple, just take scalar- (as opposed to matrix-) valued $\Aso$ and don't worry about making it nontrapping.
\item Smoothness of $\nso$ doesn't really matter, just take smooth in the first instance for simplicity (and also don't worry about nontrapping).
\item $\Vhp$ piecewise linear.
\item Linear system $\Amato \bu = \Smat_{A} \balpha$ for some arbitrary complex-valued vector $\balpha$ and some arbitrary $A\in L^\infty$. (I claim this corresponds to the problem described in Part (ii) of \cref{cond:2},  but please check this!)
\item For each $\Aso, \nso, \balpha$, solve linear system for increasing values of $k$, first with $h\sim k^{-2}$, and then with $h\sim k^{-3/2}$.
\item Goal: see if the bound \cref{eq:bound4} holds, using 
\beqs
\N{\sum_j \alpha_j (A\nabla \phi_j)}_{\LtDR} \quad \text{ as a proxy for } \quad \N{F}_{(\HokDR)'}.
\eeqs
\eit
%\item Varying  $\|\Aso-\Ast\|_{L^\infty}$ and $\|\nso-\nst\|_{L^\infty}$ in \emph{weighted} GMRES.
\een

