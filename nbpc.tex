\section{Introduction and Motivation from UQ}\label{sec:intronbpc}

\subsection{Motivation from uncertainty quantification for the Helmholtz equation} 
Consider the stochastic Helmholtz equation 
\beq\label{eq:nbpchh}
\nabla\cdot\big(A(\omega,\bx) \nabla u(\omega,\bx) \big) + k^2 n(\omega,\bx) u(\omega,\bx) =-f(\bx), \quad \bx\in\Dp,
\eeq
as defined in \cref{chap:stochastic}. If $Q(u)$ is some quantity of interest of the solution, then the simplest way to approximate $\EXP{Q(u)}$ is via a sampling-based method, i.e. using the approximation
\beq\label{eq:samplingexp}
\EXP{Q(u)} \approx \frac1N \sum_{l=1}^N Q(u(\omegasl)),
\eeq
where the $\omegasl$ are elements of the sample space $\Omega.$ To calculate the right-hand side of \cref{eq:samplingexp},  one must solve many deterministic Helmholtz problems, corresponding to different samples $\omega^l$, i.e. corresponding to different realisations of the coefficients $A(\omega,\cdot)$ and $n(\omega,\cdot)$.
Solving all these deterministic problems is a very computationally-intensive task because linear systems arising from discretisations of the Helmholtz equation are notoriously difficult to solve; see the discussion in \cref{sec:numsolve} above. In particular, direct solvers involving a sparse LU decomposition of the linear system have a computational cost of the order $\cO\mleft(N^{3/2}\mright)$ in 2-d  and $\cO\mleft(N^2\mright)$ in 3-d (see \cite[Section 1]{DuErRe:76} and \cite[Equation 3]{DuErRe:76}, respectively, for a particular regular grid).

However, if one already has access to the LU decomposition, then the cost of applying a direct solver using the LU decomposition is much cheaper; $\cO\mleft(N\log N\mright)$ in 2-d \cite[Section 1]{DuErRe:76} and $\cO\mleft(N^{4/3}\mright)$ in 3-d \cite[Equation 4]{DuErRe:76}. In the context of Uncertainty Quantification for the Helmholtz equation this reduction in cost when one has access to an LU decomposition suggests the following question: When can the LU decomposition corresponding to a particular realisation of \cref{eq:nbpchh} be used as a preconditioner for other realisations of \cref{eq:nbpchh}?

This question of reusing preconditioners is more widely applicable than just for LU decompositions. For \emph{any} preconditioner for the Helmholtz equation, one could ask when the preconditioner corresponding to one realisation of \cref{eq:nbpchh} can be re-used for other realisations. In this \lcnamecref{chap:nbpc}, for simplicity, we restrict our attention to the case where the preconditioner is an exact LU decomposition.

One expects this reuse of the preconditioner to work well if the two realisations are `nearby' in some sense. This idea of reusing preconditioners is the `nearby preconditioning' strategy proposed in this \lcnamecref{chap:nbpc}. To analyse this `nearby preconditioning' strategy rigorously, we first consider the following problem and question.

%% Therefore, in this \lcnamecref{chap:nbpc} we consider whether one can reuse the preconditioner from one solve for subsequent solves, and thereby reduce the overall computational effort.  

%%  We note that the above suggestion of re-using preconditioners should, at first glance, reduce the computational effort needed because Helmholtz preconditioners typically require more effort to \emph{construct} than they do to \emph{apply}.

%%  For example, if we calculate the action of $\AmatI$ exactly (say via an $LU$ factorisation) then (in 3-d) the cost of calculating the preconditioner is $\cO\mleft(N^2\mright)$ and the cost of applying the preconditioner is $\cO\mleft(N^{4/3}\mright)$ (where $N$ is the number of unknowns), see, e.g., \cite[Section 4]{GaZh:19}. This relationship between the cost of construction and application is also seen in recent Helmholtz preconditioners such as a sweeping preconditioners and a domain-decompostion preconditioners (see the recent review article \cite{GaZh:19} for an overview of many types of Helmholtz preconditioners). For these preconditioners one still performs $LU$-factorisations (or direct solves) on subdomains of $\Dp$ (see, e.g., \cite[Section 2]{GaZh:19}). Therefore resuing preconditioners, and thereby calculating fewer $LU$ factorisations may still lead to considerable savings. .

Let $\Aj, \nj$, $j=1,2$ satisfy the properties of $A$ and $n$ in \cref{prob:vedp} or \cref{prob:vtedp} (we will prove results for both problems), with $\uj$ the corresponding solution and $\Dm,$ $f$, etc. as in \cref{prob:vedp} or \cref{prob:vtedp}. Let $\Amatj$, $j=1,2,$ be the Galerkin matrices for the corresponding $h$-finite-element discretisations (see \cref{eq:matrixAjdef} below for a precise definition of $\Amatj$). We seek to answer:

% Inspired by the usage of enumitem here: https://tex.stackexchange.com/a/58714
\ben[label=Q1., ref=Q1]
\item\label[itemblank]{it:nbpcq1} How small must $\N{\Aso - \Ast}$ and 
$\N{\nso - \nst}$ be (in some norm to be defined, in terms of $k$-dependence) for GMRES 
applied to $(\Amat^{(1)})^{-1}\Amat^{(2)}$ to converge in a $k$-independent number of iterations
%$ to be a good preconditioner for $\Amat^{(2)}$
 for arbitrarily large $k$? 
 \een

 The rigorous answer of \cref{it:nbpcq1} is contained in \cref{cor:1,cor:1a} below. However, an informal statement of the answer to \cref{it:nbpcq1} is that if
 \beq\label{eq:informalconditionnbpc}
k\,
\NLi{\Aso-\Ast} \quad\text{ and } \quad k\,\NLi{\nso-\nst}
%\Big) 
\quad\text{ are both sufficiently small,}
\eeq
then GMRES applied to $(\Amat^{(1)})^{-1}\Amat^{(2)}$ in some weighted norm converges in a $k$-independent number of iterations (and a similar result for standard GMRES with \cref{eq:informalconditionnbpc} replaced by a slightly stronger condition).

 \subsection{Outline of the chapter}% Don't know why cleveref didn't work here
In \cref{sec:main} we state and discuss the main results of this \lcnamecref{chap:nbpc} on the effectiveness of nearby preconditioning, and give their analogues on the PDE level. In \cref{sec:num} we describe numerical experiments investigating the sharpness of the nearby-preconditioning results in \cref{sec:main}. In \cref{sec:3} we prove the results in \cref{sec:main}. In \cref{sec:weaknorm} we extend the results in \cref{sec:main} to hold in weaker spatial norms, and we describe numerical experiments investigating the sharpness of these new results. In \cref{sec:nbpcqmc} we then apply the idea of nearby preconditioning to a Quasi-Monte-Carlo (QMC) method for the stochastic Helmholtz equation; in \cref{sec:nbpcqmcnum} we describe two algorithms for applying nearby preconditioning to QMC methods and in \cref{sec:nbpcqmcnumerics} we describe numerical experiments on the effectiveness of nearby preconditioning applied to QMC methods. In \cref{sec:nbpclitreview} we briefly review the related literature. Finally, in \cref{sec:nbpcstochastic}, we show how one can prove probabilistic results about the behaviour of nearby preconditioning, and we describe numerical experiments that investigate these probabilistic results.

\section{Statement of the main results}\label{sec:main}

\subsection{Definition of variational problems and conditions used to prove main results}\label{sec:vpGm}
As this \lcnamecref{chap:nbpc} concerns finite-element discretisations of the Helmholtz equation, we will work with the variational formulation of the Helmholtz equation. However, because the arguments we use do not directly rely on the boundary condition used to truncate the computational domain, we will state our Helmholtz problems in sufficient generality to include both the EDP (\cref{prob:vedp}) and TEDP (\cref{prob:vtedp}) above.

\bprob[General variational Helmholtz problem]\label{prob:vgen}
Let $D, A,$ and $n$ be as in \cref{prob:tedp}. We say $u \in \HozDD$ satisfies the \defn{variational formulation of a general exterior Dirichlet problem} with $\gD = 0$ if
\beq\label{eq:vgen}
\aG(u,v) = \LG(v) \tfa v \in \HozDD,
\eeq
where
\beq\label{eq:agen}
\aG(w,v) \de \int_{D} \mleft(\mleft(A \grad w\mright)\cdot\grad \vbar - k^2 n\minispace w \vbar\mright) - \DPGI{\T \trGI w}{\trGI v},
\eeq
$\T:\HhGI\rightarrow \HmhGI$ is a bounded linear map,  $\DPGI{\cdot}{\cdot}$ is the duality pairing on $\GI,$ and $\LG  \in \HozDDs.$
\eprob

\bre[\Cref{prob:vgen} is a generalisation of \cref{prob:vedp,prob:vtedp}]
With the exception of some overlap in notation, it is straightforward to see that appropriate choices of $D,$ $\GI$, $\T$, and $\LG$ allow \cref{prob:vgen} to be either \cref{prob:vedp} or \cref{prob:vtedp}. Taking $D = D$, $\GI = \GR$, $\T = \DtN$ and $\LG(v) = \int_{D} f\minispace\vbar$ (for $f$ as in \cref{prob:vedp}) in \cref{prob:vgen}, we see \cref{prob:vgen} becomes \cref{prob:vedp}. Additionally, taking $D$ and $\GI$ in \cref{prob:vgen} to be the same as the $D$ and $\GI$ in \cref{prob:vtedp}, taking $\T=ik,$ and $\LG(v) = \int_{D} f\minispace\vbar + \int_{\GI} \gI \minispace\trGI \vbar$ (for $f$ and $\gI$ as in \cref{prob:vtedp}), \cref{prob:vgen} becomes \cref{prob:vtedp}.
\ere

\bre[\Cref{prob:vgen} allows for other boundary conditions]
The strength of the general formulation in \cref{prob:vgen} is that it allows us to treat a wide variety of Helmholtz problems at once. Indeed, \emph{any} Helmholtz problem that can be written in the form \cref{eq:vgen,eq:agen} and satisfies \cref{cond:1nbpc,cond:2} below can be treated using the analysis in this \lcnamecref{chap:nbpc}.
\ere 

For the remainder of this \lcnamecref{chap:nbpc}, we let $(\Vhp)_{h>0}$ be the family of finite-element spaces.

\bas[Properties of finite-element spaces]
We assume $(\Vhp)_{h>0}$ is a family of finite-dimensional subspaces of $\HozDD$, whose union is dense in $\HozDD$. Moreover, we assume $\Vhp$ consists of nodal finite-element functions given by piecewise-polynomials on a quasi-uniform simplicial mesh $\cTh$ with mesh-size $h$
%\ednote{Euan says: have problem that want to allow $C^{1,1}$ $\Dm$, so that statements later about $H^2$ regularity are covered, but easiest to define triangulation and hence subspaces on Lipschitz domains -- Euan to discuss with Ivan}
and fixed polynomial degree $p$.
\eas
Note that the dimension $N$ of $\Vhp$ satisfies $N\sim h^{-d}$, with hidden constant dependent on $p$. (The assumption of quasi-uniformity can, in principle, be relaxed, see \cref{rem:ggsqu} below.) As in \cref{rem:crimes} above, we ignore any variational crimes resulting from this discretisation. We now define the finite-element approximation of \cref{prob:vgen}.
\bprob[Finite-element approximation of \cref{prob:vgen}]\label{prob:fevgen}
    Find $\uh \in \Vhp$ such that
\beq\label{eq:galerkin}
\aG(\uh,\vh) = \LG(\vh) \tforall \vh \in \Vhp.
\eeq
We say that $\uh \in \Vhp$ is the \defn{finite-element approximation of $u$} (the solution to \cref{prob:vgen}).
\eprob
%Observe that implicit in our use of $\aE$ in \cref{eq:galerkin} is the fact that we are realising the Dirichlet-to-Neumann map $\TR$ exactly on $\GR.$
%Definition of Galerkin method

\subsection{Definition of finite-element matrices, weighted norms, and weighted GMRES} 

\subsubsection{Finite-element matrices and weighted norms}

We now define the matrices associated with our finite-element discretisation. Let $\{\phi_i, i= 1, \ldots, N\}$ be a basis for $\Vhp$ with each $\phi_i$ \emph{real-valued}.
Let 
\beq\label{eq:matrixSjdef}
\big(\Smat_{A}\big)_{ij}\de \int_D \big(A \nabla \phi_j)\cdot\nabla \phi_i, \quad
\big(\Mmat_{n}\big)_{ij}\de \int_D n\,\phi_i\, \phi_j,
\quad\tand\quad
\big(\Nmat\big)_{ij}\de \int_{\GR} \T (\gamma\phi_j) \,\gamma \phi_i
\eeq
be the stiffness, domain-mass, and boundary-mass matrices, respectively. Note that both $\Smat_A$ and $\Mmat_n$ are \emph{real-valued}, but in general $\Nmat$ is \emph{complex-valued} (because both the  DtN operator $\DtN$ and the impedance operator $ik$ are complex-valued).
Let
\beq\label{eq:matrixAdef}
\Amat \de \Smat_{A} - k^2 \Mmat_{n} - \Nmat,
\eeq
and let $u_h\de \sum_j \uvec_j \phi_j$. Then \cref{eq:galerkin} implies that the coefficient vector $\uvec = \mleft(\uveci\mright)_{i=1}^N \in \CCN$ satisfies
\beqs
\Amat \uvec = \fvec,
\eeqs
where $(\fvec)_i \de \FE(\phi_i)$.
Similarly to above we let 
\beq\label{eq:matrixAjdef}
\Amatj \de \Smat_{A^{(j)}} - k^2 \Mmat_{n^{(j)}} - \Nmat.
\eeq

Our main results about \cref{it:nbpcq1} are \cref{cor:1,cor:1a} below. \Cref{cor:1a} gives results in the \emph{Euclidean} norm on matrices, denoted by $\|\cdot\|_2$ (induced by the Euclidean norm on vectors), whereas \Cref{cor:1} gives results in the \emph{weighted} norms $\NDmatk{\cdot}$ and $\NDmatkI{\cdot}$. These weighted norms are induced by the corresponding vector norms
\beq\label{eq:Dk}
\NDmatk{\vvec}^2\de \big( \Dmatk \vvec, \vvec\big)_2 = %\big( (\Smat_I + k^2 \Mmat_1)\vvec,\vvec\big)_2 
\N{v_h}^2_{\HokD}
\quad \tand
\quad \NDmatkI{\vvec}^2\de \big( \Dmatk^{-1} \vvec, \vvec\big)_2 %= %\big( (\Smat_I + k^2 \Mmat_1)\vvec,\vvec\big)_2 
%\N{v_h}^2_{\HokD}
\eeq
where $\Dmatk$ is given in terms of familiar finite-element stiffness- and mass-matrices by
\beqs
\Dmat_k\de \Smat_I + k^2 \Mmat_1,
\eeqs
and $\vh =\sum_i \sfvi \phii$, where the $\phii$ are the finite-element basis functions.


As described in \cref{sec:wpdisc}, the PDE analysis of the Helmholtz equation naturally takes place in the norm $\NHokD{\cdot}$, and \cref{eq:Dk} shows that the norm $\NDmatk{\cdot}$ is simply the norm on the finite-element space induced by  $\NHokD{\cdot}$. %See \cref{eq:Dk2} and \cref{eq:Dk3} below for this norm expressed in terms of the Euc
The norms $\NDmatk{\cdot}$ and $\NDmatkI{\cdot}$ recently appeared in results about the convergence of domain-decomposition methods %in this norm are proved 
for the Helmholtz equation \cite{GrSpVa:17,GrSpZo:18}, and a related norm appeared in similar results for the time-harmonic Maxwell equations \cite{BoDoGrSpTo:19}. 

The statement and proof our main results, \cref{cor:1,cor:1a} will require the following \lcnamecref{lem:normequiv}.

\ble[Norm equivalences of FE functions]\label{lem:normequiv}
There exist $m_\pm >0$ and $s_+>0$, independent of $h$ (but dependent on $p$), such that
\beq\label{eq:normequiv1}
m_- h^{d/2} \N{\vvec}_2 \leq \N{v_h}_{\LtD} \leq m_+ h^{d/2} \N{\vvec}_2,
\eeq
and
\beq\label{eq:normequiv2}
\N{\nabla v_h}_{\LtD} \leq s_+ h^{d/2-1} \N{\vvec}_2,
\eeq
for all finite-element functions $v_h =\sum_i \vvec_i \phi_i \in \Vhp$, with $\vvec = \mleft(\vveci\mright)_{i=1}^N \in \CCN.$
\ele

The proof of \cref{lem:normequiv} is on \cpageref{page:normequivpf} below.

Written in terms of the matrices $\Mmat_1$ and $\Smat_I$ defined in \cref{eq:matrixSjdef}, the bounds \cref{eq:normequiv1} and \cref{eq:normequiv2} are, respectively, the bounds
\beqs
(\Mmat_1 \vvec,\vvec)_2 \sim h^d \N{\vvec}^2_2 \quad\tand\quad (\Smat_I \vvec,\vvec)_2 \lesssim h^{d-2} \N{\vvec}^2_2.
\eeqs


\bre[Relaxing the assumption of quasi-uniformity]\label{rem:ggsqu}
We assume that $\set{\Th}_{h>0}$ is a quasi-uniform family of meshes so that the proof of \cref{lem:normequiv} is straightforward. However, this assumption can almost certainly be relaxed. In \cite{GaGrSp:15} (on which the bulk of the arguments in this \lcnamecref{chap:nbpc} are based) Gander, Graham, and Spence prove results both for quasi-uniform meshes and also for shape-regular meshes (see \cite[Sections 3.4 and 4.1.2]{GaGrSp:15}). Given the results in \cite{GaGrSp:15} for shape-regular meshes are analagous to those they obtain for quasi-uniform meshes, we expect the results in this \lcnamecref{chap:nbpc} can also be extended to shape-regular meshes. However, we note that \cite{GaGrSp:15} only contains bounds on preconditioned mass matrices (analogous to \cref{lem:keylemma1} below) but not preconditioned stiffness matrices (analogous to \cref{lem:keylemma2} below. Therefore it remains open to prove that our results in this \lcnamecref{chap:nbpc} can be extended in their entirety to shape-regular meshes.
\ere



\subsubsection{Weighted GMRES}

We now give the set-up for weighted GMRES, first introduced in by Essai in \cite{Es:98}; we largely follow \cite[Section 5]{GrSpVa:17}. Consider the abstract  linear system 
% \begin{equation*}
$\matrixC \xvec = \dvec$
%\end{equation*}
in $\mathbb{C}^N$, where $\matrixC \in \CC^{N\times N}$ is invertible. Let $\xvecz$ be the initial guess, and define the initial residual $\rvec^0 \de \dvec- \Cmat \xvec^0$ and the standard Krylov spaces:  
\beqs  
\cK^m(\Cmat, \rvec^0) \de \mathrm{span}\big\{\matrixC^j \rvec^0 : j = 0, \ldots, m-1\big\}.
\eeqs
Analagously to the definition of $\NDk{\cdot}$ above, let $(\cdot , \cdot )_{\Dmat}$ denote the inner product on $\CC^n$ 
induced by some Hermitian positive-definite matrix $\Dmat$, i.e.~
%\begin{equation*}
$(\vvec,\wvec)_{\Dmat} \de (\Dmat \vvec, \wvec)_2,$
%\end{equation*} 
and let $\Vert \cdot \Vert_\Dmat$ be the induced norm. For $m \geq 1$, define the $m$th GMRES iterate $\xvec^m$  to be  the unique element of $\cK^m$ satisfying  the  
 minimal residual  property: 
$$ \ \Vert \rvecm \Vert_\Dmat \de \Vert \dvec - \matrixC \xvec^m \Vert_\Dmat \ = \ \min_{\xvec \in \cK^m(C, \rvec^0)} \Vert {\dvec} - {\matrixC} {\xvec} \Vert_\Dmat. $$
Observe that when $\Dmat = \Imat$ this is the standard GMRES algorithm. We also note that in general, weighted GMRES requires the use of weighted Arnoldi process, also introduced by Essai in \cite{Es:98}, see also the alternative implementations of the weighted Arnoldi process in \cite{GuPe:14}.




\subsection{Main results}

\Cref{cor:1,cor:1a} are proved under the following two \lcnamecrefs{cond:1nbpc}, which are the minimal conditions needed for the proof of \cref{cor:1,cor:1a}. Therefore, in particular, \cref{cond:2} is a very weak condition on the finite-element space $\Vhp,$ since it does not even require convergence (for fixed $k$) as the mesh is refined.

\begin{condition}[Nontrapping bound on $u^{(1)}$]\label{cond:1nbpc}
$\Aso, \nso,$ and $D$ are such that, given $f\in L^2(D)$
  the solution of \cref{prob:vgen} with
  \beq\label{eq:LGf}
  \LG(v) = \int_D f\vbar,
  \eeq
$u^{(1)}$, exists, is unique, and, given $k_0>0$, $u^{(1)}$ satisfies the bound 
\beq\label{eq:bound1}
\big\|u^{(1)}\big\|_{\HokD} \leq C^{(1)}_{\rm bound} \N{f}_{L^2(D)} \quad \tfa k\geq k_0,
\eeq
where $C^{(1)}_{\rm bound}$ is independent of $k$, but dependent on $\Aso, \nso, D$, and $k_0$.
\end{condition}

\begin{condition}[$k$-independent accuracy of the FE solution for $a^{(1)}(\cdot,\cdot)$]
\label{cond:2}

\

\noindent\ben

\item\label[itempart]{it:femasspt1} Given $\kz>0$, $h$ and $p$ are chosen to depend on $k$ such that for all $k \geq \kz$, if $f= n\sum_j \alpha_j\phi_j$ for some $\alpha_j \in \CC$ and  $n\in \LiDR$  (i.e.~$f$ is an arbitrary element of $\Vhp$ multiplied by $n$), then 
  \bit
\item The solution $u_h$ of \cref{prob:fevgen} (with $\aG = \aGo$, and $\LG(v)$ given by \cref{eq:LGf}) exists and is unique, and
\item The error bound
  \beq\label{eq:bound3}
\N{u-u_h}_{\HokD} \leq C^{(1)}_{\rm FEM1} \N{f}_{\LtD} \quad, 
\eeq
holds, where $C^{(1)}_{\rm FEM1}$  is independent of $k$ and $h$, but dependent on $\Aso, \nso, D, k_0$, and $p$.
  \eit

\item Given $\kz>0$, $h$ and $p$ are chosen to depend on $k$ such that for all $k \geq \kz$, if $\LG(v)= (A\nabla \widetilde{f},\nabla v)_{\LtD}$, where $A\in \LiDRRdtd$ and $\widetilde{f} \de \sum_j \alpha_j \phi_j$ with $\alpha_j\in \CC$  (i.e.~$\widetilde{f}$ is an arbitrary element of $\Vhp$), then,
  \bit
\item The solution $u_h$ of \cref{prob:fevgen} with $\aG = \aGo$ exists and is unique, and
\item The error bound
\beq\label{eq:bound4}
\N{u-u_h}_{\HokD} \leq C^{(1)}_{\rm FEM2}\,k\, \N{\LG}_{\HokDs} \quad, 
\eeq
holds, where $C^{(1)}_{\rm FEM2}$  is independent of $k$ and $h$, but dependent on $\Aso, \nso, D, k_0$, and $p$.  
  \eit
\een
\end{condition}

For details of when \cref{cond:1nbpc,cond:2} are satisfied (for \cref{prob:vedp,prob:vtedp}), see \cref{sec:pdetheory} (for \cref{cond:1nbpc}) and \cref{sec:helmfedisc} (for \cref{cond:2} \cref{it:femasspt1}). \Cref{cond:1nbpc,cond:2} can be informally stated as 
\bit
\item the obstacle $\Dm$ and the coefficients $\Aso$ and $\nso$ are such that $u^{(1)}$ exists, is unique, and the problem is \emph{nontrapping} (in the sense described in  \cref{sec:wpdisc} above), and
\item the meshsize $h$ and polynomial degree $p$ in the finite-element method are chosen to depend on $k$ to ensure that the 
finite-element approximation to the solution of the problem with coefficients $\Aso$ and $\nso$ exists, is unique, and has bounded error in the $H^1_k$-norm as
%Galerkin method (with coefficients $\Aso$ and $\no$) 
$k\tendi$. 
\eit 


\bre[\Cref{eq:bound4} has the same $k$-dependence as \cref{eq:bound3}]\label{rem:yesitis}
Observe that the bound \cref{eq:bound4} has the same $k$-dependence as \cref{eq:bound3} despite the fact that a factor $k$ appears on the right-hand side. If $\LG(v) = \int_{D} f\vbar,$ then
\begin{align*}
  \NHokDs{\LG} = \sup_{v \in \HozDD} \frac{\abs{\LG(v)}}{\NHokD{v}} &\leq\sup_{v \in \HozDD} \frac{\NLtD{f}\NLtD{v}}{\NHokD{v}}\\
  &\lesssim \frac1k \sup_{v \in \HozDD} \frac{\NLtD{f}\NLtD{v}}{\NLtD{v}}\\
  &= \frac{\NLtD{f}}k.
\end{align*}
The factor $k$ appears in \cref{eq:bound4} since we use the weighted norm $\NHokD{\cdot}$ in the definition of $\NHokDs{\cdot},$ rather than the unweighted norm $\NHoD{\cdot}.$
\ere

\bth[Answer to \cref{it:nbpcq1}: $k$-independent weighted GMRES iterations]\label{cor:1}

\

\noindent Let $\kz \geq 0,$ $k \geq \kz,$ and assume that $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc}, $h$ and $p$ satisfy \cref{cond:2}, and $\Ast$ and $\nst$ are as in \cref{prob:vgen}. Then there exist constants $\Co$ and $\Ct$  independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso, p$, and $\kz$) such that if 
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:cond}
C_1 \,k \,\NLiDop{\Aso-\Ast} +C_2 \, k\, \NLiDRR{\nso-\nst}
\leq \frac{1}{2},
\eeq
then \emph{both} weighted GMRES working in $\|\cdot\|_{\Dmat_k}$ (and the associated inner product) applied to 
\beq\label{eq:pcsystem1}
(\Amat^{(1)})^{-1}\Amat^{(2)}\uvec = \fvec
\eeq
\emph{and} weighted GMRES working in $\|\cdot\|_{(\Dmat_k)^{-1}}$ (and the associated inner product) applied to 
\beq\label{eq:pcsystem2}
\Amat^{(2)}(\Amat^{(1)})^{-1}\vvec = \fvec
\eeq
 converge in a $k$-independent number of iterations.
 \enth

The constants $\Co$ and $\Ct$ are given explicitly  in \cref{eq:C1nbpc,eq:C2} below. The proof of \cref{cor:1} is on \cpageref{page:cor1proof} below.

\bth[Answer to \cref{it:nbpcq1}: $k$-independent (unweighted) GMRES iterations]\label{cor:1a}

\

\noindent Let $\kz \geq 0,$ $k \geq \kz,$ and assume that $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc}, $h$ and $p$ satisfy \cref{cond:2}, and $\Ast$ and $\nst$ are as in \cref{prob:vgen}. Let $C_1$ and $C_2$ be as in \cref{cor:1}, and let $s_{+}>0$ and $m_{\pm}>0$ be as in \cref{lem:normequiv} (note that all these constants are independent of $k$, $h$, and $p$). Then if 
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:conda}
 C_1 \,\left(\frac{s_+}{m_-}\right) \,\frac{1}{h} \,
\NLiDop{\Aso-\Ast} + C_2 \, \left(\frac{m_+}{m_-} \right)k \, \NLiDRR{\nso-\nst},
\leq \frac{1}{2}
\eeq
then standard GMRES (working in the Euclidean norm and inner product) applied to either of the equations \cref{eq:pcsystem1} or \cref{eq:pcsystem2}
%\beqs
%(\Amat^{(1)})^{-1}\Amat^{(2)}\uvec = \fvec\quad\text{ or } \quad\Amat^{(2)}(\Amat^{(1)})^{-1}\vvec = \fvec
%\eeqs
 converges in a $k$-independent number of iterations.
 \enth

 The proof of \cref{cor:1a} is on \cpageref{page:cor1aproof} below.

Three notes regarding \cref{cor:1,cor:1a}: (i) The $L^\infty$ norms of $\Ao-\At$ and $\no-\nt$ in \cref{cor:1,cor:1a} can be replaced by $L^p$ norms with $p < \infty$, at the price of making the conditions \cref{eq:cond,eq:conda} more restrictive; see \cref{sec:weaknorm} for more details. (ii) The $\NLiDop{\cdot}$ norm on matrix-valued functions appearing on the right-hand sides of \cref{eq:main1} and \cref{eq:main1a} is defined by \cref{eq:ineditsnorm1}.
(iii) The factor $1/2$ on the right-hand sides of \cref{eq:cond} and \cref{eq:conda} can be replaced by any number $<1$ and the result still holds, although the number of GMRES iterations may then be different---but is still independent of $k$.
%In \cref{sec:proofFEM}, the constant $C_2$ is expressed explicitly in terms of $C_1$.

\bre
When $h\sim  k^{-1}$, the bounds \cref{eq:cond,eq:conda} are identical in their $k$-dependence; however, when $h\ll k^{-1}$ (as one needs to take to overcome the pollution effect, as discussed in \cref{sec:helmfedisc}) the bound \cref{eq:conda} for standard GMRES is more pessimistic than the bound \cref{eq:cond} for weighted GMRES.
\ere

\bre[Link to the results of \cite{GaGrSp:15}]
A result analogous to the Euclidean-norm bounds in \cref{thm:1} was proved in \cite{GaGrSp:15} for the case that $\Aso= \Ast= I$, $\nst= 1$, and $\nso = 1 + i\eps/k^2$, with the `absorption parameter' or `shift' $\eps$ satisfying $0<\eps\lesssim k^2$. (Recall from \cref{rem:ggsqu} that the proof strategy used in this \lcnamecref{chap:nbpc} is based on the strategy in \cite{GaGrSp:15}.) The motivation for proving the results in \cite{GaGrSp:15} was that the so-called `shifted Laplacian preconditioning' of the Helmholtz equation is based on preconditioning (with these choices of parameters) $\Amatt$ with an approximation of $\Amato$. Similar to \cref{cor:1}, bounds on $\|\Imat -  (\Amato)^{-1}\Amatt \|_2$ and 
$\|\Imat - \Amatt  (\Amato)^{-1}\|_2$
 then give upper bounds on how large the `shift' $\eps$ can be for GMRES for $\AmatoI\Amatt$ to converge in a $k$-independent number of iterations in the case when the action of $(\Amato)^{-1}$ is computed exactly.

%\cite[Lemma 4.1]{GaGrSp:15}
The main differences between \cite{GaGrSp:15} and this work are that: (i)  \cite{GaGrSp:15} focused on the TEDP, not both the TEDP and the EDP,
(ii) \cite{GaGrSp:15} focused on the particular case that $\Dm$ is star-shaped with respect to a ball, finding a $k$- and $\eps$-explicit expression for $C^{(1)}_{\rm bound}$ in this case using Morawetz identities, whereas we assume the existence of $\Cboundo,$
(iii) \cite{GaGrSp:15} required a bound on 
$(\Amato)^{-1}\Mmat_{n}$, analogous to the bounds in \cref{lem:keylemma1} along with one on $(\Amato)^{-1}\Nmat$ (in the case that $T_R$ is approximated by $i k$), but \emph{not} on 
$(\Amato)^{-1}\Smat_{A}$, and (iv) \cite{GaGrSp:15} only proved bounds in the $\|\cdot\|_2$ norm.
%The result of \cref{thm:1}
\ere


\subsection{PDE analogues to \cref{cor:1,cor:1a} }
Numerical experiments in \cref{sec:num} below indicate that the condition \cref{eq:cond} is sharp, i.e., that the $k$ in \cref{eq:cond} cannot be replaced by $k^\alpha$ for $\alpha<1$. This indicated sharpness of \cref{eq:cond} is also supported by the PDE-result \cref{thm:2} below. Indeed, \cref{thm:2} % and \cref{lem:1} 
 shows that the condition
\beq\label{eq:sufficientlysmall}
k\,
\NLiDop{\Aso-\Ast} \quad\text{ and } \quad k\,\NLiDRR{\nso-\nst}
%\Big) 
\quad\text{ both sufficiently small}
\eeq
is not only an answer to \cref{it:nbpcq1} (about finite-element discretisations), but is also the natural answer to the analogue of \cref{it:nbpcq1} at the level of PDEs, namely 
\ben[label=Q2., ref=Q2]
\item\label[itemblank]{it:nbpcq2}
How small must $\NLiDop{\Aso - \Ast}$ and 
$\NLiDRR{\nso - \nst}$ be (in terms of their $k$-dependence) for the relative error in approximating 
%$u^{(1)}$ to be a good approximation to 
$u^{(2)}$ by $u^{(1)}$ to be bounded independently of $k$ for arbitrarily-large $k$? 
\een
\Cref{lem:sharp} shows that the condition ``$k\NLiDRR{\nso - \nst}$ sufficiently small" is the \emph{provably-sharp} answer to \cref{it:nbpcq2} when $\Aso= \Ast= I$.

%Before stating these PDE results, we define the weighted $H^1$ norm
%\beq\label{eq:1knorm}
%\N{v}^2_{\HokD} \de \N{\grad v}^2_{L^2(\D)} + k^2 \N{v}^2_{L^2(D)} \quad \tfor v \in H^1_{0,D}(D),
%\eeq
%where the space $H^1_{0,D}(D)$, defined by \cref{eq:spaceEDP} below, is the natural space containing the solution of the exterior Dirichlet problem. 
To state these PDE results, we use the notation for $a,b>0$ that $a\lesssim b$ when $a\leq C b$ for some $C>0$, independent of $k$, and $a\sim b$ if $a\lesssim b$ and $b\lesssim a$.


%The sharpness of \cref{eq:cond} and \cref{eq:main1} is also supported by the answer to the analogue of \cref{it:nbpcq1} at the level of PDEs. Indeed, the following \cref{thm:2} is the analogue of \cref{thm:1} 

\begin{theorem}[Answer to \cref{it:nbpcq2} (the PDE analogue of \cref{it:nbpcq1})]\label{thm:2}
%Given $f\in L^2(D)$ such that $\supp \, f \subset \BR$, 
Let $\kz > 0$ and $k \geq \kz.$ Let $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc} applied to \cref{prob:vedp} (so that the solution of \cref{prob:vedp} $\uso$ exists, is unique, and satisfies a $k$-independent a priori bound). Let $\Dm$, $\Ast$, and $\nst$ be such that $u^{(2)}$ exists
for any $f\in L^2(D)$ such that $\supp \, f \subset \BR$. 
Then, there exists $C_3>0$, independent of $k$ and given explicitly in terms of $\Dm$, $\Aso$, and $\nso$ in \cref{eq:C3} below, such that
\beq\label{eq:PDEbound}
\frac{\big\|u^{(1)}-u^{(2)}\big\|_{\HokD}
}{
\N{u^{(2)}}_{\HokD}
}\leq C_3 \,k\, \max\set{\NLiDop{\Aso-\Ast}\,,\, \NLiDRR{\nso-\nst}}
\eeq
for all $k\geq k_0$. 
\end{theorem}

The proof of \cref{thm:2} is on \cpageref{page:thm2proof} below.

\ble[Sharpness of the bound \cref{eq:PDEbound} when $\Aso = \Ast= I$]\label{lem:sharp}
There exist particular choices of  $f, \,\nso$, and $\nst$ (with $\nso\neq \nst$ both continuous) such that 
the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of \cref{prob:edp} with $\Aso = \Ast= I$ exist, are unique, and satisfy
\beq\label{eq:sharp1}
\frac{\N{u^{(1)}-u^{(2)}}_{\HokD}
}{
\N{u^{(2)}}_{\HokD}
}
\sim 
\frac{\N{u^{(1)}-u^{(2)}}_{L^2(D)}
}{
\N{u^{(2)}}_{L^2(D)}
}\sim k \NLiDRR{\nso-\nst}.
\eeq
%\noi (ii) There exist $f, \Aso, \Ast$, (with $\Aso\not\equiv \Ast$), such that 
%the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of the exterior Dirichlet problem with $\nso \equiv \nst\equiv 1$ exist, are unique, and satisfy
%%There exist $f\in L^2(D), \Aj \in C^{0,1}(D)$, $j=1,2$ (with $\Aso\not\equiv \Ast$), such that the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of the exterior Dirichlet problem with $\nso \equiv \nst\equiv 1$ satisfy
%\beq\label{eq:sharp2}
%\frac{\N{u^{(1)}-u^{(2)}}_{\HokD}
%}{
%\N{u^{(2)}}_{\HokD}
%}
%\sim 
%\frac{\N{u^{(1)}-u^{(2)}}_{L^2(D)}
%}{
%\N{u^{(2)}}_{L^2(D)}
%}\sim k \big\|\Aso-\Ast\big\|_{L^\infty(D)}.
%\eeq
\ele

The proof of \cref{lem:sharp} is on \cpageref{page:lemsharpproof} below.

\bre[Physical interpretation for $k$-dependence]\label{rem:physical1k}
It is perhaps unsurprising that the condition \cref{eq:sufficientlysmall} is a sufficient condition to answer both \cref{it:nbpcq1} and \cref{it:nbpcq2}. Recall that $1/k$ is proportional to the wavelength $2\pi/k$ of the wave $u$ (at least when $A=I$ and $n=1$). As the wavelength is the natural length scale associated with the wave $u$, one expects perturbations of magnitude up to $\cO(1/k)$ to be `unseen' by the PDE or numerical method. This is exactly what we see; perturbations of size (up to) $1/k$ give bounded relative difference (in \cref{it:nbpcq2}) and bounded GMRES iterations for the nearby-preconditioned linear system (in \cref{it:nbpcq1}). Also, on a PDE level, perturbations of order $1/k$ being `unseen' by the PDE can also be seen in bounds proved for $u$ where $n = \no + \eta,$ with $\no$ nontrapping and $\NLiDRR{\eta} \lesssim 1/k,$ see \cref{rem:kdep} above.
\ere


\section[Numerical experiments investigating sharpness]{Numerical experiments investigating the\newline sharpness of Theorems {\ref{cor:1}} and {\ref{cor:1a}}}\label{sec:num}

The numerical experiments in this section seek to verify \cref{cor:1a,cor:1} for \cref{prob:vtedp}, and investigate their sharpness. More specifically, the experiments seek to verify whether the condition \cref{eq:cond} is:
\ben
\item sufficient, and
\item necessary
  \een
  for \emph{standard} GMRES applied to \cref{eq:pcsystem1} to converge in a number of iterations that is independent of $k.$

Based on the PDE results \cref{thm:2,lem:sharp} above, we expect that the condition \cref{eq:sufficientlysmall} is a necessary and sufficient condition for standard GMRES applied to \cref{eq:pcsystem1} to converge in a $k$-independent number of iterations, even though we can only prove this is a sufficient condition for \emph{weighted} GMRES. We expect this because \cref{eq:sufficientlysmall} is a sufficient condition for \cref{it:nbpcq2}, the PDE analogue of \cref{it:nbpcq1}. Indeed, this is exactly the behaviour we observe in numerical experiments; we see that if \cref{eq:sufficientlysmall} holds, then standard GMRES applied to \cref{eq:pcsystem1} converges in a $k$-independent number of iterations, and moreover, \cref{eq:sufficientlysmall} may be sharp. We now describe our numerical experiments in more detail.

To verify this expected behaviour, we perform numerical experiments with the setup described in \cref{app:compsetup} with $\Aso = I$ and $\nso = 1$. We define $f$ and $\gI$ to correspond to a plane wave incident from the bottom left passing through a homogeneous medium given by coefficients $\Aso$ and $\nso$. We perform experiments for $A$ and $n$ separately, i.e., first we perform experiments with $\Ast=I$ and $\nst$ varying, and then we perform experiments with $\Ast$ varying and $\nst=1.$ When we vary $\Ast$ we measure $\Aso-\Ast$ in the $\NLiDRRdtd{\cdot}$ norm, as this norm is easier to control than the $\NLiDop{\cdot}$ norm. However, these two norms are equivalent on $\LiDRRdtd$ (see the comment above \cref{eq:ineditsnorm1}).

We define $\Ast$ and $\nst$ to be piecewise constant (matrix-valued and real-valued respectively) on a $10\times10$ square grid, with their values on each square chosen independently at random from a $\Unif\mleft(1-\alpha,1+\alpha\mright)$ distribution, with $\alpha \in (0,1)$ chosen as described below. For $\Ast,$ we impose the restriction that on each square $\Ast$ is positive-definite almost surely. We solve the linear systems \cref{eq:pcsystem1} for $k = 20,40,60,80,100$ using standard GMRES and record the number of GMRES iterations taken to achieve a (relative) tolerance of $10^{-5}$ (relative to $\Nt{\bvec}$).

We perform experiments taking $\alpha = 0.5 \times k^{-\beta}$ for $\beta \in 0,0.1,\ldots,0.9,1.$ We expect that when $\beta \neq 1$  the number of GMRES iterations required for convergence will increase as $k$ increases, whereas we expect that when $\beta = 1$ the number of GMRES iterations required for convergence will remain bounded as $k$ increases, even though this behaviour for $\beta=1$ has only been proved for $\NLiDop{\Aso-\Ast}$ for weighted GMRES (compare the restrictions on $\NLiDop{\Aso-\Ast}$ in \cref{cor:1,cor:1a}).


In \cref{fig:linfinityA0,fig:linfinityA1,fig:linfinityA2,fig:linfinityn0,fig:linfinityn1,fig:linfinityn2}, when $\beta \in \set{0,\ldots,0.3}$ (for $\NLiDRRdtd{\Aso-\Ast}$) and $\beta \in \set{0,\ldots,0.5}$ (for $\NLiDRR{\nso-\nst}$) we see growth in the maximum number of GMRES iterations needed (over all realisations) to achieve convergence, otherwise we see that the number of GMRES iterations is bounded as $k$ increases. This behaviour is better than expected; as the number of GMRES iterations is apparently bounded for a range of $\beta < 1.$ However, we note that this behaviour could be (i) because we are in a pre-asymptotic regime, and the number of GMRES iterations would grow if we increased $k$ further, or (ii) the particular structure of $\nst$ (being piecewise constant, with the pieces independently, randomly chosen) could result in some kind of `averaging' behaviour, meaning the preconditioner is better than would otherwise be expected. However, we do not investigate these issues further in this thesis.

    \begin{figure}
      \centering
\input{nbpc-linfinity-plot-A-0.pgf}
  \caption[Maximum GMRES iteration counts when $\NLiDRRdtd{\Aso-\Ast} = 0.5\times  k^{-\beta}$ for $\beta = 0,0.1,0.2,0.3.$]{Maximum GMRES iteration counts for solving systems with matrix $\AmatoI\Amatt$, where $\nso=\nst=1$ and $\NLiDRRdtd{\Aso-\Ast} = 0.5\times  k^{-\beta}$ for $\beta = 0,0.1,0.2,0.3.$}\label{fig:linfinityA0}
    \end{figure}
    
    \begin{figure}
      \centering
\input{nbpc-linfinity-plot-A-1.pgf}
   \caption[Maximum GMRES iteration counts when $\NLiDRRdtd{\Aso-\Ast} = 0.5\times  k^{-\beta}$ for $\beta = 0.4,0.5,0.6,0.7.$]{Maximum GMRES iteration counts for solving systems with matrix $\AmatoI\Amatt$, where $\nso=\nst=1$ and $\NLiDRRdtd{\Aso-\Ast} = 0.5\times  k^{-\beta}$ for $\beta = 0.4,0.5 ,0.6,0.7.$}\label{fig:linfinityA1}
\end{figure}

    \begin{figure}
      \centering
\input{nbpc-linfinity-plot-A-2.pgf}
    \caption[Maximum GMRES iteration counts when $\NLiDRRdtd{\Aso-\Ast} = 0.5\times  k^{-\beta}$ for $\beta = 0.8,0.9,1.$]{Maximum GMRES iteration counts for solving systems with matrix $\AmatoI\Amatt$, where $\nso=\nst=1$ and $\NLiDRRdtd{\Aso-\Ast} = 0.5\times  k^{-\beta}$ for $\beta = 0.8,0.9,1.$}\label{fig:linfinityA2}
\end{figure}

    \begin{figure}
      \centering
\input{nbpc-linfinity-plot-n-0.pgf}
  \caption[Maximum GMRES iteration counts when $\NLiDRR{\nso-\nst} = 0.5\times  k^{-\beta}$ for $\beta = 0,0.1,0.2,0.3.$]{Maximum GMRES iteration counts for solving systems with matrix $\AmatoI\Amatt$, where $\Aso=\Ast=1$ and $\NLiDRR{\nso-\nst} = 0.5\times  k^{-\beta}$ for $\beta = 0,0.1,0.2,0.3.$}\label{fig:linfinityn0}
    \end{figure}
    
    \begin{figure}
      \centering
\input{nbpc-linfinity-plot-n-1.pgf}
   \caption[Maximum GMRES iteration counts when $\NLiDRR{\nso-\nst} = 0.5\times  k^{-\beta}$ for $\beta = 0.4,0.5,0.6,0.7.$]{Maximum GMRES iteration counts for solving systems with matrix $\AmatoI\Amatt$, where $\Aso=\Ast=1$ and $\NLiDRR{\nso-\nst} = 0.5\times  k^{-\beta}$ for $\beta = 0.4,0.5,0.6,0.7.$}\label{fig:linfinityn1}
\end{figure}

    \begin{figure}
      \centering
\input{nbpc-linfinity-plot-n-2.pgf}
   \caption[Maximum GMRES iteration counts when $\NLiDRR{\nso-\nst} = 0.5\times  k^{-\beta}$ for $\beta = 0.8,0.9,1.$]{Maximum GMRES iteration counts for solving systems with matrix $\AmatoI\Amatt$, where $\Aso=\Ast=1$ and $\NLiDRR{\nso-\nst} = 0.5\times  k^{-\beta}$ for $\beta = 0.8,0.9,1.$}\label{fig:linfinityn2}
\end{figure}
  
%  \ednote{Euan---I had to redo these computations, as there was a bug in my code, so we now don't see \emph{really} rapid growth in the number of GMRES iterations as $k$ inreases. I think the previous code may have may the variations in $n$ too large, meaning we got really rapid growth.}


%Say that these are for the TEDP defined in \cref{def:TEDP}.

\section[Proofs of Theorems \MakeLowercase{\ref{cor:1}, \ref{cor:1a}, and \ref{thm:2}} and Lemma \MakeLowercase{\ref{lem:sharp}}]{Proofs of Theorems \ref{cor:1}, \ref{cor:1a}, and \ref{thm:2} and \newline Lemma \ref{lem:sharp}}\label{sec:3}

%then the weighted norm $\|\cdot\|_{\Dmat_k}$ is given by 
%\beq\label{eq:Dk3}
%\N{\vvec}_{\Dmat_k}^2\de   \N{v_h}^2_{\HokD}=\big( \Dmat_k \vvec,\vvec\big)_2,
%\eeq
%for
%$v_h =\sum_i v_i \phi_i$.



%For a proof of \cref{lem:normequiv}, see\footnote{In \cite[Chapter V, Lemma 2.5]{Br:07} the assumption is made that the meshes underlying $\Vhp$ are \emph{uniform}. However, the definition of uniformity in \cite[Chapter 2, Definition 5.1(4)]{Br:07} is the same as the more standard definition of quasi-uniformity in \cref{def:quasiuniform}.} \cite[Chapter V, Lemma 2.5]{Br:07}.

%% \bpf[Sketch proof of \cref{lem:normequiv}]\opntodo{can omit this if can find a good reference. One possibility . Need to check basis scaling business.}
%% The inequalities in \cref{eq:normequiv1} follow from writing $\|v_h\|_{\LtD}$ as a sum of integrals over elements of the mesh, and then mapping to the reference element \ednote{Euan to discuss with Ivan}.
%% %\beqs
%% %\N{v_h}^2_{L^2(\O
%% %\eeqs
%% Then, \cref{eq:normequiv2} follows from \cref{eq:normequiv1} and the inequalities
%% \beqs
%% \N{v_h}_{L^2(D)}\lesssim \N{\nabla v_h}_{L^2(D)}\lesssim \frac{1}{h} \N{v_h}_{L^2(D)},
%% \eeqs
%% the first of which follows from the Poincar\'e inequality, since $v_h \in \HozDD$
%% (see, e.g., \cite[Proposition 5.3.4]{BrSc:00}), the second of which follows from a standard inverse estimate (see, e.g., \cite[Theorem 4.5.11]{BrSc:00}).
%% \epf




\subsection{Proof of the main ingredient of the proofs of \cref{cor:1,cor:1a}}\label{sec:proofs}
As the first step towards proving \cref{cor:1,cor:1a}, we prove \cref{lem:normequiv}, concerning norm equivalences of finite-element functions.

\bpf[Proof of \cref{lem:normequiv}]
\label{page:normequivpf}
We first show \cref{eq:normequiv1} by direct computation, before concluding \cref{eq:normequiv2} from \cref{eq:normequiv1} and a standard inverse inequality. Throughout this proof, when we use $\sim$ the hidden constants are independent of $\tau \in \Th,$ the mesh size $h$, and $\vh \in \Vhp,$ but may depend on $p.$

For any $\vh \in \Vhp$ we have (letting $\bnj$ denote a node of $\Th$)
\begin{align}
  \NLtD{\vh}^2 &= \sum_{\tau \in \Th} \int_\tau \abs{\vh}^2\label{eq:twiddlenumber1}\\
  &\sim \sum_{\tau \in \Th} \abs{\tau} \sum_{\bnj \in \tau} \abs{\vh(\bnj)}^2 \label{eq:twiddlenumber2}\\
  &\sim h^d \sum_{\tau \in \Th} \sum_{\bnj \in \tau} \abs{\vh(\bnj)}^2, \text{ by quasi-uniformity,}\nonumber\\
  &\sim h^2 \Nt{\uvec},
\end{align}
i.e., \cref{eq:normequiv1}. The expression \cref{eq:twiddlenumber2} follows from \cref{eq:twiddlenumber1} because the terms defined on $\tau$ are equivalent norms on $\tau$ of functions in $\Vhp.$

To show \cref{eq:normequiv2} we recall the standard inverse inequality (see, e.g., \cite[Theorem 4.5.11 and Remark 4.5.20]{BrSc:08})
\beq\label{eq:twiddleinverse}
\NHoD{\vh} \lesssim h^{-1} \NLtD{\vh}.
\eeq
By combining \cref{eq:twiddleinverse} and the right-hand side of \cref{eq:normequiv1}, we obtain \cref{eq:normequiv2}.
\epf


The main part of the proofs of \cref{cor:1,cor:1a} is the following \lcnamecref{thm:1}.

\begin{theorem}[Main ingredient of the answer to \cref{it:nbpcq1}]\label{thm:1}
Let $\kz \geq 0,$ $k \geq \kz,$ and assume $\Dm$, $\Aso$, and $\nso$ satisfy \cref{cond:1nbpc}, and assume that $h$ and $p$ satisfy \cref{cond:2}. 
Let the $k$- and $h$-independent constants $\mpm$ and $\splus$ be given as in \cref{lem:normequiv}.
Then there exist $\Co, \Ct>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $\kz$) such that
\begin{align}\nonumber
&\max\Big\{
\NDmatk{\Imat - (\Amat^{(1)})^{-1}\Amat^{(2)}}, 
\N{\Imat -\Amat^{(2)} (\Amat^{(1)})^{-1}}_{(\Dmat_k)^{-1}}
\Big\}\\
&\hspace{3cm} 
\leq C_1 \,k \,
\NLiDop{\Aso-\Ast} + C_2 \, k \, \NLiDRR{\nso-\nst}
\label{eq:main1}
\end{align}
and 
\begin{align}\nonumber
&\max\Big\{
\N{\Imat - (\Amat^{(1)})^{-1}\Amat^{(2)}}_2, 
\N{\Imat -\Amat^{(2)} (\Amat^{(1)})^{-1}}_2
\Big\}\\
&\hspace{0cm} 
\leq C_1 \,\left(\frac{s_+}{m_-}\right) \,\frac{1}{h} \,
\NLiDop{\Aso-\Ast} + C_2 \, \left(\frac{m_+}{m_-} \right)k \, \NLiDRR{\nso-\nst}.
\label{eq:main1a}
\end{align}
\end{theorem}

The proof of \cref{thm:1} is given after the following two lemmas, that are the heart of the proof of \cref{thm:1}.

\ble[Bounds on $(\Amato)^{-1} \Mmat_{n}$]\label{lem:keylemma1}
Assume that \cref{cond:1nbpc} holds, and assume that part (i) of \cref{cond:2} holds. Then, for $n\in \LiDRR$,
\beq\label{eq:keybound1}
\max\Big\{\big\| (\Amato)^{-1} \Mmat_{n} \big\|_{\Dmat_k}, \,\,
\big\|  \Mmat_{n}(\Amato)^{-1} \big\|_{(\Dmat_k)^{-1}}
\Big\}\leq 
C_2
%\frac{m_+}{m_-} \left[ C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}\right] 
\frac{\NLiDRR{n}}{k}
\eeq
and 
\beq\label{eq:keybound1a}
\max\Big\{\big\| (\Amato)^{-1} \Mmat_{n} \big\|_2, \,\,
\big\|  \Mmat_{n}(\Amato)^{-1} \big\|_2 
\Big\}\leq 
C_2 
%\frac{m_+}{m_-} \left[ C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}\right] 
\left(\frac{m_+}{m_-}\right) \frac{\NLiDRR{n}}{k},
\eeq
where
\beq\label{eq:C2}
C_2\de%\frac{m_+}{m_-} 
%\left[ 
C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}.%\right].
\eeq
\ele

The proof of \cref{lem:keylemma1} is on \cpageref{page:lemkeylemma1proof} below.

\ble[Bounds on $(\Amato)^{-1} \Smat_A$]\label{lem:keylemma2}
Assume that \cref{cond:1nbpc} holds, and assume that part (ii) of \cref{cond:2} holds. Then, for $A\in \LiDRRdtd$,
\beq\label{eq:keybound2}
\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_{(\Dmat_k)^{-1}}, \,\,
\big\| \Smat_A (\Amato)^{-1} \big\|_{\Dmat_k}\Big\} \leq C_1\, k\NLiDop{A}
\eeq
and
\beq\label{eq:keybound2a}
\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_2, \,\,
\big\| \Smat_A (\Amato)^{-1} \big\|_2\Big\} \leq C_1\,\left(\frac{s_+}{m_-}\right) \frac{1}{h}\NLiDop{A},
\eeq
%\begin{align}\nonumber
%&\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_2, \,\,
%\big\| \Smat_A (\Amato)^{-1} \big\|_2\Big\}\nonumber \\
%&\hspace{2cm}
% \leq \frac{s_+}{s_-} \left[ C_{\rm FEM2}^{(1)} + 
% \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right]k\N{A}_{L^\infty(D)}\label{eq:keybound2}
%% + C_{\rm bound}^{(1)}\right) \frac{\N{n}_{L^\infty(D)}}{k}.
%\end{align}
where
\beq\label{eq:C1nbpc}
C_1\de%\frac{s_+}{s_-} 
\left[ C_{\rm FEM2}^{(1)} + 
 \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right].
\eeq
\ele

The proof of \cref{lem:keylemma2} is on \cpageref{page:lemkeylemma2proof} below.

\bpf[Proof of \cref{thm:1} using \cref{lem:keylemma1,lem:keylemma2}]
Using the definition of the matrices $\Amatj, \SmatA$, and $\Mmatn$ in \cref{eq:matrixAjdef} and \cref{eq:matrixSjdef}, we have
\begin{align}\nonumber
\Imat - (\Amato)^{-1}\Amatt = (\Amato)^{-1}\big(\Amato-\Amatt\big) &=  (\Amato)^{-1}\left( \Smat_{A^{(1)}} - \Smat_{A^{(2)}} - k^2 \big(\Mmat_{n^{(1)}}-\Mmat_{n^{(2)}}\big)\right)\\
&= (\Amato)^{-1}\left( \Smat_{A^{(1)}-A^{(2)}} - k^2 \Mmat_{n^{(1)}-n^{(2)}}\right),\label{eq:idea1}
\end{align}
and similarly 
\beq\label{eq:idea2}
\Imat -\Amatt  (\Amato)^{-1}= \left( \Smat_{A^{(1)}-A^{(2)}} - k^2 \Mmat_{n^{(1)}-n^{(2)}}\right)(\Amato)^{-1}.
\eeq
The bounds in  \cref{eq:main1} on $\NDk{\Imat - (\Amato)^{-1}\Amatt}$ and  $\NDkI{\Imat - \Amatt(\Amato)^{-1}}$ then follow from using the bounds \cref{eq:keybound1,eq:keybound2} in \cref{eq:idea1,eq:idea2}. The bounds in \Cref{eq:main1a} on $\Nt{\Imat - (\Amato)^{-1}\Amatt}$ and  $\Nt{\Imat - \Amatt(\Amato)^{-1}}$ follow completely analagously, except we use the bounds \cref{eq:keybound1a,eq:keybound2a} instead of the bounds \cref{eq:keybound1,eq:keybound2}.
%
%, and $C_1$, $C_2$ in \cref{eq:main1} are given explicitly by
%\beq\label{eq:C1nbpc}
%C_1\de%\frac{s_+}{s_-} 
%\left[ C_{\rm FEM2}^{(1)} + 
% \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right] \,\,\tand\,\,
%\quad C_2\de  %+ \frac{m_+}{m_-} 
% \left[ C_{\rm FEM1}^{(1)} + C_{\rm bound}^{(1)}\right].
%\eeq
\epf

\subsubsection{Proofs of \cref{lem:keylemma1,lem:keylemma2}}

The proofs of \cref{lem:keylemma1,lem:keylemma2} require the concept of the \emph{adjoint} sesquilinear form to $\aG(\cdot,\cdot)$.

\begin{definition}[The adjoint sesquilinear form $\aGs(\cdot,\cdot)$]\label{def:adjoint}
Let $D$, $A$, and $n$ be as in \cref{prob:vgen}. The adjoint sesquilinear form, $\aGs(\cdot,\cdot)$, to $\aG(\cdot,\cdot)$ defined in \cref{eq:agen} is given by
\beq\label{eq:EDPadjoint}
\aGs(w,v) \de \int_{D} 
\Big((A \grad w)\cdot\grad \vb
 - k^2 n w\vb\Big) -  \DPGI{\trI w}{\T(\trI v)}.
\eeq
\end{definition}

\noi It is then straightforward to check that
\beq\label{eq:A*}
\Amat^\dagger \de \Smat_A -k^2 \Mmat_n - \Nmat^\dagger
\eeq
(where $^\dagger$ denotes conjugate transpose) is the Galerkin matrix for the sesquilinear form $\aGs(\cdot,\cdot)$; i.e.~$(\Amat^\dagger)_{ij} = \aGs(\phi_j, \phi_i)$.

The following \lcnamecref{lem:adjoint} shows that if $w$ solves an adjoint Helmholtz problem, then $\wbar$ solves a (standard) Helmholtz problem with a related right-hand side.

\ble[Link between variational problems involving $\aG(\cdot,\cdot)$ and $\aGs(\cdot,\cdot)$]\label{lem:adjoint}

\

\noindent If the source term $\LG$ is as in \cref{prob:vgen}, $w$ is the solution to \cref{prob:vgen}, the boundary operator $\T$ satisfies
\beq\label{eq:DPconj}
\DPGI{\T \psi}{\phibar} = \DPGI{\T \phi}{\psibar} \tfa \phi,\psi \in \HhGI,
\eeq
and
\beq\label{eq:adjoint1}
\aGs(w,v)= \LG(v) \quad\tfa v\in \HozDD,
\eeq
then $\overline{w}$ satisfies
\beq\label{eq:adjoint2}
\aG(\overline{w},v)= \overline{\LG(\overline{v})} \quad\tfa v\in \HozDD.
\eeq
\ele

\bpf[Proof of \cref{lem:adjoint}]
From \cref{eq:adjoint1} we have that 
\beqs
\overline{\aGs(w,\overline{v})}= \overline{\LG(\overline{v})} \quad\tfa v\in \HozDD.
\eeqs
Using the definition of $\aGs(\cdot,\cdot)$ and the property \cref{eq:DPconj} in the left-hand side of this last equation, we find \cref{eq:adjoint2}.
\epf

\bco[\Cref{eq:adjoint2} holds for \cref{prob:vedp,prob:vtedp}]\label{cor:adjoint}
If \cref{prob:vgen} is chosen to represent either \cref{prob:vedp} or \cref{prob:vtedp}, then \cref{eq:adjoint2} holds.
\eco

\bpf[Proof of \cref{cor:adjoint}]
The only thing we need to check is that \cref{eq:DPconj} holds for both \cref{prob:vedp,prob:vtedp}. For \cref{prob:vtedp}, when $\T = ik$, the proof is straightforward. For \cref{prob:vedp} when $\T = \DtN$ we need the following property of the DtN map $\DtN$:
\beq\label{eq:DtN}
\DPGR{\DtN\psi}{\phibar} = \DPGR{\DtN \phi}{\psibar} \quad\tfa \phi,\psi \in H^{1/2}(\GR).
\eeq
This property follows from the fact that, if $\uo$ and $\ut$ are solutions of the homogeneous Helmholtz equation $\Delta u +k^2 u=0$ in $\RRd\setminus \overline{\BR}$, both satisfying the Sommerfeld radiation condition \cref{eq:src}, then
\beqs
\int_{\GR} (\gamma \uo)\, \dn \utb = \int_{\GR} (\gamma \ut)\,\dn \uob;
\eeqs
which follows from Green's theorem and, e.g., \cite[Lemma 4.10]{Sp:15}.
\epf

We can now prove \cref{lem:keylemma1,lem:keylemma2}.

\bpf[Proof of \cref{lem:keylemma1}]
\label{page:lemkeylemma1proof}
We first concentrate on proving \cref{eq:keybound1}.
Given $\fvec \in \CC^N$ and $n\in \LiDRR$, we create a variational problem whose Galerkin discretisation leads to the equation $\Amato \tbu = \Mmat_n\,\fvec$.
Indeed, let $\widetilde{f} \de \sum_j f_j \phi_j\in \HozDD$. Define $\widetilde{u}$ to be the solution of the variational problem 
\beq\label{eq:411}
a^{(1)}(\widetilde{u},v)= \IPLtD{n\widetilde{f}}{v} \quad\text{ for all } v\in \HozDD,
\eeq
and let $\tu_h$ be the solution of the finite-element approximation of \cref{eq:411}, i.e.,
\beq\label{eq:41}
a^{(1)}(\tu_h,v_h)= \IPLtD{n\widetilde{f}}{v_h} \quad\text{ for all } v_h\in \Vhp,
\eeq
and let $\tbu$ be the vector of nodal values of $\tu_h$. The definition of $\widetilde{f}$ then implies that \cref{eq:41} is equivalent to the linear system $\Amato \tbu = \Mmat_{n}\,\fvec$, and so to obtain a bound on $\|(\Amato)^{-1}\Mmat_n\|_{\Dmat_k}$ we need to bound $\|\tbu\|_{\Dmat_k}$ in terms of $\|\fvec\|_{\Dmat_k}$. (Recall $\fvec \in \CCN$ was arbitrary.) Because of the definition 
of $\|\cdot\|_{\Dmat_k}$ in \cref{eq:Dk}, this bound is equivalent to bounding $\|\tu_h\|_{\HokD}$ in terms of $\|\widetilde{f}\|_{\HokD}$.

%First observe that the bound \cref{eq:bound3} from Part (i) of \cref{cond:2} holds for the solution of the variational problem
%\beqs%\label{eq:411}
%a^{(1)}(u,v)= (n\phi_j,v)_{L^2(\Omega)} \quad\text{ for all } v\in H^1(\Omega),
%\eeqs
%and hence, by linearity, it also holds for the solution $\widetilde{u}$ of the variational problem \cref{eq:411}.

Using %the bounds in \cref{eq:normequiv1}, 
the triangle inequality and the bounds \cref{eq:bound1} and \cref{eq:bound3} from \cref{cond:2,cond:1nbpc} respectively, we find
%Note that the hypotheses imply that the bound on the solution operator 
%\cref{eq:bound_unif} holds (by \cref{cor:uniform}), and also that if $h k\sqrt{|k^2-\eps|} \leq C_1$ then quasi-optimality \cref{eq:qoeps_lemma} holds (by \cref{lem:qo}).
%Starting with \cref{eq:equiv} we then have 
\begin{align}
%m_- h^{d/2}k \N{\tbu}_2 \leq k\N{\tu_h}_{\LtD}\leq  
\N{\tu_h}_{\HokD} \leq
\N{\tu-\tu_h}_{\HokD} + \N{\tu}_{\HokD} \label{eq:mainevent1}
& \leq C^{(1)}_{\rm FEM1}\NLtD{n\ftilde} + C^{(1)}_{\rm bound}\NLtD{n\ftilde} \\ 
& \leq \mleft(C^{(1)}_{\rm FEM1} + C^{(1)}_{\rm bound}\mright)\NLiDRR{n}\NLtD{\ftilde} \label{eq:mainevent1a} \\
& \leq\big(C^{(1)}_{\rm FEM1}+  C^{(1)}_{\rm bound}\big)\NLiDRR{n}\frac{\big\|\widetilde{f}\big\|_{\HokD}}{k};\nonumber
%& \leq\big(C^{(1)}_{\rm FEM1}+  C^{(1)}_{\rm bound}\big)\N{n}_{L^\infty(D)} m_+ h^{d/2} \N{\fvec}_2,
\end{align}
the bound on $\|(\Amato)^{-1}\Mmat_n\|_{\Dmat_k}$ in \cref{eq:keybound1} then follows from the definition of $\|\cdot\|_{\Dmat_k}$ in \cref{eq:Dk} and the definition of $C_2$ \cref{eq:C2}.

To prove the bound on $\|\Mmat_n(\Amato)^{-1}\|_{(\Dmat_k)^{-1}}$ in \cref{eq:keybound1}, first observe that the definitions of $\|\cdot\|_{\Dmat_k}$ and $\|\cdot\|_{(\Dmat_k)^{-1}}$ in \cref{eq:Dk} imply that, for any matrix $\Cmat \in \CCNtN$ and for any $\vvec\in \CC^N$,
\beq\label{eq:A380-0}
\frac{
\big\|\matrixC \vvec \big\|_{(\Dmat_k)^{-1}}
}{
\big\|\vvec\|_{(\Dmat_k)^{-1}}
} = 
\frac{
\big\|\matrixC^\dagger \wvec \big\|_{\Dmat_k}
}{
\big\|\wvec\|_{\Dmat_k}
}
\eeq
where $\wvec \de (\Dmat_k)^{1/2}\vvec$, and where $\matrixC^\dagger$ is the conjugate transpose of $\matrixC$ (i.e.~the adjoint with respect to $(\cdot,\cdot)_2$).
Therefore, since $\Mmat_n$ is a real, symmetric matrix,
\beqs
\frac{
\big\|\Mmat_n (\Amato)^{-1}\vvec\big\|_{(\Dmat_k)^{-1}}
}{
\N{\vvec}_{(\Dmat_k)^{-1}}
}
=
\frac{\NDk{\mleft(\AmatoI\Mmatn\mright)^\dagger \wvec}}{\NDk{\wvec}}
= 
\frac{
\big\|((\Amato)^\dagger)^{-1}\Mmat_n\wvec\big\|_{\Dmat_k}
}{
\N{\wvec}_{\Dmat_k}
},
 \eeqs
 so that 
\beq\label{eq:A380} 
 \big\|\Mmat_n (\Amato)^{-1}\big\|_{(\Dmat_k)^{-1}}=\big\|((\Amato)^\dagger)^{-1}\Mmat_n\big\|_{\Dmat_k}.
 \eeq 
Recall from the text below \cref{eq:A*} that $(\Amato)^\dagger$ is the Galerkin matrix corresponding to the variational problem \cref{eq:adjoint1} -- the adjoint problem. \cref{lem:adjoint} implies that if the EDP %with coefficients $A^{(1)}$ and $n^{(1)}$ 
satisfies \cref{cond:1nbpc,cond:2}, then so does the adjoint problem. Therefore, the argument above leading to the bound on $\|(\Amato)^{-1}\Mmat_n\|_{\Dmat_k}$ under \cref{cond:1nbpc} and Part (i) of \cref{cond:2} proves the same bound on $\|((\Amato)^\dagger)^{-1}\Mmat_n\|_{\Dmat_k}$, and then, using \cref{eq:A380}, also on $\big\|\Mmat_n(\Amato)^{-1}\big\|_{(\Dmat_k)^{-1}}$.

To prove the bound on  $\|(\Amato)^{-1}\Mmat_n\|_{2}$ in \cref{eq:keybound1a}, we use the bounds 
\beqs
m_- h^{d/2} k \N{\tbu}_2 \leq k \N{\widetilde{u}_h}_{\LtD} \leq \N{\widetilde{u}_h}_{\HokD}
\,\tand\,
\big\|\widetilde{f}\big\|_{\LtD} \leq m_+ h^{d/2}\N{\fvec}_2,
\eeqs
on either side of the inequality \cref{eq:mainevent1}, with these bounds coming from \cref{eq:normequiv1}. The proof of the bound on 
$\|\Mmat_n((\Amato)^\dagger)^{-1}\|_{2}$ in \cref{eq:keybound1a} follows in a similar way to above, using the fact that 
$\|\Mmat_n (\Amato)^{-1}\|_2=\|((\Amato)^\dagger)^{-1}\Mmat_n\|_2$ (compare to \cref{eq:A380}).
%, namely the variational problem \cref{eq:EDPvar} with the operator $T_R$ in $a^{(1)}(\cdot,\cdot)$ replaced by $\overline{T_R}$ (corresponding to the $-\ri k$ in the radiation condition \cref{eq:src} being changed to $+i k$).
%
%Now, if $u$ is the solution of the adjoint problem with data $\LE(v)$, then $\overline{u}$ is the solution of the original problem with data $\overline{\LE(\overline{v})}$; 
%
%in particular if $\LE(v)$ is as in \cref{eq:EDPa}, then the $L^2$ data of the adjoint problem is just $\overline{f}$. Therefore, if the EDP satisfies \cref{cond:1nbpc,cond:2}, then so does its adjoint, and
% the bound in \cref{eq:keybound1} on $\|(\Amato)^{-1}\Mmat_n\|_{2}$ also holds for $\|((\Amato)^\dagger)^{-1}\Mmat_n\|_{2}$.
\epf

The proof of \cref{lem:keylemma2} uses the following \lcnamecref{lem:H1}, which one can prove using the G\aa rding inequality \cref{eq:gardingbrief}; see \cite[Lemma 5.1]{GrPeSp:19}.

\ble[Bound for data in $\HozDDs$]\label{lem:H1}
%With the sesquilinear form $a(\cdot,\cdot)$ defined by \cref{eq:EDPa} with $A=\Aso$ and $n=\nso$, 
Given $\LGtilde\in \HozDDs$, let $\widetilde{u}$ be the solution of the variational problem
\beqs
\text{ find } \,\,\widetilde{u} \in H^1_{0,D}(D) \,\,\tst \,\,
a^{(1)}(\widetilde{u},v)=\LGtilde(v) \,\, \tfa v\in H^1_{0,D}(D).
\eeqs
If \cref{cond:1nbpc} holds, then $\widetilde{u}$ exists, is unique, and satisfies the bound
\beq\label{eq:bound2}
\N{\widetilde{u}}_{\HokD} \leq \frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomax  k\right) \big\|\LGtilde\big\|_{(\HokD)^*}
\eeq
for all $k\geq k_0$.
\ele
%Observe that, similar to \cref{rem:yesitis}, \cref{eq:bound2} is a $k$-independent bound, due to the norm $\NHokDs{\LE}$ on the right-hand side.


\bpf[Proof of \cref{lem:keylemma2}]
\label{page:lemkeylemma2proof}
In a similar way to the proof of \cref{lem:keylemma1}, given $\fvec \in \CC^N$ and $A\in \LiDRRdtd$, let $\widetilde{f} \de \sum_j f_j \phi_j$ and observe that $\widetilde{f} \in \HozDD$. Define $\widetilde{u}$ to be the solution of the variational problem 
\beq\label{eq:411a}
a^{(1)}(\widetilde{u},v)= \LGtilde(v) \quad\text{ for all } v\in \HozDD,
\quad\text{ where } \quad
 \LGtilde(v) \de \IPLtD{(A\nabla\widetilde{f}}{\nabla v}.
\eeq
Observe that the definition of the norms $\|\cdot\|_{(\HokD)^*}$ and $\|\cdot\|_{\HokD}$ \cref{eq:weightednorm} and the Cauchy-Schwarz inequality imply that
\begin{align}
\big\| \LGtilde\big\|_{(\HokD)^*}&\leq \big\|A\nabla \widetilde{f}\big\|_{\LtD}\nonumber\\
&\leq \NLiDop{A} \big\|\nabla \widetilde{f}\big\|_{\LtD}\label{eq:Fbounda}\\
&\leq \NLiDop{A} \big\| \widetilde{f}\big\|_{\HokD}.\label{eq:Fbound}
\end{align}
Let $\tu_h$ be the solution of the finite element approximation of \cref{eq:411a}, i.e.,
\beq\label{eq:41a}
a^{(1)}(\tu_h,v_h)= \LGtilde(v_h) \quad\text{ for all } v_h\in \Vhp,
\eeq
and let $\tbu$ be the vector of nodal values of $\tu_h$. The definition of $\widetilde{f}$ then implies that \cref{eq:41a} is equivalent to $\Amato \tbu = \Smat_A\,\fvec$. 

Similar to the proof of \cref{lem:keylemma1},
using the triangle inequality, the bound \cref{eq:bound4} from \cref{cond:2}, the bound \cref{eq:bound2} from \cref{lem:H1}, the bound \cref{eq:Fbound}, and the definition of $C_1$ \cref{eq:C1nbpc},
we find
%Note that the hypotheses imply that the bound on the solution operator 
%\cref{eq:bound_unif} holds (by \cref{cor:uniform}), and also that if $h k\sqrt{|k^2-\eps|} \leq C_1$ then quasi-optimality \cref{eq:qoeps_lemma} holds (by \cref{lem:qo}).
%Starting with \cref{eq:equiv} we then have 
\begin{align}\nonumber 
%s_- h^{(d-2)/2} \N{\tbu}_2 &\leq \N{\nabla \tu_h}_{\LtD}\leq  
\N{\tu_h}_{\HokD} &\leq
\N{\tu-\tu_h}_{\HokD} + \N{\tu}_{\HokD},\nonumber \\ \nonumber
& \leq \left[ C^{(1)}_{\rm FEM2} k + 
\frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomax k  \right) 
\right]\big\|\LGtilde\big\|_{(\HokD)^*},\\
&\leq C_1 \, k\, 
%\left[C^{(1)}_{\rm FEM2} k + \frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomaxk  \right) \right]
\NLiDop{A} \big\|\nabla\widetilde{f}\big\|_{\LtD},\label{eq:mainevent2}\\
&\leq C_1 \, k\, 
%\left[C^{(1)}_{\rm FEM2} k + \frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomaxk  \right) \right]
\NLiDop{A} \big\|\widetilde{f}\big\|_{\HokD},\nonumber
%&\leq \left[ C^{(1)}_{\rm FEM2} k + 
%\frac{1}{\min\{\Asomin,\nsomin\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomaxk  \right) 
%\right]\big\|A\big\|_{L^\infty(D)}s_+ h^{(d-2)/2} \N{\fvec}_2,
\end{align}
and the bound on $\|(\Amato)^{-1}\Smat_A\|_{\Dmat_k}$ in \cref{eq:keybound2} follows.

The bound on $\|\Smat_A(\Amato)^{-1}\|_{(\Dmat_k)^{-1}}$ follows in a similar way to how we obtained the 
bound on  $\|\Mmat_n(\Amato)^{-1}\|_{(\Dmat_k)^{-1}}$ from the bound on $\|(\Amato)^{-1}\Mmat_n\|_{\Dmat_k}$ in Part (i). Indeed, 
\cref{eq:A380-0} and the fact that $\Smat_A$ is a real, symmetric matrix imply that 
\beq\label{eq:A380-2} 
 \big\|\Smat_A (\Amato)^{-1}\big\|_{(\Dmat_k)^{-1}}=\big\|\big((\Amato)^\dagger\big)^{-1}\Smat_A\big\|_{\Dmat_k}
 \eeq 
%since 
%\beqs
%\big\|\Smat_A(\Amato)^{-1}\big\|_{2}=\big\|(\Smat_A(\Amato)^{-1})^\dagger\big\|_{2}=\big\|((\Amato)^\dagger)^{-1}\Smat_A\big\|_{2},
%\eeqs
(c.f. \cref{eq:A380}),
and then the arguments in the proof of part (i) imply that 
the bound in \cref{eq:keybound2} on $\|(\Amato)^{-1}\Smat_A\|_{\Dmat_k}$ also holds for $\|((\Amato)^\dagger)^{-1}\Smat_A\|_{\Dmat_k}$.

To prove the bound on  $\|(\Amato)^{-1}\Smat_A\|_{2}$ in \cref{eq:keybound2a}, we use the bounds 
\beqs
m_- h^{d/2} k \N{\tbu}_2 \leq k \N{\widetilde{u}_h}_{\LtD} \leq \N{\widetilde{u}_h}_{\HokD}
\,\tand\,
\big\|\nabla \widetilde{f}\big\|_{\LtD} \leq s_+ h^{d/2-1}\N{\fvec}_2,
\eeqs
on either side of the inequality \cref{eq:mainevent2}, with these bounds coming from \cref{eq:normequiv1} and \cref{eq:normequiv2} respectively. The proof of the bound on 
$\|\Smat_A((\Amato)^\dagger)^{-1}\|_{2}$ in \cref{eq:keybound2a} follows in a similar way to above, using \cref{eq:Fbound}.
\epf


%\bre[Analogue of \cref{thm:1} in a weighted norm]\label{rem:weight1}
%The PDE analysis of the Helmholtz equation naturally takes place in the weighted $H^1$ norm $\|\cdot\|_{\HokD}$ defined by \cref{eq:1knorm}. The discrete analogue of this norm is the norm $\|\cdot\|_{\Dmat_k}$ defined by 
%\beq\label{eq:Dk}
%\N{\vvec}_{\Dmat_k}^2\de \big( (\Smat_I + k^2 \Mmat_1)\vvec,\vvec\big)_2 = \N{v_h}^2_{\HokD}
%\eeq
%for
%$v_h =\sum_i v_i \phi_i$. 
%This norm is used, e.g., in recent results about the convergence of domain-decomposition methods %in this norm are proved 
%for the Helmholtz equation \cite{GrSpVa:17}, \cite{GrSpZo:18}, and for the time-harmonic Maxwell equations \cite{BoDoGrSpTo:19}. 
%
%Inspecting the proof of \cref{lem:keylemma}, we see that the bounds \cref{eq:keybound1} and \cref{eq:keybound2} hold with the $\|\cdot\|_2$ norm replaced by the $\|\cdot\|_{\Dmat_k}$ norm and without the terms involving $m_\pm$ and $s_\pm$ on the right-hand side. \cref{thm:1} 
%%(and also \cref{cor:1}) 
%therefore also holds with the $\|\cdot\|_2$ norm replaced by the $\|\cdot\|_{\Dmat_k}$ norm and the constant $C_1$ modified appropriately.
%\ere

\subsection{Proofs of the finite-element results \cref{cor:1,cor:1a}}\label{sec:mainproofs}

We first recall properties of (weighted) GMRES that we will use to prove \cref{cor:1,cor:1a}.

Let 
\beq\label{eq:fov}
W_\Dmat(\matrixC)\de \Big\{ (\matrixC \xvec, \xvec)_{\Dmat} : \xvec \in \CCN, \|\xvec\|_\Dmat=1\Big\};
\eeq
$W_\Dmat(\matrixC)$ is called the \emph{numerical range} or \emph{field of values} of $\matrixC$ (in the $(\cdot,\cdot)_\Dmat$ inner product).

%Recall the so-called ``Elman estimate" for GMRES

\begin{theorem}[Elman estimate for weighted GMRES]\label{thm:GMRES1_intro} 
Let $\matrixC$ be a matrix with $\zerovec\notin W_\Dmat(\matrixC)$. Let $\beta\in[0,\pi/2)$ be defined such that
\beq\label{eq:cosbeta}
\cos \beta \de \frac{\mathrm{dist}\big(\zerovec, W_\Dmat(\matrixC)\big)}{\N{\matrixC}_{\Dmat}}.
\eeq
If the matrix equation $\matrixC \xvec = \by$ is solved using weighted GMRES then, 
for $m\in \mathbb{N}$, the GMRES residual $\rvecm$ %\de \matrixC \xvec_m - \by$ 
satisfies
\beq\label{eq:Elman}
\frac{\N{\rvecm}_{\Dmat}}{\N{\rvecz}_{\Dmat}} \leq \sin^m \beta. %, \quad \text{ where}\quad 
\eeq
\end{theorem}
The bound \cref{eq:Elman} with $\Dmat=\Imat$ was first proved in \cite[Theorem 6.3]{El:82} (see also \cite[Theorem 3.3]{EiElSc:83}) and was written in the above form in \cite[Equation 1.2]{BeGoTy:06}. The bound \cref{eq:Elman} (for arbitrary Hermitian positive-definite $\Dmat$) was stated implicitly (without proof) in \cite[p. 247]{CaWi:92} and proved in \cite[Theorem 5.1]{GrSpVa:17}. % (see also \cite[Remark 5.2]{GrSpVa:17}). 



\cref{thm:GMRES1_intro} has the following \lcnamecref{cor:GMRES_intro}, and the proofs of \cref{cor:1,cor:1a} follow from combining this with \cref{thm:1}.

\begin{corollary}
\label{cor:GMRES_intro} 
If $\|\Imat - \matrixC \|_\Dmat \leq \alpha < 1$, then, with $\beta$ defined as in \cref{eq:cosbeta},
\beqs
\cos \beta \geq \frac{1-\alpha}{1+\alpha}\eeqs
and
\beq\label{eq:gmressin}
\sin \beta \leq \frac{2 \sqrt{\alpha}}{(1+\alpha)^2}.
\eeq
\end{corollary}

\bpf[Proof of \cref{cor:1}]
\label{page:cor1proof}
This follows from \cref{thm:1} by applying \cref{cor:GMRES_intro} first with $\matrixC= (\Amato)^{-1} \Amatt$, $\Dmat=\Dmat_k$, and $\alpha=1/2$, and then with $\matrixC= \Amatt(\Amato)^{-1} $, $\Dmat=(\Dmat_k)^{-1}$, and $\alpha=1/2$.
\epf

\

\bpf[Proof of \cref{cor:1a}]
\label{page:cor1aproof}
This follows from \cref{thm:1} by applying \cref{cor:GMRES_intro} first with $\matrixC= (\Amato)^{-1} \Amatt$, $\Dmat=\Imat$, and $\alpha=1/2$, and then with $\matrixC= \Amatt(\Amato)^{-1} $, $\Dmat=\Imat$, and $\alpha=1/2$.
\epf


\bre[The improvement of the Elman estimate \cref{eq:Elman} in \cite{BeGoTy:06}]
A stronger result than \cref{eq:Elman} is given for standard (unweighted) GMRES in \cite[Theorem 2.1]{BeGoTy:06}, and then converted to a result about weighted GMRES in \cite[Theorem 5.3]{BoDoGrSpTo:19}; indeed, the convergence factor $\sin \beta$ is replaced by a function of $\beta$ strictly less than $\sin\beta$ for $\beta\in (0,\pi/2)$. Using this stronger result, however, does not improve the $k$-dependence of \cref{cor:1}.
\ere


%\section{Proof of }\label{sec:proofPDE}

\subsection{Proofs of the PDE results \cref{thm:2,lem:sharp}}\label{sec:pdeproofs}

\bpf[Proof of \cref{thm:2}]
\label{page:thm2proof}
%We first prove the upper bound \cref{eq:PDEbound}.
Because we assumed \cref{cond:1nbpc} holds for the EDP (\cref{prob:vedp}), $u^{(1)}$ and $u^{(2)}$ exist, are unique, satisfy $a^{(1)}(u^{(1)}, v) = \LG(v)$  and $a^{(2)}(u^{(2)}, v) = \LG(v)$ for all $v \in \HozDD$, respectively, where $\LG$ is given by \cref{eq:Ledp}. Subtracting these equations, we find that $u^{(1)}- u^{(2)}$ satisfies the variational problem
\beq\label{eq:vp1}
a^{(1)}(u^{(1)}-u^{(2)},v) = \LGtilde(v) \quad\tfa v\in H^1_{0,D}(D)
\eeq
where
\beqs
 \LGtilde(v)\de \int_{D} \left((\Ast-\Aso) \nabla u^{(2)}\right) \cdot\overline{\nabla v} + k^2 (\nso-\nst) u^{(2)}\overline{v}.
\eeqs
Now, by the Cauchy-Schwarz inequality and the definition of the norm $\|\cdot\|_{\HokD}$ (see \cref{eq:weightednorm}), we have
\begin{align*}
| \LGtilde(v)| &\leq \NLiDop{\Aso-\Ast} \big\|\nabla u^{(2)}\big\|_{L^2(D)}
\N{\nabla v}_{L^2(D)} 
\\& \hspace{5cm}+ k^2 
\NLiDRR{\nso-\nst} \big\| u^{(2)}\big\|_{L^2(D)}
\N{v}_{L^2(D)}\\
&\leq\max\Big\{\NLiDop{\Aso-\Ast}\,,\, \NLiDRR{\nso-\nst}\Big\}
\big\| u^{(2)}\big\|_{\HokD} \N{v}_{\HokD}
\end{align*}
(by Cauchy--Schwarz in $\RR^2$). Therefore, by the definition of the norm $\|\cdot\|_{(\HokD)^*}$
\beqs
\big\|\LGtilde\big\|_{(\HokD)^*}\leq \max\set{\NLiDop{\Aso-\Ast},\NLiDRR{\nso-\nst}}.
\big\| u^{(2)}\big\|_{\HokD}.
\eeqs
Since \cref{cond:1nbpc} holds, we can then apply \cref{lem:H1}, i.e.~the bound \cref{eq:bound2}, to the solution of the variational problem \cref{eq:vp1}  to find that 
\begin{align*}
\frac{\big\| u^{(1)} - u^{(2)}\big\|_{\HokD}}
{\big\| u^{(2)}\big\|_{\HokD}, 
}
 \leq 
\,&\frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( 1 + 2 C^{(1)}_{\rm bound}\nsomax  k\right)
\\
&\quad \mleft(\max\set{\NLiDop{\Aso-\Ast},\NLiDRR{\nso-\nst}}\mright),
\end{align*}
and then the result \cref{eq:PDEbound} follows with 
\beq\label{eq:C3}
C_3\de \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right).
\eeq
\epf

\bpf[Proof of \cref{lem:sharp}]
\label{page:lemsharpproof}
We actually prove the stronger result that given any function $c(k)$ such that $c(k)>0$ for all $k>0$, there exist 
$f, \nso,$ and $ \nst$ (with $\nso\not= \nst$) with
\beq\label{eq:nck}
\NLiDRR{\nso-\nst} \sim c(k)
\eeq
such that the corresponding solutions $u^{(1)}$ and $u^{(2)}$ of \cref{prob:vedp} with $\Aso = \Ast= I$ exist, are unique, and satisfy \cref{eq:sharp1}. 

The heart of the proof is the equation
\beq\label{eq:obs1}
(\Delta + k^2) \big(e^{i k r}\chi(r)\big) =  e^{i k r}\mleft(\Delta \chi(r) + 2ik\frac{\partial \chi}{\partial r}(r) + i k \frac{d-1}{r} \chi(r)\mright)=: -\widetilde{f}(r),
\eeq
where $\chi(r)$ is chosen to have $\supp \chi \subset D$. Observe that \cref{eq:obs1} is the Helmholtz operator applied to a circular wave $e^{ikr}$, with the added factor $\chi$ which can be chosen to have compact support. The equation \cref{eq:obs1} can be proved using the formula for the Laplacian in $d$-dimensional spherical coordinates
\beq\label{eq:sphericallaplacian}
\Delta \chi = \frac{1}{r^{d-1}} \frac{\partial}{\partial r}\mleft(r^{d-1} \frac{\partial \chi}{\partial r} \mright) + \frac1{r^2} \LapBel \chi,
\eeq
where $\LapBel$ is the Laplace--Beltrami operator on the $d-1$-dimensional sphere (see, e.g., \cite[Equations (17.23) and (17.25)]{RiHoBe:97} for \cref{eq:sphericallaplacian} in $d=2$ and $3.$). Observe that $e^{ikr} \chi(r)$ has $\LapBel e^{ikr} \chi(r) = 0.$

We expect that \cref{eq:obs1} will be key in the proof of the sharpness of \cref{eq:PDEbound}, for the following reasons. Observe that \cref{eq:obs1} proves the sharpness of the nontrapping resolvent estimate \cref{eq:bound1}, since $\NLtD{\ftilde}\sim k$ and $\NHokD{e^{ikr}\chi(r)}\sim k$  and hence $\NHokD{e^{ikr}\chi(r)} \sim \NLtD{\ftilde}$ (see, e.g., \cite[Lemma 3.10]{ChMo:08},  \cite[Lemma 4.12]{Sp:14}).

Also, recall that  the nontrapping resolvent estimate \cref{eq:bound1} was used in the proof of the PDE bound \cref{eq:PDEbound} applied to $\uso-\ust.$ Therefore we expect that if we set things up so that
\beq\label{eq:sharpdiff}
\uso-\ust = e^{ikr}\chi(r),
\eeq
then  combining \cref{eq:obs1} and \cref{eq:sharpdiff} will show the sharpness of the PDE bound \cref{eq:PDEbound}. Moreover, the function $e^{ikr} \chi(r)$ was used to prove the sharpness of resolvent estimates in \cite[Discussion on p. 1445 and Lemma 3.10]{ChMo:08} and \cite[Lemma 4.12]{Sp:14}, and so we can expect it will also be effective for proving sharpness in our setting.

We now set things up so that \cref{eq:sharpdiff} holds. We define $\nso = 1$ and
\beq\label{eq:fiddlyntdone}
\nst = \nso + c(k) \chitilde(r),
\eeq
for some function $\chitilde(r)$ such that $\widetilde{\chi}\in C^{\infty}(D)$, $\widetilde{\chi}\not = 1$ (so that $\nst\not = \nso$), $\supp \, \widetilde{\chi} \compcont D$, and $\NLiDRR{\chitilde} = 1$ (so that $\NLiDRR{\nso-\nst} = c(k)$).   As above, let $\chi=\chi(r)$ with $\chi \in C^{\infty}(D)$ and $\supp \,\chi \compcont D$. We will specify $\chitilde$ and $\chi$ in more detail later.

Let $\ftilde(r)$ be as defined in \cref{eq:obs1}, and define
\beq\label{eq:obs3}
u^{(2)}(\bx)\de -\frac{1}{k^2 c(k)}\frac{\widetilde{f}(r)}{\widetilde{\chi}(r)}
\eeq
and
\beq\label{eq:fiddlyf}
f(\bx)\de -\big(\Delta +k^2 \nst(\bx)\big) u^{(2)}(\bx).
\eeq
I.e., $\ust$ solves \cref{prob:vedp} with coefficients $\Ast = I$ and $\nst$ given by \cref{eq:fiddlyntdone}, and right-hand side $f.$ We will define $\chi$ and $\chitilde$ below in such a way that $\ust \in \HoD$ and $f \in \LtD.$ In particular, we choose $\chi$ and $\chitilde$ so that if $\chitilde=0,$ then $\chi = 0$. Since $\ftilde$ depends on $\chi,$ this relation means we understand the right-hand side of \cref{eq:obs3} to be zero if $\chitilde$ is zero. In addition, since $\widetilde{\chi}(r)$ has compact support and $\ftilde$ depends on $\chi,$ we need to tie both the support of $\widetilde{\chi}$ and how fast $\widetilde{\chi}$ vanishes in a neighbourhood of its support to the definition of $\chi$ for both $u^{(2)}$ and $f$ to be well defined. As the final part of the setup, let $\uso$ solve
\beqs
\mleft(\Delta + k^2\mright) \uso = -f.
\eeqs
I.e., $\uso$ solves \cref{prob:vedp} with coefficients $\Aso = I$ and $\nso =1$ and right-hand side $f$.

Now observe that by construction (since $\nst$ is given by \cref{eq:fiddlyntdone})

\begin{align*}
  \mleft(\Delta + k^2\mright) \mleft(\uso-\ust\mright) &= \mleft(\Delta + k^2\mright)\uso - \mleft(\Delta + k^2 \nst - k^2\mleft(\nst - 1\mright)\mright)\ust\\
  &= -f + f + k^2\mleft(\nst-1\mright)\ust\\
   &= k^2 \mleft(\nst-1\mright)\ust\\
  &= k^2 c(k) \chitilde \frac{-1}{k^2 c(k)} \frac{\ftilde}{\chitilde}\\
  &= -\ftilde\\
  &= \mleft(\Delta + k^2 \mright)\mleft(e^{ikr}\chi(r)\mright).
\end{align*}
Therefore, by uniqueness of the solution of \cref{prob:vedp} (with constant coefficients)
 \beq\label{eq:obs4}
u^{(1)}(\bx)- u^{(2)}(\bx) = e^{i k r}\chi(r).
\eeq
Therefore, by \cref{eq:obs4} and the properties of $e^{ikr} \chi(r)$ discussed above,  we have
\beq\label{eq:pdenumber1}
\big\|u^{(1)}-u^{(2)}\big\|_{L^2(D)} \sim 1
\quad \tand \quad
\big\|u^{(1)}-u^{(2)}\big\|_{\HokD} \sim k.
\eeq
Furthermore, the definitions of $u^{(2)}$ and $\widetilde{f}$ imply that
\beq\label{eq:pdenumber2}
\big\| u^{(2)}\big\|_{L^2(D)} \sim \frac{1}{k\, c(k)} \quad\tand \quad 
\big\| u^{(2)}\big\|_{\HokD} \sim \frac{1}{c(k)},
\eeq
and, since $\|\nso- \nst\|_{L^\infty(D)} = c(k)$, by combining \cref{eq:pdenumber1,eq:pdenumber2}, we see \cref{eq:nck} holds, as required.

Therefore, to complete the proof, we only need to show that there exists a choice of $\chi$ and $\widetilde{\chi}$ for which $u^{(2)}$ and $f$ defined by \cref{eq:obs3,eq:fiddlyf} are 
in $H^{1}(D)$ and $\LtD$ respectively (in fact, we prove that they are in $W^{1,\infty}(D)$ and $L^\infty(D)$ respectively).
%well-defined. 
Because $\chi$ and $\widetilde{\chi}$ are in $C^\infty(D)$ and we choose $\chi$ and $\chitilde$ so that if $\chitilde=0,$ then $\chi=0$, the only issue is what happens at the boundary of the support of $\widetilde{\chi}$, where $u^{(2)}$ has the potential to be singular.
Since $\clos{\Dm} \subset \BR$, there exist $0<R_1<R_2<R$ such that $\overline{\Dm} \subset B_{R_2}\setminus B_{R_1} \subset \BR$. Let $\supp \chi = B_{R_2}\setminus B_{R_1}$ and let $\chi$ vanish to order $m$ at $r= R_1$ and $r=R_2$; i.e.~$\chi(r) \sim (r-R_1)^m$ as $r \downarrow R_1$ and 
$\chi(r) \sim (R_2-r)^m$ as $r \uparrow R_2$. The definition of $\widetilde{f}$ \cref{eq:obs1} then implies that $\widetilde{f}$ vanishes to order $m-2$. Let $\widetilde{\chi}(r)$ vanish to order $\mtilde$ at $r= R_1$ and $r=R_2$. 
We now claim that if $m >\mtilde+4$, then $u^{(2)}\in W^{1,\infty}(D)$ and $f$ $\in L^\infty(D)$. Indeed,  
straightforward calculation from \cref{eq:obs3} shows that  $u^{(2)}(r) \sim (r-R_1)^{m-\mtilde-2}$, $\nabla u^{(2)}(r) \sim (r-R_1)^{m-\mtilde-3}$, and $\Delta u^{(2)}(r) \sim (r-R_1)^{m-\mtilde-4}$ as $r \downarrow R_1$, with analogous behaviour at $r=R_2$.
The assumption 
$m >\mtilde+4$ therefore implies that $u^{(2)}$, $\nabla u ^{(2)}$, and $\Delta u^{(2)}$ vanish (and hence are finite) at $r=R_1$ and $r=R_2$.
\epf

\bre[Why doesn't \cref{lem:sharp} cover the case $\Aso\neq  \Ast$?]
When $\nj\de1$, $j=1,2,$ $\Aso\de I$, and $\Ast\de I + c(k)\widetilde{\chi}$, the variational problem \cref{eq:vp1} implies that 
\beq\label{eq:obs2}
\Delta \big( u^{(1)} - u^{(2)}\big) + k^2 \big( u^{(1)} - u^{(2)}\big) = c(k)\nabla\cdot \big(\widetilde{\chi}\nabla u^{(2)}\big).
\eeq
It is now much harder than in \cref{eq:obs2} to set things up so that $ u^{(1)}(\bx) - u^{(2)}(\bx)=e^{i kr}\chi(r)$ (so that one can then use \cref{eq:obs1}).
\ere

%\section{Proof of \cref{lem:2}}

\section[Extension to weaker norms]{Extension of the nearby preconditioning results to weaker norms}\label{sec:weaknorm}
Recall from \cref{sec:num,sec:main} that GMRES applied to $\AmatoI \Amatt$ converges in a $k$-indepen\-dent number of iterations if $k\NLiDRR{\nso-\nst}$ is sufficiently small (with an analagous result for $\Aso-\Ast$). This result (and the related numerics) shows that $1/k$ may be a sharp threshold when we consider the maximum norm of the difference between $\nso$ and $\nst$. However, this result does not say anything if $\nso-\nst$ is merely small in some integral norm. For example if $\nso$ and $\nst$ (defined on the unit square) are given by
\beq\label{eq:noweak}
\nso(\bx) =
\begin{dcases}
  1 &\tif \bx_1 \leq \half\\
  2  &\tif \bx_1 > \half
  \end{dcases}
\eeq
and
\beq\label{eq:ntweak}
\nst(\bx) =
\begin{dcases}
  1 &\tif \bx_1 \leq \half+\alpha\\
  2  &\tif \bx_1 > \half+\alpha
  \end{dcases}
\eeq
for some $0 < \alpha < 1/2,$ then $\NLiDRR{\nso-\nst} = 1$ for all $\alpha$, but one would expect that for small $\alpha$ the corresponding solutions of \cref{prob:edp} would satisfy $\uso \approx \ust.$ In addition, one might expect that GMRES applied to $\AmatoI\Amatt$ would converge in a $k$-independent number of iterations. Therefore, in this \lcnamecref{sec:weaknorm} we seek to obtain analogues of \cref{thm:1,cor:1,cor:1a} with the difference in $\nso-\nst$ and $\Aso-\Ast$ measured in weaker norms than the $L^\infty$ norm.

The (realistic) best-case result we could obtain would be that GMRES applied to $\AmatoI\Amatt$ converges in a $k$-independent number of iterations if $\NLoDRR{\nso-\nst} \lesssim 1/k$. This result is `best' in the sense that it depends optimally on $k$; recall the discussion in \cref{rem:physical1k} that $1/k$ is the length scale governing the behaviour of Helmholtz problems. In addition, we measure $\nso-\nst$ in the $L^\infty$ norm as above, we are able to control the magnitude of $\nso-\nst$, but not the spatial variability; if $\nso-\nst \neq 0$ only on a set of small (but nonzero) measure, and $\nso-\nst=1$ on this small set, then $\NLiDRR{\nso-\nst} = 1$, regardless of the measure of the set. In contrast, the $L^1$ norm allows us to control both the magnitude of $\nso-\nst$ and the measure of the sets on which it is nonzero.

We will give numerical results indicating that this theoretical best-case result can be achieved (our numerical results actually indicate that we can obtain $k$-independent convergence when $\NLqDRR{\nso-\nst}\sim 1/k$ for any $1 \leq q < \infty$). We will also provide theory results that are, to our knowledge, the best one can prove, although they are sub-optimal in both $q$ and the dependence on $k.$


\subsection{Theory in weaker norms}\label{sec:weakertheory}
Before we prove results analogous to \cref{cor:1,cor:1a} in weaker norms (using a result analogous to \cref{thm:1} in weaker norms), we first recap why the terms  $\NLiDop{\Aso-\Ast}$ and $\NLiDRR{\nso-\nst}$ appear in \cref{thm:1}. These terms appear in \cref{thm:1} because the terms $\NLiDRR{n}$ and $\NLiDop{A}$ appear in \cref{lem:keylemma1,lem:keylemma2}, respectively. These terms appear in these \lcnamecrefs{lem:keylemma1} because in \cref{eq:mainevent1a,eq:Fbounda} we use the bounds
\beq\label{eq:keynbound}
\NLtD{n\ftilde} \leq \NLiDRR{n}\NLtD{\ftilde}
\eeq
and
\beq\label{eq:keyAbound}
\NLtD{A \grad \ftilde} \leq \NLiDop{A}\NLtD{\grad \ftilde}
\eeq
respectively, for an arbitrary function $\ftilde \in \Vhp,$ and these bounds are carried through the rest of the proof.

However, we observe that we have the following generalisation of H\"older's inequality: If $q,s > 2$ such that $1/2 = 1/q+1/s,$ then
\beq\label{eq:genholder}
\NLtD{\vo\vt} \leq \NLqD{\vo}\NLsD{\vt}.
\eeq

If we instead use \cref{eq:genholder} to bound \cref{eq:keynbound,eq:keyAbound} we obtain
\beq\label{eq:keynbound2}
\NLtD{n\ftilde} \leq \NLqDRR{n}\NLsD{\ftilde}
\eeq
and
\beq\label{eq:keyAbound2}
\NLtD{A\grad\ftilde} \leq \NLqDop{A}\NLsD{\grad\ftilde}.
\eeq

As $\ftilde \in \Vhp$, we can apply an inverse inequality to bound $\NLsD{\ftilde}$ by $\NLtD{\ftilde}$. The required inverse inequality is (see \cite[Theorem 4.5.11 and Remark 4.5.20]{BrSc:08}
\beq\label{eq:inverses}
\NLsD{\ftilde} \leq \Cinvs h^{d\mleft(\frac1{s} - \half\mright)} \NLtD{\ftilde}.
\eeq
If we then apply \cref{eq:inverses} to \cref{eq:keynbound2,eq:keyAbound2} we obtain
\beq\label{eq:keynboundfinal}
\NLtD{n\ftilde} \leq \Cinvs \NLqDRR{n} h^{d\mleft(\frac1{s} - \half\mright)} \NLtD{\ftilde} = \Cinvs \NLqDRR{n} h^{-\frac{d}q} \NLtD{\ftilde}
\eeq
and
\beq\label{eq:keyAboundfinal}
\NLtD{A\grad\ftilde} \leq \Cinvs \NLqDop{A} h^{d\mleft(\frac1{s} - \half\mright)} \NLtD{\grad\ftilde} = \Cinvs \NLqDop{A} h^{-\frac{d}q} \NLtD{\grad\ftilde}.
\eeq

Replacing \cref{eq:mainevent1a,eq:Fbounda} with \cref{eq:keynboundfinal,eq:keyAboundfinal} in the proofs of \cref{lem:keylemma1,lem:keylemma2}, and proceeding as in those proofs, we can obtain the following \lcnamecrefs{cor:1alt}, the analogues of \cref{cor:1,cor:1a}.

\bth[Alternative answer to \cref{it:nbpcq1}: $k$-independent weighted GMRES iterations]\label{cor:1alt}

\

\noindent Let the assumptions of \cref{cor:1} hold.  Given $q >2$, there exist $\Cotilde, \Cttilde>0$, independent of $h$ and $k$ (but dependent on $d, \Dm, \Aso, \nso$, $p$, $q$, and $\kz$) such that if 
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:condalt}
\Cotilde kh^{-\frac{d}{q}} \NLqDop{\Aso-\Ast} +\Cttilde  kh^{-\frac{d}{q}} \NLqDRR{\nso-\nst},
\leq \frac{1}{2}
\eeq
then \emph{both} weighted GMRES working in $\|\cdot\|_{\Dmat_k}$ (and the associated inner product) applied to \cref{eq:pcsystem1} \emph{and} weighted GMRES working in $\|\cdot\|_{(\Dmat_k)^{-1}}$ (and the associated inner product) applied to \cref{eq:pcsystem2}  converge in a $k$-independent number of iterations.
\enth

\bth[Alternative answer to \cref{it:nbpcq1}: $k$-independent (unweighted) GMRES iterations]\label{cor:1aalt}

\

\noindent Let the assumptions of \cref{cor:1a} hold.  Given $q >2$, there exist $\Cotilde, \Cttilde>0$, independent of $h$ and $k$ (but dependent on $d, \Dm, \Aso, \nso$, $p$, $q$, and $\kz$) such that if
% there exists $C_2>0$, independent of $h$ and $k$ (but dependent on $\Dm, \Aso, \nso$, $p$, and $k_0$) and given explicitly in \cref{eq:C2} below,
% such that if 
\beq\label{eq:condaalt}
\Cotilde \mleft(\frac{\splus}{\mminus}\mright) h^{-\frac{d}{q}-1} \NLqDop{\Aso-\Ast} + \Cttilde \mleft(\frac{\mplus}{\mminus}\mright) kh^{-\frac{d}{q}} \NLqDRR{\nso-\nst} \leq \half,
\eeq
then standard GMRES (working in the Euclidean norm and inner product) applied to either of the equations \cref{eq:pcsystem1} or \cref{eq:pcsystem2}
%\beqs
%(\Amat^{(1)})^{-1}\Amat^{(2)}\uvec = \fvec\quad\text{ or } \quad\Amat^{(2)}(\Amat^{(1)})^{-1}\vvec = \fvec
%\eeqs
 converges in a $k$-independent number of iterations.
 \enth

 A sketch proof of \cref{cor:1alt,cor:1aalt} is on \cpageref{page:cor1altcor1aaltproof} below.

\bre[Trade off between the type of norm and powers of $h$ and $k$]
Observe that in \cref{cor:1alt,cor:1aalt} there is a trade-off between the norm that one uses to measure $\nso-\nst$ (or $\Aso-\Ast$) and the restriction on the magnitude of this norm. E.g., the condition on $\nso-\nst$ in both \cref{cor:1alt,cor:1aalt} can be summarised as
\beq\label{eq:altsufficientlysmall}
\NLqDRR{\nso-\nst} k h^{-\frac{d}{q}} \text{ is sufficiently small}.
\eeq
with analogous conditions on $\Aso-\Ast.$ Observe that as $q \downarrow 2,$ we measure $\nso-\nst$ in a weaker norm, but the condition \cref{eq:altsufficientlysmall} becomes more restrictive; the power of $h$ increases. Conversely, as $q \uparrow \infty,$ we measure $\nso-\nst$ in a stronger norm, but the condition \cref{eq:altsufficientlysmall} becomes less restrictive; the power of $h$ decreases. (Also observe that in the $q\uparrow\infty$ limit we recover the condition \cref{eq:sufficientlysmall} we previously proved for $\NLiDRR{\nso-\nst}.$
\ere

\bre[\Cref{cor:1,cor:1a} are a special case of \cref{cor:1alt,cor:1aalt}]
Observe that in the case $q=\infty$ \cref{cor:1alt,cor:1aalt} become our previous results in the $L^\infty$ norm, \cref{cor:1,cor:1a}.
\ere

The numerical experiments in \cref{sec:weakernumerics} below suggest that, at least in certain cases, a sufficient condition for nearby preconditioning to be effective is
\beq\label{eq:experimentalsufficientlysmall}
\NLqDRR{\no-\nt} k \quad\text{is sufficiently small},
\eeq
for \emph{any} $q \geq 1$, and moreover \cref{eq:experimentalsufficientlysmall} appears sharp in its $k$-dependence. (This requirement would fit with our previous observation about $1/k$ being the length scale below which perturbations cannot be seen---see \cref{rem:physical1k} above.) However, we do not say that \cref{eq:experimentalsufficientlysmall} is sufficient for all cases; recall that for transmission problems, very small perturbations in $n$ can lead to very different behaviour in the solution $u$ if $k$ is a quasi-resonance for $\no$ or $\nt$; see the discussion at the end of \cref{sec:wpdisc} above.


\subsubsection{Proof of \cref{cor:1alt,cor:1aalt}}

We first state analogues of \cref{lem:keylemma1,lem:keylemma2} in weaker norms; these \lcnamecrefs{lem:keylemma1} are the key to the proofs of \cref{cor:1alt,cor:1aalt} above. The essence of the proofs of \cref{lem:keylemma1a,lem:keylemma2a} are the discussion at the start of \cref{sec:weakertheory}.

\ble[Alternative bounds on $(\Amato)^{-1} \Mmat_{n}$]\label{lem:keylemma1a}
Under the assumptions of \cref{lem:keylemma1}, for $n\in \LiDRR$ and for any $q > 2$,
\beq\label{eq:keybound12}
\max\set{\NDk{\AmatoI \Mmatn},\NDkI{\Mmatn\AmatoI}} \leq \Cttilde h^{-\frac{d}{q}} \frac{\NLqDRR{n}}k
\eeq
and 
\beq\label{eq:keybound1a2}
\max\set{\Nt{\AmatoI \Mmatn},\Nt{\Mmatn\AmatoI}} \leq \Cttilde\mleft(\frac{\mplus}{\mminus}\mright) h^{-\frac{d}q} \frac{\NLqDRR{n}}k,
\eeq
where
\beq\label{eq:C2tilde}
\Cttilde\de%\frac{m_+}{m_-} 
%\left[ 
\Cinvs\Ct,
\eeq
where $\Ct$ is defined by \cref{eq:C2} and $1/s = 1/2 - 1/q.$
\ele

\ble[Alternative bounds on $(\Amato)^{-1} \Smat_A$]\label{lem:keylemma2a}
Under the assumptions of \cref{lem:keylemma2}, for $A\in L^\infty(D,\RR^{d\times d})$ and for any $q > 2$
\beq\label{eq:keybound22}
\max\set{\NDk{\AmatoI \SmatA},\NDkI{\SmatA\AmatoI}} \leq \Cotilde h^{-\frac{d}q}k \NLqDop{A}
\eeq
and
\beq\label{eq:keybound2a2}
\max\set{\Nt{\AmatoI \SmatA},\Nt{\SmatA\AmatoI}} \leq \Cotilde\mleft(\frac{\splus}{\mminus}\mright) h^{-\frac{d}q-1} \NLqDRR{A},
\eeq
%\begin{align}\nonumber
%&\max\Big\{\big\| (\Amato)^{-1} \Smat_A \big\|_2, \,\,
%\big\| \Smat_A (\Amato)^{-1} \big\|_2\Big\}\nonumber \\
%&\hspace{2cm}
% \leq \frac{s_+}{s_-} \left[ C_{\rm FEM2}^{(1)} + 
% \frac{1}{\min\big\{\Asomin,\nsomin\big\}}\left( \frac{1}{k_0} + 2 C^{(1)}_{\rm bound}\nsomax  \right) \right]k\N{A}_{L^\infty(D)}\label{eq:keybound2}
%% + C_{\rm bound}^{(1)}\right) \frac{\N{n}_{L^\infty(D)}}{k}.
%\end{align}
where
\beq\label{eq:C1tildenbpc}
\Cotilde \de \Cinvs\Co,
\eeq
where $\Co$ is given by \cref{eq:C1nbpc} and $1/s = 1/2 - 1/q.$
\ele

The proofs of \cref{lem:keylemma1a,lem:keylemma2a} are virtually identical to the proofs of \cref{lem:keylemma1,lem:keylemma2}, with the modifications for $L^q$ norms detailed at the beginning of \cref{sec:weakertheory}.

\bre[Reduction to \cref{lem:keylemma1,lem:keylemma2}]
Observe that in the case $s=2$ and $q=\infty$ \cref{lem:keylemma1a,lem:keylemma2a} reduce to our previous results \cref{lem:keylemma1,lem:keylemma2}.
\ere

We can use \cref{lem:keylemma1a,lem:keylemma2a} in place of \cref{lem:keylemma1,lem:keylemma2} to obtain the following analogue of \cref{thm:1} in weaker norms.

\begin{theorem}[Alternative main ingredient to answer to \cref{it:nbpcq1}]\label{thm:1alt}
If all the assumptions of \namecref{thm:1} \ref{thm:1} hold, then there exist $\Cotilde, \Cttilde>0$, independent of $h$ and $k$ (but dependent on $d, \Dm, \Aso, \nso$, $p$, $q$, and $\kz$) such that
\begin{align}\nonumber
&\max\set{\NDk{\Imat - \AmatoI\Amatt},\NDkI{\Imat -\Amatt\AmatoI}}\\
&\hspace{3cm} 
\leq \Cotilde kh^{-\frac{d}q} \NLqDop{\Aso-\Ast} + \Cttilde  kh^{-\frac{d}q}  \NLqDRR{\nso-\nst}
\label{eq:main1alt}
\end{align}
and 
\begin{align}\nonumber
&\max\set{\Nt{\Imat - \AmatoI\Amatt}, \Nt{\Imat -\Amatt\AmatoI}}\\
&\hspace{0cm}
\leq \Cotilde \mleft(\frac{\splus}{\mminus}\mright) h^{-\frac{d}q-1}\NLqDop{\Aso-\Ast} + \Cttilde \mleft(\frac{\mplus}{\mminus}\mright) kh^{-\frac{d}q}\NLqDRR{\nso-\nst}.
\label{eq:main1aalt}
\end{align}
\end{theorem}

The proof of \cref{thm:1alt} is identical to the proof of \cref{thm:1}, with \cref{lem:keylemma1,lem:keylemma2} replaced by \cref{lem:keylemma1a,lem:keylemma2a}.

%We can now use \cref{thm:1alt} to obtain the following analogues to \cref{cor:1,cor:1a} in weaker norms.

\bpf[Sketch proof of \cref{cor:1alt,cor:1aalt}]
\label{page:cor1altcor1aaltproof}
The proofs of \cref{cor:1alt,cor:1aalt} are completely analagous to the proofs of \cref{cor:1,cor:1a}, with the exception that we use \cref{thm:1alt} in place of \cref{thm:1}.
\epf


\subsection{Numerics in weaker norms}\label{sec:weakernumerics}
For our computations, we use the computational setup as in \cref{app:compsetup}, with $f$ and $\gI$ corresponding to a plane wave passing through homogeneous media.  We let $\Aso=\Ast=I,$ and we define $\nso$ and $\nst$ by \cref{eq:noweak,eq:ntweak}. For $\alpha = 0.2k^{-\beta},$ $\beta = 0,0.1,\ldots,0.9,1$ and for $k=10,20,\ldots,100$ we used GMRES to solve $\AmatoI\Amatt = \AmatoI \fvec$ (for $\fvec$ given by the Helmholtz problem), and we record the number of GMRES iterations taken to achieve convergence.

Our results in \cref{fig:l1low,fig:l1med,fig:l1high} (also displayed in \cref{tab:l1}) indicate the following conclusions for $\NLqDRR{\nso-\nst} \sim 0.1/k^{-\beta}$, for all $1 \leq q < \infty$:
\bit
\item For $\beta \in (0,0.6)$ there is clear growth of the number of GMRES iterations with $k$,
\item For $\beta = 1$ there is clear boundedness of the number of GMRES iterations with $k$, and
  \item for $\beta \in (0.7,0.9)$ it is unclear if the number of GMRES iterations grows with $k.$
    \eit
We note that the results in \cref{fig:l1low,fig:l1med,fig:l1high} are the analogues of those in \cref{fig:linfinityn0,fig:linfinityn1,fig:linfinityn2}.

If we compare our numerical results with the theory results in \cref{cor:1aalt}, we see that the theory (if $h \sim k^{-3/2}$ and $d=2$, as in our computational experiments) predicts that the number of iterations will remain bounded if $\NLqDRR{\nso-\nst} k^{1+3/q}$ is sufficiently small, for any $q > 2.$ Our computed results indicate that this result is not sharp. The computed results indicate that if $\NLqDRR{\nso-\nst} \sim k^{-1}$ for any $q \geq 1,$ then the number of GMRES iterations is bounded as $k$ increases. Observe again that the `best case' $1/k$ condition is only predicted by the theory in the $q\rightarrow \infty$ limit.

\begin{figure}
\input{l1-low.pgf}
  \caption[GMRES iteration counts when $\NLqDRR{\nso-\nst} = 0.2\times k^{-\beta},$ for any $1 \leq q < \infty$ and $\beta = 0,0.1,0.2,0.3$.]{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2\times k^{-\beta},$ for $\beta = 0,0.1,0.2,0.3.$}\label{fig:l1low}
\end{figure}

\begin{figure}
  \input{l1-med.pgf}
    \caption[GMRES iteration counts when $\NLqDRR{\nso-\nst} = 0.2\times k^{-\beta},$ for any $1 \leq q < \infty$ and $\beta = 0.4,0.5,0.6,0.7$.]{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2\times k^{-\beta},$ for $\beta = 0.4,0.5,0.6,0.7.$}\label{fig:l1med}
\end{figure}
    
    \begin{figure}
    \input{l1-high.pgf}
      \caption[GMRES iteration counts when $\NLqDRR{\nso-\nst} = 0.2\times k^{-\beta},$ for any $1 \leq q < \infty$ and $\beta = 0.8,0.9,1$.]{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2\times k^{-\beta},$ for $\beta = 0.8,0.9,1.$}\label{fig:l1high}
\end{figure}

\begin{table}
  \centering
  \input{l1-table}
  \caption[GMRES iteration counts when $\NLqDRR{\nso-\nst} = 0.2\times k^{-\beta},$ for any $1 \leq q < \infty$ and $\beta = 0,0.1,\ldots,1$.]{GMRES iteration counts for $\AmatoI\Amatt$ given by \cref{eq:noweak,eq:ntweak}, where $\alpha = 0.2\times k^{-\beta}.$}\label{tab:l1}
  \end{table}



\section[Applying nearby preconditioning to QMC]{Applying nearby preconditioning to a Quasi-Monte-Carlo method for the Helmholtz equation}\label{sec:nbpcqmc}

We now apply nearby preconditioning in the implementation of a Quasi-Monte-Carlo (QMC) method for the Helmholtz equation. We begin with a brief description of QMC methods, before detailing two ways in which we apply nearby preconditioning to these methods. Finally, we give computational results illustrating this application.

\subsection{Brief description of QMC}

QMC methods (or rules) are high-dimensional quadrature rules, designed to give rates of convergence (with respect to the number of integration points) which are superior to those of Monte-Carlo methods, under certain conditions. Suppose one wants to approximate $\EXP{Q},$ where $Q$ is some random variable (later in this \lcnamecref{sec:nbpcqmc}, $Q$ will be a function of the solution $u(\omega)$ of a stochastic Helmholtz equation). By definition, the expectation is
\beq\label{eq:qmcexpdef}
\EXP{Q} = \int_\Omega Q(\omega)\ \ddPPomega.
\eeq

If we now suppose $Q$ depends on the sample space $\Omega$ via a finite set of random variables $\Uo,\ldots,\UJ$, then we can rewrite \cref{eq:qmcexpdef} as
\beq\label{eq:qmcexp2}
\EXP{Q} = \int_\Omega Q\mleft((\Uo(\omega),\ldots,\UJ(\omega)\mright)\, \ddPPomega.
\eeq
If, for example, the $\Uj$ are all independant uniform random variables on $\mleft[-1/2,1/2\mright]$, then \cref{eq:qmcexp2} can be rewritten as
\beq\label{eq:qmcexp3}
\EXP{Q} = \int_{\cube{J}} \hspace{-3em}Q\mleft(\by\mright)\, \dd\lambda(\by),
\eeq
where $\lambda$ denotes Lebesgue measure.

Any quadrature rule, or method for approximating $\EXP{Q}$, can then be seen as a method for approximating the $J$-dimensional integral on the right-hand side of \cref{eq:qmcexp3} and vice-versa. Equal-weight quadrature rules choose points $\byo,\ldots,\byNpoints \in \cube{J}$ and use the approximation
\beqs
\EXP{Q} \approx \frac1{\Npoints}\sum_{l=1}^{\Npoints} Q\mleft(\byl\mright).
\eeqs
Monte-Carlo and Quasi-Monte-Carlo rules correspond to different choices of the points $\byl$. In a Monte-Carlo rule the points are chosen at random in accordance with the associated probability distribution. For example, in the case that the $\Uj$ are $\Unif(-1/2,1/2)$ random variables, the points $\byl$ are chosen according to the Uniform distribution on $\cube{J}$. Observe that Monte-Carlo rules do not need the dependence of $Q$ on $\omega$ to take the form prescribed in \cref{eq:qmcexp2}, indeed, they apply to any random variable.

Quasi-Monte-Carlo rules, in contrast to Monte-Carlo rules, do require the dependence on $\omega$ to be via finitely- or countable-many random variables. This is because QMC rules are high-dimensional quadrature rules (in the simplest case performing quadrature on the high-dim\-en\-sion\-al cube $\cube{J}).$ In pure QMC rules the points $\byl$ are chosen deterministically, unlike Monte-Carlo rules.

The main advantage of QMC rules is that they can exhibit higher rates of convergence compared to Monte-Carlo rules; Monte Carlo rules typically converge with rate $\Npoints^{-1/2}$ (see, e.g., \cite[Section 1.1]{Gi:15}), whereas QMC rules can converge with rates up to $\Npoints^{-1}$  or with even higher rates for higher-order QMC rules, see, e.g., \cite[Penultimate paragraph of Section 1.2]{KuNu:16}.

In applying QMC rules to stochastic PDEs, we assume that the random coefficient ($n$ in our case) is defined via finitely many (or countably many) random variables, as in \cref{eq:qmcexp2} above, and we then use QMC rules to estimate expectations of quantities of interest of the solution $u$, i.e., $Q = Q(u).$ We note that applying QMC rules to stochastic PDEs is a vibrant and active research area. For recent overviews of this field, see \cite{KuNu:16,KuNu:18b} (and the associated tutorial \cite{KuNu:18a}). We note that there is currently no rigorous study of how QMC methods behave for the Helmholtz equation, although we understand some such work is currently underway by Ganesh, Kuo, and Sloan \cite{GaKuSl}.

\subsection{Methods for applying nearby preconditioning to QMC}\label{sec:nbpcqmcnum}
In all of our previous uses of nearby preconditioning, we have fixed $\nso$, the value for which we calculate the preconditioner, and have then used $\Amato$ to precondition $\Amatt$ for different values of $\nst.$ However, the key idea for applying nearby preconditioning to QMC methods for the Helmholtz equation is to choose \emph{a number} of different realisations of $\nso$ and use each realisation of $\nso$ as a preconditioner only for those  realisations of $\nst$ for which $\Amato$ is a \emph{good} preconditioner for $\Amatt.$ We adopt this approach because it is highly unlikely that a single realisation of $\nso$ will be a good preconditioner for every realisation of $\nst.$

Therefore, the algorithms presented in this \lcnamecref{sec:nbpcqmcnum} seek to answer the two questions:
\ben
\item \emph{For} which realisations of $n$ should a preconditioner be \emph{calculated}?
  \item \emph{To} which realisations of $n$ should each preconditioner be \emph{applied}?
\een

We now detail two methods for using nearby preconditioning to speed up QMC methods for the Helmholtz equation. To apply these methods, we use the following model problem: We consider the Interior Impedance Problem in 2-d with $f=1$ and $\gI=0$, $A = I$, and $n$ given by
\beq\label{eq:artificialkl}
n(\omega,\bx) = 1 + \sum_{j=1}^{10} \Uj(\omega) \sqrt{\lambdaj} \psij(\bx),
\eeq
where
\beq\label{eq:artificialkllambdas}
\sqrt{\lambdaj} = j^{-2}
\eeq
and
\beq\label{eq:artificialklfuns}
\psij(\bx) = \cos\mleft(\frac{j\pi}4 x\mright)\cos\mleft(\frac{\mleft(j+1\mright)\pi}4 y\mright).
\eeq
Observe that $\NLiDR{\psij}=1$ for all $j,$ and $\sqrt{\lambdaj} \rightarrow 0$ as $j \rightarrow \infty.$ Also note that $\nmin = 1 - \mleft(\sum_{j=1}^{10} j^{-2}\mright)/2 \approx 0.225.$ This expansion is based on the random-field expansion used in \cite[Section 5.1]{GiGrKuScSl:19}, although the main change we make from \cite{GiGrKuScSl:19} is to introduce the factors $1/4$ in \cref{eq:artificialklfuns}. We introduce this factor to ensure that the oscillations in the medium $n$ are `low frequency' compared to the frequency $k$ of the waves passing through the medium\footnote{The highest `frequency' associated with the oscillations in the medium is $(10+1)\pi/4 \approx 26$, whereas we consider waves with frequencies $k=10,\ldots,60$. Therefore (for $k > 26$) the waves are of a `higher frequency' than the medium. Moreover, we would see if there is any change in the behaviour of our algorithm as the frequency of the waves increases past the `frequency' of the medium. However, we do not see any such change.}. Expansions similar to \cref{eq:artificialkl} are often decribed as `artificial Karhunen--Lo\`eve expansions' due to their similarity with the Karhunen--Lo\`eve expansion of a random field. In a Karhunen--Lo\`eve expansion the $\Uj$ are independent random variables whose distribution is determined by the distribution of the random field, and the $\lambdaj$ and $\psij$ are the eigenvalues and eigenvectors of the covariance operator, see, e.g., \cite[Section 7.4]{LoPoSh:14}.

In the remainder of this \lcnamecref{sec:nbpcqmcnum} we will be using QMC methods to approximate $\EXP{Q(u)}$ (for some quantities of interest $Q$). Observe that this expectation can be written
\beqs
\EXP{Q(u)} = \int_{\Omega} Q(u(\omega)) \,\ddPPomega = \int_{\mleft[-\half,\half\mright]^{10}}Q(u(U_1,\ldots,U_{10})) \,\dd U_1 \cdots \dd U_{10},
\eeqs
where we consider $n$ (and therefore $u$) as depending on each of the Uniform random variables $\Uj$ individually. Therefore, because of this correspondence between $n$ as function on $\Omega,$ and $n$ as a function on $\mleft[-1/2,1/2\mright]^{10}$ we will sometimes instead write $n(\by)$ for $\by \in \cube{10}$, by which we mean
\beqs
n(\by) = 1 + \sum_{j=1}^{10} \by_{j} \sqrt{\lambdaj} \psij.
\eeqs
There is no a priori reason that one must have such an affine dependence of the random field on the randomness in order to apply nearby preconditioning to QMC methods. One could, for example, take $n$ to be a lognormal random field, in which case $n$ would take the form $n(\by) = \exp\mleft(\nz + \sum_j \Nj \sqrt{\lambdaj} \psij\mright)$ where the $\Nj$ are Normal$(0,1)$ random variables. However, in the case of affine dependence there is a `parallelisable' nearby-preconditioning-QMC algorithm which we present below.

We stress that the results in this \lcnamecref{sec:nbpcqmcnum} are strictly numerical; there is no current theory to support these calculations. In particular, we observe in \cref{sec:nbpcqmcnumerics} below that in these experiments, for the QMC error for Helmholtz problems to remain bounded as $k$ increases, one must increase the number of QMC points with $k.$ We again remark that there is currently no theoretical justification for this behaviour.


\paragraph{Terminology} Before we describe the nearby-preconditioning-QMC algorithms in detail, we establish two pieces of terminology that will be of use in describing them. Firstly, we will use the word `point' to refer to a point in the parameter space $\cube{J}$, and use phrases such as `calculate a preconditioner at the point $\by$' as shorthand for `calculate the LU decomposition of the system matrix $\Amat$ corresponding to the finite-element discretisation of the Helmholtz IIP (as described above) with coefficient $n(\by)$'.

We also use the words `nearby' and `nearest' (when referring to QMC points) to mean: nearest in the metric
  \beq\label{eq:dapprox}
\dapprox(\byo,\byt) \de \sum_{j=1}^{J} \sqrt{\lambdaj} \abs{{\byo}_{j} - {\byt}_{j}}.%, \tfor \byo,\byt \in \cube{J}.
\eeq

\paragraph{The approximate metric} The metric $\dapprox$ is an approximation of the metric
\beq\label{eq:dqmc}
\dQMC(\byo,\byt) = \NLiDRR{n(\byo)-n(\byt)},% \tfor \byo,\byt \in \cube{J},
\eeq
i.e., the metric on $\cube{J}$ induced by the spatial $L^\infty$ norm. The metric $\dapprox$ is an approximation of $\dQMC$ in the sense made precise in the following \lcnamecref{lem:approxmetric}.

\ble[$\dapprox$ approximates $\dQMC$]\label{lem:approxmetric}
For all $\byo,\byt \in \cube{J},$
\beqs
\dQMC\mleft(\byo,\byt\mright) \leq \dapprox\mleft(\byo,\byt\mright).
\eeqs
\ele

The proof of \cref{lem:approxmetric} is straightforward and omitted.

Observe further that the structure of $\dapprox$ is similar to that of $\dQMC$ and $\dapprox$ is a weighted $L^1$ metric on $\cube{J}$, with the weights corresponding to the terms in \cref{eq:artificialkl}. Recall that $\sqrt{\lambdaj} \rightarrow 0$ as $j \rightarrow \infty$; therefore the higher dimensions contribute less to the value of $\dapprox$ (or, informally, points are `closer' in higher dimensions, or higher dimensions are `smaller' than lower dimensions).

Ideally, for the purposes of utilising nearby preconditioning, we would use the metric $\dQMC$ when describing the geometry of the QMC points  (and computing the nearest QMC point), since the best rigorous results on the behaviour of nearby preconditioning (in terms of $k$-dependence) are proved in \cref{sec:3} for the spatial $L^\infty$-norm\footnote{Although, in line with the results in \cref{sec:weaknorm}, we could instead use a spatial $L^q$ norm, for some $q \geq 1$ in \cref{eq:dqmc}.}. However, computing $\dQMC$ exactly is, in principle. complicated. In contrast, it is easy to compute with $\dapprox,$ since $\dapprox$ enables one to think of $\cube{J}$ as the high-dimensional rectangle $\mleft[0,\sqrt{\lambdao}\mright]\times\cdots\times\mleft[0,\sqrt{\lambdaJ}\mright]$ equipped with the standard $L^1$ metric. Moreover, as discussed above, $\dapprox$ is an approximation of $\dQMC,$ and therefore we expect that it will induce a similar geometry on $\cube{J}.$

\paragraph{Computational complexity of calculating the nearest point} At various points in the two nearby-preconditioning-QMC algorithms we present below, given a point $\by \in \cube{J}$ and a subset $S$ of $\cube{J}$ we must calculate $\nearest(\by,S) \in \cube{J}$, that is the element of $S$ that is closest to $\by$ in the metric $\dapprox.$ In all of the numerical results we present below, we calculate $\nearest(\by,S)$ by brute force, i.e., we calculate $\dapprox(\by,\bytilde)$ for all $\bytilde \in S,$ and choose the element of $S$ that minimises $\dapprox(\by,\bytilde)$. Since calculating $\dapprox(\by,\bytilde)$ involves $\cO\mleft(J\mright)$ operations, the brute force approach to calculating $\dapprox(\by,\bytilde)$ involves $\cO\mleft(J\abs{S}\mright)$ operations. Clearly, this method of calculating $\nearest(\by,S)$ does not scale in $J,$ the stochastic dimension, although it is computationally feasible for our numerical experiments (with $J=10$) below. See \cref{sec:nbpcfuture} below for a suggestion of an alternative, scalable way to calculate $\nearest(\by,S)$.

\subsubsection{A sequential algorithm}
We first describe a straightforward algorithm that uses nearby preconditioning to speed up a QMC calculation. We call this a `sequential' algorithm because, unlike the `parallel' algorithm that we describe below, it is intrinsically sequential and cannot be parallelised, i.e., finite-element solves for different realisations of the random field $n$ cannot be treated in parallel. Although, when performing the individual finite-element solves, one is not restricted to a single core, i.e., one can use parallelisation for each finite-element solve  if the linear systems $\Amat$ are large enough to warrant this.

An overview of the algorithm is:
\ben
\item Choose a QMC point $\by$ for which to calculate a preconditioner
\item\label[itemstep]{it:nearest} Find the nearest QMC point $\byp$ to $\by$ and attempt a GMRES solve of the problem at $\byp$ using the LU decomposition of the system at $\by$ as a preconditioner.
    \item If GMRES converges quickly (i.e., in fewer than a preset number of iterations), return to \cref{it:nearest}.
\item If GMRES takes too long to converge, recalculate the preconditioner at $\byp,$ set $\by = \byp$, and return to \cref{it:nearest}.
  \een
  The algorithm is written in more formal pseudocode in \cref{alg:seq}.
\begin{algorithm}[h]
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
%\SetKwInOut{Output}{Output}
\SetKwFunction{Nearest}{nearest}

\Input{$\maxGMRES$, $\SQMC$}
\BlankLine
Choose starting point $\bystart$\;
$\bypre \defined \bystart$\;
$\Sremaining \defined \SQMC\setminus\set{\bypre}$\;
Calculate and store preconditioner $\Lmat\Umat = \AmatpreI$\;
$\bycurrent \defined$ \Nearest{$\bypre,\Sremaining$}\;
\While{$\Sremaining \neq \emptyset$}{
\eIf{GMRES applied to $\UmatI\LmatI\Amat\bycurrent = \UmatI\LmatI \bff$ converges in fewer than $\maxGMRES$ iterations}{
$\Sremaining \defined \Sremaining\setminus\set{\bycurrent}$\;
$\bycurrent \defined$ \Nearest{$\bypre,\Sremaining$}\;
}{
$\bypre \defined \bycurrent$\;
Calculate and store preconditioner $\Lmat\Umat = \AmatpreI$\;
}
}
\caption[The sequential nearby-preconditioning-Quasi-Monte-Carlo algorithm]{The sequential nearby-preconditioning-Quasi-Monte-Carlo algorithm\label{alg:seq}. $\maxGMRES$ is the maximum allowed number of GMRES iterations and $\SQMC$ is the set of all QMC points. $\nearest(\bypre,\Sremaining)$ denotes the point in $\SQMC$ nearest to $\bypre$ in the $\dQMC$ metric.}
\end{algorithm}
\subsubsection{A parallel algorithm}

The main disadvantage of the `sequential' algorithm described above is that the points at which preconditioners are calculated are identified as the algorithm progresses. The algorithm cannot be parallelised by sending different collections of QMC points to different processors (as one does not know  a priori which preconditioner to use for each QMC point). Therefore, we now suggest an alternative algorithm that allows one to specify the number of preconditioning points \emph{before} the algorithm begins. The algorithm then calculates which points to use as preconditioning points, before performing the linear solves. Because the preconditioners are known in advance, the solves can be computed in parallel if required. The most complicated part of the algorithm is deciding at which points to calculate the preconditioners, and so we describe this part of the algorithm in more detail here. A more formal pseudocode description of the algorithm is given in \cref{alg:par}.

Suppose we are given a set $\SQMC = \set{\byo,\ldots,\byNQMC}$ of QMC points and a number $\Npretarget$; the target number of preconditioners to compute. The aim of this algorithm is to select (approximately) $\Npretarget$ QMC points that are (approximately) equally spaced with respect to the $\dQMC$ metric defined above. If such a goal is achieved, then one expects that the preconditioning points are best located to minimise the total number of GMRES iterations across the solves for all of the QMC points.

The algorithm contains two key ideas:
\ben
  \item Use a surrogate metric in place of $\dQMC$, and
\item Locate the preconditioning points according to a tensor-product rule.
  \een
  We now describe each of these two ideas in turn, before describing our final algorithm.

\paragraph{Surrogate metric} Whilst the metric $\dQMC$ is the metric in which  nearby preconditioning is analysed (as described in \cref{sec:intronbpc} above), in practice $\dQMC$ is difficult to work with, since the  geometry it induces on $\cube{J}$ is nontrivial as this geometry is dependent on the interaction between the functions $\psij$ in the expansion \cref{eq:artificialkl}. Therefore, we work in an alternative, although related metric HELP

\paragraph{Tensor-product algorithm for locating preconditioning points} We first describe the intuition behind our use of a tensor-product rule to locate the preconditioning points (even though we do not use this intuition in the final algorithm). Once we have described this intution, we will then show how it can be adapted to provide the final algorithm. To understand why we use locate the preconditioning points using a tensor-product rule, we first decribe the heuristic we use. Let us assume we want to cover $\cube{J}$ with `balls' of radius $r$. Observe that these balls are measured in the $\dapprox$ metric, and therefore have a similar geometry to balls on $\cube{J}$ in the $L^1$ metric. Therefore, given the centres $\bcone$ and $\bct$ of two adjacent balls, we will have
\beq\label{eq:centres2r}
\dapprox(\bcone,\bct) = 2r.
\eeq
The question now arises of how we choose $\bcone$ and $\bct$ so that \cref{eq:centres2r} holds. We observe that, by the definition of $\dapprox$, if we choose $\bcone$ and $\bct$ such that
\beqs
\sqrt{\lambdaj}\abs{{\bcone}_{j}-{\bct}_j} = \frac{2r}{J} \tforall j = 1,\ldots,J,
\eeqs
then we will have \cref{eq:centres2r} by construction, because
\beqs
\dapprox(\bcone,\bct) = \sum_{j=1}^J \frac{2r}J = 2r.
\eeqs
Therefore, in dimension $j$ we choose the centres of the balls to be spaced
\beqs
\min\set{\frac{2r}{J\sqrt{\lambdaj}},1}
\eeqs
apart (where we include the minimum so that, for high dimensions, we include at least one centre). That is, in dimension $j$, we take
\beq\label{eq:Nj}
\Nj \de \max\set{1,\frac{J\sqrt{\lambdaj}}{2r}}
\eeq
equally spaced points in the sets $\centresj = \set{c_{j,1},\ldots,c_{j,\Nj}},$ and then we form the centres $\bcone,\ldots,\bcNpre$ by taking all possible tensor products of the points in $\centreso,\ldots,\centresJ,$ giving a total of
\beq\label{eq:Npre}
\Npre = \No \times \cdots \times \NJ
\eeq
preconditioning points.

However, we face three immediate difficulties with  the above approach:
\ben
\item The above procedure assumes we know the radius $r$, and then returns the total number of preconditioning points, and their locations. However, we only know in advance the ideal total number of preconditioning points.
\item There is no guarantee that the numbers of points $\Nj$ calculated above are integers.
  \item There is no guarantee the preconditioning points given by the above procedure are QMC points.
    \een
    These questions are all completely valid, and so we slightly modify the above procedure to deal with them.

    \paragraph{Definition of the parallel algorithm} Recall that we assume that we are given a target number of preconditioners $\Npretarget$. The above procedure (amongst other things) defines a map $\Npreideal:\RRp \rightarrow \RRp$ given by $r \mapsto \Npre,$ where $\Npre$ is defined by \cref{eq:Npre} and the number of preconditioners in each dimension is given by \cref{eq:Nj}.  Therefore we can numerically invert this map (or more precisely, calculate numerically the value $\rideal$ such that $\Npreideal(\rideal) = \Npretarget$). (In our computations, we do this calculation via interval bisection.)

Given we expect that the size of the balls over which nearby preconditioning is effective decreases with $\cO\mleft(1/k\mright)$ (in line with \cref{cor:1}), and the number of QMC points needed to keep the error bounded increases with $k$ (see \cref{sec:nbpcqmcnumerics} below), it is not obvious that we should know $\Npreideal$ in advance. See \cpageref{page:seqandpar} for how we use the sequential algorithm to determine how $\Npreideal$ scales with $k$ for the parallel algorithm. We assume for now that we know $\Npreideal$ and hence $\rideal.$

    Once we know the value of $\rideal,$ we can then calculate the numbers of centres in each dimension $\No(\rideal),\ldots,\NJ(\rideal)$ as above (recalling that the $\Nj(\rideal)$ are not necessarily integers). We then obtain integers $\Npreactualj = \round{\Nj(\rideal)}$, where $\round{\cdot}$ denotes rounding to the nearest integer. (Recall $\Nj(\rideal) \geq 1$ for all $j$ by construction, so $\Npreactualj$ will be a positive integer for all $j.$)

We then take $\Npreactualj$ centres in each dimension and define the sets $\centresj$ with $\Nj = \Npreactualj$, as described above. We then obtain a total of $\Npreactual = \Npreactualo \times \cdots \times \NpreactualJ$ preconditioning points.

These points may not be QMC points. We could simply calculate the preconditioners at these non-QMC points. However we instead replace each calculated centre with its nearest QMC point (calculated using brute-force) and calculate the preconditioners at these QMC points. We denote the set of preconditioning points by $\Spre$. Finally, we calculate the map $\Prenearest:\SQMC\rightarrow\Spre$, i.e., for each QMC point we find its nearest preconditioning point, and use the corresponding preconditioner for the linear solve.

    This algorithm is summarised more formally in \cref{alg:par}.
    
    \bre[Is calculating $\Prenearest$ computationally expensive?]
    We note that calculating the map $\Prenearest:\SQMC\rightarrow \Spre$ is an $\cO\mleft(\NQMC\Npre\mright)$ operation, because for each QMC point we must find the nearest preconditioning point. Given that $\Spre \subseteq \SQMC,$ it is possible that calculating $\Prenearest$ could actually be an $\cO\mleft(\NQMC^2\mright)$ operation.

    However, we expect that $\Npre$ will be small relative to $\NQMC$ (and this is borne out in the numerical experiments summarised in \cref{tab:nbpcqmcpar} below) and therefore we expect $\cO\mleft(\NQMC\Npre\mright) \approx \cO\mleft(\NQMC\mright).$ Hence calculating $\Prenearest$ should not be an expensive computational task.

    A similar line of reasoning shows that calculating the nearest QMC point to each of calculated tensor-product points (as outlined above) should also be an $\cO\mleft(\NQMC\mright)$ task.
    \ere

%% Define
%% \beqs
%% \Npreidealj(r) = \max\set{\frac{J \sqrt{\lambdaj}}{2r},1}.
%% \eeqs
%% The `ideal' total number of QMC points is
%% \beqs
%% \Npreideal(r)=\prod_{j=1}^J  \Npreidealj(r)
%% \eeqs

%% Want to calculate the number of preconditioners $\Npre$, the set
%% \beqs
%% \Spre=\set{\ypreo,\ldots,\ypreNpre}
%% \eeqs
%% of QMC points at which to calculate the preconditioner and the map
%% \beqs
%% \nearestpre:\SQMC\rightarrow\Spre
%% \eeqs
%% taking each QMC point to its nearest (in the induced spatial $L^\infty$ norm) preconditioner, where $\SQMC$ is the set of QMC points.

\begin{algorithm}[h]
\DontPrintSemicolon
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetKwFunction{Round}{round}

\Input{$\Npretarget \in \NN$}
\Output{The set $\Spre$, the map $\nearestpre:\SQMC\rightarrow\Spre$}
\BlankLine
Solve (numerically) $\Npreideal(\rideal) = \Npretarget$ for $\rideal$\;
\For{j $= 1$ \KwTo $J$}{
Calculate $\Npreactualj =$ \Round{$\Npreidealj(\rideal)$}\;
Define $\Sprej$ to be set of $\Npreactualj$ equally spaced points in $\mleft[-1/2,1/2\mright]$\;
}
Define $\displaystyle\Npre = \prod_{j=1}^J \Npreactualj$\;
Define $\Spre$ by taking all possible tensor products of points in $\Sprej$, and then finding the nearest QMC point to each one\;
\For{l $=1$ \KwTo $\NQMC$}{
Calculate $\nearestpre\mleft(\by^{(l)}\mright)$\;
}
\caption[The main part of the parallel nearby-preconditioning-Quasi-Monte-Carlo algorithm.]{The main part of the parallel nearby-preconditioning-Quasi-Monte-Carlo algorithm. This part of the algorithm determines $\Spre$ and $\nearestpre$. $\Spre$ is the set of preconditioning points, and $\nearestpre:\SQMC\rightarrow\Spre$ maps each QMC points to its nearest preconditioning point.\label{alg:par}}
\end{algorithm}

\subsubsection{Comparing and Constrasting the two algorithms}

We now briefly list the main differences in the two algorithms given above.

\paragraph{Complexity} The sequential algorithm is simple and intuitive to describe, given that it mainly revolves around `finding the nearest point'. However, the parallel algorithm is much more complicated, both in the underlying ideas, but also in its technical definition.

\paragraph{Heuristics} The sequential algorithm has very minimal heuristics; one only needs to specify the maximum number of GMRES iterations and this could be determined, for example, by the memory constraints of the machine one is using. In contrast, for the parallel algorithm one needs a heuristic for how many preconditioning points to choose, as this is not given by the algorithm. (In our numerical experiments below, we obtain this heuristic by using the sequential algorithm for low $k$, and then extrapolating the proportion of preconditioning points used for low values of $k$ to larger values of $k.$

\paragraph{Parallelisability} Unsurprisingly (given the name) the sequential algorithm is inherently serial; one must see whether a given solve converges in the required number of GMRES iterations before knowing whether we must recalculate the preconditioner for subsequent solves. (In principle one could parallelise the algorithm by splitting the QMC points up onto different groups of processors, and then use the sequential algorithm on each group of processors. However, there is no guarantee one would split the QMC points up in a way that grouped nearby points, therefore this approach could lead to a substantial increase in computational work.) In contrast, the parallel algorithm is fully parallelisable; once the preconditioning points and the map $\nearestpre:\SQMC\rightarrow\Spre$ have been calculated, one can send different linear solves to different groups of processors as one chooses. (Although note that, unless one sends all of the QMC points corresponding to a single preconditioner to the \emph{same} group of processors, one may need to calculate the same preconditioner several times, on different groups of processors\footnote{In our code, we split up the points with respect to the order they are generated by the QMC code. This was purely to make the code simpler.} However, the decrease in computational time gained from parallelisation should more than offset this increase in computational effort.)

\paragraph{Choice of preconditioning points} Neither algorithm will necessarily pick the optimal set of preconditioning points (optimal in the sense of the minimal number of preconditioning points needed). In the sequential algorithm, there is no guarantee that this method for exploring the sample space and choosing the preconditioning points will give an optimal collection of preconditioning points. Also, whilst for the parallel algorithm the preconditioning points should fill the parameter space `well' (given the points are chosen a priori to be well spaced according to the $\dapprox$ metric), the number of preconditioning points generated is not exactly $\Npretarget$ due to rounding the `ideal' number of centres in each dimension to the nearest integer. Therefore, even in the parallel case, one may not end up with an optimal set of preconditioning points.

\subsection{Numerical Experiments}\label{sec:nbpcqmcnumerics}
We now describe numerical experiments that demostrate the effectiveness of the above algorithms.         Our main result is that, for a particular QMC model problem, nearby preconditioning gives a substantial speedup, with around 98\% of solves being computed using a previously-calculated LU decomposition.

For the computational setup, including the algorithm we use to generate our QMC points, see \cref{app:compsetup}.

Before we perform our numerical experiments, we need to determine:
\bit
\item How the number of QMC points should scale with $k$, and
  \item How many preconditioners we should choose.
    \eit
    Throughout this \lcnamecref{sec:nbpcqmcnumerics} we use the model problem detailed in \cref{eq:artificialkl,eq:artificialkllambdas,eq:artificialklfuns} above.

\subsubsection{QMC error estimators}
    
    To determine how the number of QMC points should scale with $k$, we first estimate how the QMC error grows as $k$ increases. The QMC rule we use is a randomly shifted QMC rule, we use such a rule because there exists an error estimator for this rule, see \cref{eq:errest} below. Our exposition below follows that in \cite[Section 4.2]{GrKuNuScSl:11}.

    Suppose our QMC points are $\byo,\ldots,\byNQMC$, and the resulting QMC rule is
    \beqs
\QMC{Q} = \frac1{\NQMC}\sum_{l=1}^{\NQMC} Q\mleft(u\mleft(\byl\mright)\mright).
\eeqs
For a `shift' $\shift \in \cube{J}$ we define the shifted QMC rule
\beqs
\QMCshift{Q}{\shift} = \frac1{\NQMC}\sum_{l=1}^{\NQMC} Q\mleft(u\mleft(\byl\oplus\shift\mright)\mright),
\eeqs
where $\by \oplus \shift$ denotes $\by + \shift$ `wrapped around' onto the hypercube $\cube{J}$. (Formally $\by \oplus \shift = \fracoperator{\mleft(\by + \bhalf\mright)+\shift} - \bhalf,$ where $\fracoperator{\cdot}$ denotes the fractional part and $\bhalf$ denotes the $J$-dimensional vector with every entry $1/2.$)

We can then define the randomly-shifted QMC rule (with multiple randomly-chosen shifts $\shifto,\ldots,\shiftNshifts$)
\beqs
\QMCrandshift{Q}{\Nshifts} = \frac1{\Nshifts}\sum_{s=1}^{\Nshifts} \QMCshift{Q}{\shifts} = \frac1{\NQMC\Nshifts}\sum_{s=1}^{\Nshifts}\sum_{l=1}^{\NQMC} Q\mleft(u\mleft(\byl\oplus \shifts\mright)\mright).
\eeqs

Having defined the randomly shifted QMC rule, one can use the standard statistical estimator of the standard deviation of the statistical error in $\QMCrandshift{Q}{\Nshifts}$ \cite[Equation (4.6)]{GrKuNuScSl:11}
\beq\label{eq:errest}
\QMCerror{\NQMC}{\Nshifts} = \mleft(\frac1{\Nshifts\mleft(\Nshifts-1\mright)}\sum_{s=1}^{\Nshifts} \mleft(\QMCshift{Q}{\shifts} - \QMCrandshift{Q}{\Nshifts}\mright)^2\mright)^{\half}.
\eeq
(See \cref{app:complexerror} for proof that $\QMCerror{\NQMC}{\Nshifts}^2$ is an unbiased estimator of the variance of $\QMCrandshift{Q}{\Nshifts}$; recall that it does \emph{not} then follow that $\QMCerror{\NQMC}{\Nshifts}$ is an \emph{unbiased} estimator of the standard deviation of $\QMCrandshift{Q}{\Nshifts}$.)

\subsubsection{$k$-dependence of the number of QMC points}

We first sought to determine how $\QMCerror{\NQMC}{\Nshifts}$ depends on $k.$ We estimated the error $\QMCerror{\NQMC}{\Nshifts}$ for the setup described in \cref{app:compsetup} with $\NQMC = 2048$ and $\Nshifts=20$ (i.e., 40,960 PDE solves in total) for $k = 10,20,30,40,50,60$. We set $h = 0.002$ for all of the computations (as $0.002 \approx 60^{-3/2}$), as then by \cref{thm:fembound} the finite-element error is of the order $h^2k^3 \sim (k/60)^3 \lesssim 1$ for all the values of $k$ we consider\footnote{Observe that we do not let $h$ depend on $k$, in contrast to the rest of this thesis. This decision means we do not have to consider the effect of changing the mesh on the resulting interpolation of the random field $n$, and how this interpolation may affect the overall error. In addition, since $k \leq 60,$ our particular choice of mesh ensures that the finite-element error is small for all the values of $k$ we consider.}. The quantities of interest (QoIs) we considered were:
\bit
\item The integral of $u$ over the whole domain $\mleft[0,1\mright]^2$,
\item The value of $u$ at the origin,
\item The value of $u$ at the top-right corner of the domain, and
\item The $x$-component of $\grad u$ at the top-right corner of the domain.
  \eit
  Observe that these QoIs require a certain amount of regularity of the solution. (The integral is defined for functions in $\LoD$, point evaluation for functions in $\Hfn{}{3/2 + \eps}{D}$ and point evaluation of the gradient for functions in $\Hfn{}{5/2+\eps}{D}$ (in 3-d - the corresponding function spaces are $\Hfn{}{1+\eps}{D}$ and $\Hfn{}{2+\eps}{D}$ in 2-d) for any $\eps > 0.$) Therefore computing for this range of QoIs will give a good insight into the behaviour of QMC applied to the Helmholtz equation\footnote{We can evaluate point values of $\uh$ because $\uh$ is continuous, and we use the constant value of $\grad \uh$ on the upper-rightmost mesh element as a proxy for $\grad \uh((1,1))$; such a use is possible due to the structure of our mesh, see \cref{fig:grid}, and the fact that we use first-order finite elements.}.
%%     That is, we randomly choose $\shifto,\ldots,\shiftNshifts$ points in $\cube{J}$ (the `shifts')rause the standard error estimator
%%     \beqs
%%     \mleft(\frac1{\nu\mleft(\nu-1\mright)} \sum_{s=1}^{\Nshifts} 
%%     \eeqs$h = 0.002$ (relation to $k=60$ - maximum?)

%In \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha} we plot how $C$ and $\alpha$ depend on $k$, for the plots of the QMC error with increasing $\NQMC$ for each value of $k,$ see \cref{app:hhqmcconv}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{\textwidth}
      \centering
\input{integral-C-plot.pgf}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
                \centering
      \input{integral-alpha-plot.pgf}
    \end{subfigure}
\caption[The computed Quasi-Monte-Carlo convergence rate for $Q(u) =  \int_D u$.]{Plots of the computed values of $C$ (top) and $\alpha$ (bottom) against $k$ in \cref{eq:qmcerrorform} for $Q(u) = \int_D u$. Observe the $x$-axes are on a $\log_{10}$ scale, but $\loge$ is the natural logarithm. \label{fig:integralCalpha}}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{\textwidth}
            \centering
\input{origin-C-plot.pgf}
  \end{subfigure}
    \begin{subfigure}{\textwidth}
                \centering
\input{origin-alpha-plot.pgf}
    \end{subfigure}

\caption[The computed Quasi-Monte-Carlo convergence rate for $Q(u) =  u(\bzero)$.]{The computed values of $C$ (top) and $\alpha$ (bottom) against $k$ in \cref{eq:qmcerrorform} for $Q(u) =  u(\bzero)$. Observe the $x$-axes are on a $\log_{10}$ scale, but $\loge$ is the natural logarithm. \label{fig:originCalpha}}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{\textwidth}
            \centering
\input{top_right-C-plot.pgf}
  \end{subfigure}
    \begin{subfigure}{\textwidth}
            \centering
\input{top_right-alpha-plot.pgf}
    \end{subfigure}
\caption[The computed Quasi-Monte-Carlo convergence rate for $Q(u) =  u(1,1)$.]{The computed values of $C$ (top) and $\alpha$ (bottom) against $k$ in \cref{eq:qmcerrorform} for $Q(u) = u((1,1))$. Observe the $x$-axes are on a $\log_{10}$ scale, but $\loge$ is the natural logarithm.  \label{fig:toprightCalpha}}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{\textwidth}
            \centering
\input{gradient_top_right-C-plot.pgf}
  \end{subfigure}
    \begin{subfigure}{\textwidth}
            \centering 
\input{gradient_top_right-alpha-plot.pgf}
    \end{subfigure}
\caption[The computed Quasi-Monte-Carlo convergence rate for $Q(u) =  \gradu(1,1)$.]{The computed values of $C$ (top) and $\alpha$ (bottom) against $k$ in \cref{eq:qmcerrorform} for $Q(u) = \gradu((1,1))$. Observe the $x$-axes are on a $\log_{10}$ scale, but $\loge$ is the natural logarithm.  \label{fig:gradienttoprightCalpha}}
\end{figure}


Motivated by QMC theory for other applications, e.g., \cite[Equation 4.2]{GrKuNuScSl:11}, we test experimentally the assumption that the QMC error satisfies
\beq\label{eq:qmcerrorform}
\QMCerror{Q}{\Nshifts} = C \NQMC^{-\alpha},
\eeq
for some $C, \alpha > 0.$ Using data for the values of $k$ listed above, \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha} plot the computed values of $C$ and $\alpha$ against $k$. (In \cref{app:hhqmcconv}, we plot the QMC error for increasing $\NQMC$ for each $k \in \set{10,20,30,40,50,60}$ and for each QoI---these plots allow us to determine the values of $C$ and $\alpha$ for each value of $k.$) For the QoIs that are point evaluations (\cref{fig:originCalpha,fig:toprightCalpha}), $C$ appears not to vary very much; thus we assume $C$ is constant in all of the following calculations.

\cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha} (bottom panes) show $\alpha$ decreasing at a rate proportional to $\log k$. Therefore we conjecture
\beq\label{eq:alphaform}
\alpha(k) = \alphaz - \alphao\loge(k),
\eeq
for some constants $\alphaz,\alphao > 0.$ (Throughout this \lcnamecref{sec:nbpcqmcnumerics}, $\loge$ denotes the natural logarithm.) We fitted $\alphaz$ and $\alphao$ numerically, and have plotted the resulting line of best fit on \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha}. (Observe that the conjectured form \cref{eq:alphaform} cannot hold for $k$ very large, as then $\alpha(k)$ would be negative, and there would be no convergence as the number of QMC points is increased. Nevertheless, for the range of $k$ we consider in these numerical experiments, the form \cref{eq:alphaform} seems to give a good fit with the data.) The values of $C$ and $\alpha$ for the different QoIs are given in \cref{fig:integralCalpha,fig:originCalpha,fig:toprightCalpha,fig:gradienttoprightCalpha}.

Having understood how the QMC error increases with $k$ for fixed $\NQMC$, we now use this knowledge to determine how one should increase $\NQMC$ with $k$ in order to keep the QMC error bounded. Recalling that we assume $C$ in \cref{eq:qmcerrorform} is constant, if we take
\beq\label{eq:Nform}
\NQMC(k) = \exp\mleft(\Ctilde \alpha(k)^{-1}\mright),
\eeq
for some constant $\Ctilde > 0$, then substituting \cref{eq:Nform} into \cref{eq:qmcerrorform}, we see that the QMC error should remain bounded, with
\beqs
\QMCerror{Q}{\Nshifts} = C \exp\mleft(-\Ctilde\mright).
\eeqs
Observe that, since $\alpha(k)$ decreases as $k$ increases, \cref{eq:Nform} will increase as $k$ increases.

In our numerical experiments with increasing $\NQMC(k)$ below, we set $\Ctilde$ so that $\NQMC(10) = 2048,$ because in our numerical experiments to determine the behaviour of the QMC error, we used $\NQMC = 2048$ (with 20 shifts). Also in our numerical experiments below we take the number of QMC points to be a power of 2, because the lattice rule we use to generate the points is a complete lattice rule if $\NQMC$ is a power of 2 (see \cite{NuREADME}). We choose $\NQMC$ to be a power of 2 by setting $\NQMC(k) = 2^{M(k)},$ where
\beqs
M(k) = \round{\logtwo\mleft(\exp\mleft(\Ctilde \alpha(k)^{-1}\mright)\mright)}.
\eeqs

Based on the results for the QoIs in \cref{tab:qmcalpha} (excluding the results for the QoI being the integral of $u$ and $\grad u((1,1))$, as these seem to display slightly different convergence characteristics), in our numerical experiments below we take $\alpha(k) = 1.38 - 0.19  \loge(k).$ The resulting values of $\NQMC$ are summarised in \cref{tab:qmcpoints}.

\begin{table}[h!]
  \centering
  \input{qmc-alpha-table}
  \caption{The quantities $\alphaz$ and $\alphao$ for different QoIs, where the QMC error $\approx C \NQMC^{-\mleft(\alphaz - \alphao\loge(k)\mright)}$.}\label{tab:qmcalpha}
  \end{table}


\begin{table}[h]
  \centering
  \input{num-qmc-points-table}
  \caption{The ideal and actual number of QMC points $\NQMC$ used in the numerical experiments summarised in \cref{tab:nbpcqmcseq,tab:nbpcqmcpar}, chosen so that the QMC error is empirically bounded for all $k$.\label{tab:qmcpoints}}
  \end{table}

\subsubsection{Numerical results for nearby preconditioning applied to QMC}

Now that we have an estimate of how the number of QMC points should scale with $k$ in order to keep the QMC error bounded, we apply nearby preconditioning to QMC (with the number of points chosen as in \cref{tab:qmcpoints}) and observe how the computational work of this nearby-preconditioning-QMC (NP-QMC) algorithm scales with $k.$

As outlined above, we combine our sequential- and parallel-NPQMC algorithms:\label{page:seqandpar}
\bit
\item We first use the sequential algorithm for low $k$ (fixing the maximum number of GMRES iterations) and observe how the number of preconditioners (as a proportion of the number of QMC points) changes with $k$. We thus obtain an empirical relationship between $k$ and the proportion of QMC points used to construct preconditioners.
  \item We then use the parallel algorithm (with the above proportion of preconditioners) for higher values of $k.$
    \eit
    We remark that, in principle, one could use the sequential algorithm for all values of $k$, however, this would take an incredibly long time--- we see in \cref{tab:qmcpoints} that for $k=60$ we must perform $2^{17}$ Helmholtz solves; if we performed these solves sequentially, and each solve took 10 seconds, this computation would take over 2 weeks to complete.

    The results for the sequential algorithm are summarised in \cref{tab:nbpcqmcseq}, for $k = 10,\,20,\,30$. The results show that nearby preconditioning is effective, with the number of preconditioners growing (approximately) linearly in $k$, but at a very low percentage of the total number of solves. Also, observe than nearby preconditioning is much more effective than mean-based preconditioning, where we use a single preconditioner, corresponding to the mean of $n$, to precondition all the realisations.

    Performing a linear fit for the percentage of LU-factorisations used in the nearby-preconditioning algorithm, we obtain that the percentage of LU-factorisations grows like $-0.04 + 0.02k$ (see \cref{fig:lu}). This result indicates that although the radius of the balls in which nearby preconditioning is effective decreases with $\cO\mleft(1/k\mright)$, the fact that the number of QMC points increases with $k$ means that a large proportion of the solves are computed using a previously-calculated LU decomposition. Observe that if the number of QMC points remained constant in $k$, we would expect the number of preconditioners to (potentially) increase like $k^J$, because the number of balls of radius $\sim 1/k$ in $\cube{J}$ is $\sim k^J.$

    Based on these sequential results, we then used the parallel algorithm with a target proportion of preconditioners of ($-0.04 + 0.02k$)\%. (Although recall from our discussion above that the actual proportion of preconditioners used can vary due to rounding in the algorithm.) The results of these computations are summarised in \cref{tab:nbpcqmcpar}. We observe that the fraction of preconditioners is approximately $-0.04 + 0.02k$, but the maximum (and average) number of GMRES iterations appears to grow slowly with $k.$ This growth (which did not occur with the sequential algorithm) may be because the placement of the preconditioning points is not optimal with respect to the $\dQMC$ metric; we conjecture that oversampling the number of preconditioners needed (for example, taking a proportion of ($0.05k$)\%) may result in a bounded number of GMRES iterations Nevertheless, we see that nearby preconditioning gives considerable speedup, drastically reducing the number of preconditioners that must be calculated.

    \begin{figure}
      \input{lu-graph.pgf}
      \caption[The number of LU factorisations in the sequential nearby-preconditioning-QMC algorithm as a percentage of the total number of solves.]{The number of LU factorisations in the sequential algorithm as a percentage of the total number of solves.\label{fig:lu}}
      \end{figure}
    \afterpage{% Heard about from https://tex.stackexchange.com/questions/11471/how-to-wrap-text-around-landscape-page
\begin{landscape} % Heard about from https://tex.stackexchange.com/questions/19017/how-to-place-a-table-on-a-new-page-with-landscape-orientation-without-clearing-t/19021#19021 and https://tex.stackexchange.com/questions/25369/how-to-rotate-a-table 
    \begin{table}
  \centering
  \input{nbpc-qmc-sequential-table}
  \caption[Results for the sequential nearby-preconditioning-Quasi-Monte-Carlo algorithm.]{Results applying our sequential nearby-preconditioning-Quasi-Monte-Carlo algorithm with the maximum number of GMRES iterations $=10$, alongside results for mean-based preconditioning.}\label{tab:nbpcqmcseq}
\end{table}

\begin{table}
  \centering
  \input{nbpc-qmc-parallel-table}
  \caption{Results applying our parallel nearby-preconditioning-Quasi-Monte-Carlo algorithm with the target proportion of preconditioners as $(-0.04+0.02k)$\%.}\label{tab:nbpcqmcpar}
\end{table}
\end{landscape}
}

    In conclusion, we see that nearby preconditioning gives a significant speedup when applied to a QMC model problem.%, with around 98\% of solves being computed using a previously-calculated LU decomposition.% We therefore expect that this technique will give significant speed up when applied to other, more realistic problems.
    

    
%% \section{Extension of the results to the truncated exterior Dirichlet problem}\label{sec:TEDP}

%% We now briefly outline how the results in \cref{sec:main} above can be extended to \cref{prob:vtedp}, the Truncated Exterior Dirichlet Problem.

%% %\subsection{Definition of the TEDP and analogues of the results in \cref{sec:3}}

%% %% \paragraph{The impedance boundary $\Gamma_I$.} By comparing \cref{eq:src,eq:ibc}, we see that, in the case $g_I=0$, the TEDP approximates the DtN operator $T_R$ by $\ri k$. Indeed, by using Green's first identity and the definition of the normal derivative (see, e.g., \cite[Lemma 4.3]{Mc:00}), show that the boundary condition on $\Gamma_I$ imposed in the variational problem \cref{prob:vtedp} is 
%% %% %In this BVP, the DtN operator $T_R$ Sommerfeld radiation condition 
%% %% \beq\label{eq:imp}
%% %% \dudnu - \ri k\gamma u = g_I \ton \Gamma_I.
%% %% \eeq
%% %% where $\nu$ is the unit outward-pointing normal vector to $\Omega$ on $\Gamma_I$.

%% \paragraph{Existence and uniqueness of a solution to the TEDP.} The sesquilinear form $\aT(\cdot,\cdot)$ defined in \cref{eq:aT} satisfies the G\aa rding inequality \cref{eq:gardingbrief}, and existence and uniqueness of a solution to the TEDP follow under the same condition on $A$ (piecewise-Lipschitz) as for the EDP, as discussed in \cref{thm:tedp}.%sec:vpGm}; in the case of Lipschitz scalar $A$, these unique-continuation arguments are summarised in \cite[\S2]{GrSa:18}.

%% \paragraph{Finite-element/Galerkin solution.}
%% The Galerkin matrix $\Amat$ is defined exactly as in \cref{eq:matrixAdef}, except that 
%% \beq\label{eq:NTEDP}
%% \big(\Nmat\big)_{ij}\de i k\int_{\Gamma_I}  (\gamma\phi_i) \,\gamma \phi_j.
%% \eeq

%% \paragraph{The adjoint sesquilinear form.} For the TEDP, the adjoint sesquilinear form is given by 
%% \beq\label{eq:TEDPadjoint}
%% a^\dagger(u,v) \de \int_{D} 
%% \Big((A \grad u)\cdot\grad \vb
%%  - k^2 n u\vb\Big) +i k\int_{\Gamma_I} \gamma u\, \overline{\gamma v};
%% \eeq
%% then \cref{eq:A*} holds (with $\Nmat$ now given by \cref{eq:NTEDP}), and the analogue of \cref{lem:adjoint} follows in a straightforward way.


%% \paragraph{The analogues of \cref{cond:1nbpc,cond:2}.}
%% The statement of the TEDP analogues of \cref{cond:1nbpc,cond:2} are the same as for the EDP, apart from the following.
%% \ben
%% \item
%% $\supp \,f$ need not be a subset of $\widetilde{\Omega}$ (i.e.~the support of $f$ can go up to the impedance boundary $\Gamma_I$), and
%% \item the assumption $g_I= 0$ needs to be added to \cref{cond:1nbpc} and Part (i) of \cref{cond:2}.
%% \een
%%  Note that, since $\aT(\cdot,\cdot)$ for the TEDP satisfies the same G\aa rding inequality \cref{eq:gardingbrief} as $a(\cdot,\cdot)$ for the EDP, \cref{lem:H1} holds for the TEDP under the TEDP-analogue of \cref{cond:1nbpc}.

%% \paragraph{The main results \cref{thm:1,cor:1}.}
%% Since \cref{cond:1nbpc,cond:2} are essentially unchanged from the EDP case, \cref{lem:keylemma1,lem:keylemma2} hold for the TEDP, and thus so do \cref{thm:1,cor:1,cor:1a}.

%% \paragraph{The PDE results \cref{thm:2} and \cref{lem:sharp}.}

%% The PDE bound \cref{thm:2} relies only on \cref{lem:H1}, which, as stated above, also holds for the TEDP. Therefore \cref{thm:2} holds for the TEDP under the TEDP-analogue of \cref{cond:1nbpc} described above. The construction in \cref{lem:sharp} to show sharpness of the bound in \cref{thm:1} (at least when $\Aso= \Ast= I$) also holds for the TEDP; this is because one can choose the supports of $\chi$ and $\widetilde{\chi}$ to be contained inside $\widetilde{\Omega}$, and then $u^{(1)}$ and $u^{(2)}$ defined in \cref{lem:sharp} satisfy the impedance boundary condition \cref{eq:imp} on $\Gamma_I$.

%% %% \paragraph{When the TEDP-analogue of \cref{cond:1nbpc} holds.}

%% %% In \cref{sec:cond1hold} we discussed 4 situations (Cases 1-4) where \cref{cond:1nbpc} is proved to hold for the EDP. We now discuss the TEDP-analogues of these.
%% %% %Cases 1, 3, and 4 (there is no proof yet for the TEDP-analogue of Case 2).

%% %% \emph{Cases 1 and 2: $\Aso$, $\nso$, and $\Gamma_I$  are $C^\infty$.} 
%% %% With the rays defined as in the EDP case (by the Melrose--Sj{\"o}strand generalized bicharacteristic flow 
%% %% \cite[\S24.3]{Ho:85}), the TEDP-analogue of nontrapping for the EDP is the assumption that 
%% %% every ray eventually hits the boundary at a \emph{non-diffractive point} (defined in \cite[Page 1037]{BaLeRa:92}). Note that, in the case $\Dm=\emptyset$ $\Aso= I$, and $\nso=1$, every ray eventually hits the boundary at a non-diffractive point by \cite[Lemma 5.3]{BaSpWu:16}.
%% %% Under the additional assumption that $\nso= 1$, \cref{cond:1nbpc} follows from the results of \cite{BaLeRa:92} by combining \cite[Theorem 1.8]{BaSpWu:16} and \cite[Remark 5.6]{BaSpWu:16}, but $C^{(1)}_{\rm bound}$ is not given explicitly.

%% %% \emph{Case 3: $\Dm$ is starshaped with respect to the origin, $\Aso$ and $\nso$ are Lipschitz and satisfy radial monotonicity-like conditions.}
%% %% When $\Gamma_I$ is also starshaped with respect to the origin and $A$ and $n$ satisfy \cref{eq:A1nbpc} and \cref{eq:n1nbpc} respectively (with $\Dp$ replaced by $\Omega$), 
%% %% \cite[Theorem A.6(i)]{GrPeSp:19} proves that
%% %% \cref{cond:1nbpc} holds, with an explicit expression for $C^{(1)}_{\rm bound}$. Analogous results when (a) $2\Aso - (\bx\cdot\nabla)\Aso \geq \mu_1$ and $\nso= 1$,
%% %% and  (b) $\Aso= I$ and  $2\nso + \bx \cdot \nabla \nso \geq \mu_2$, 
%% %% are contained in \cite[Theorem A.6(ii)]{GrPeSp:19} and \cite[Theorem A.6(iii)]{GrPeSp:19} respectively.
%% %% When $A$ is scalar, these results were also proved in \cite[Theorem 1]{BrGaPe:17} and, when $\Aso= I$ and $\Dm=\emptyset$, also in \cite[Theorem 3.2]{GrSa:18}.

%% %% \emph{Case 4: %\item[Case 4:]
%% %%  $\Aso$ and $\nso$ are allowed to be discontinuous.}
%% %% %\een
%% %% \cref{cond:1nbpc} is proved in \cite{CaVo:10} (without an explicit expression for $C^{(1)}_{\rm bound}$) when $\Dm$ is $C^\infty$ and nontrapping, $\Gamma_I$ is $C^\infty$, $\Aso= I $, and $\nso$ is a piecewise-constant, monotonically non-decreasing function, jumping on interfaces that are $C^\infty$ with strictly positive curvature.
%% %% Recall from \cref{cond:1nbpc} that \cite[Theorem 2.7]{GrPeSp:19} proves that \cref{cond:1nbpc} holds for the EDP (with an explicit expression for $C^{(1)}_{\rm bound}$) when $\Dm$ is starshaped with respect to the origin, $A$ and $n$ are $L^\infty$, with $A$ monotonically \emph{non-increasing} in the radial direction, and $n$ monotonically \emph{non-decreasing}. This proof can be extended to the TEDP, with the additional assumption that $\Gamma_I$ is star-shaped with respect to the origin; see the discussion in \cite[Section A.2]{GrPeSp:19}.

%% %\cref{cond:1nbpc} is proved, with an explicit expression for $C^{(1)}_{\rm bound}$, when 

%% %\newpage
%% %
%% %\section*{Questions for Th\'eo}
%% %
%% %\ben
%% %\item At the place marked A on the scanned pages, you seem to use the inequality 
%% %\beq\label{eq:Theo1}
%% %\vert\vert\vert \xi - \cP_h \xi\vert\vert\vert \lesssim h^\alpha \N{u_\phi- \cP_h u_\phi}_{0,\Omega}.
%% %\eeq
%% %\een
%% %
%% %\newpag

%% %% \section*{Owen to do list}
%% %% \ben
%% %% \item Varying  $\|\Aso-\Ast\|_{L^\infty}$ and $\|\nso-\nst\|_{L^\infty}$ in standard GMRES.
%% %% \item Computations where $\|\Aso-\Ast\|_{L^\infty}$ and $\|\nso-\nst\|_{L^\infty}$ are sometimes large; is having the standard deviations of these $\sim 1/k$ good enough for $k$-independent GMRES iterations?
%% %% \item ***on backburner*** Checking under what conditions (if any) Part (ii) \cref{cond:2} holds by running the following experiment:
%% %% %\item Exciting experiments for random $n$ that you told us about last week.
%% %% %\item In the weighted norm, the condition on $A$ is ``$k \|\Aso-\Ast\|_{L^\infty}$ sufficiently small" but in the Euclidean norm the best we have so far is ``$h^{-1} \|\Aso-\Ast\|_{L^\infty}$ sufficiently small". You indicated before that experiments seemed to indicate that ``$k \|\Aso-\Ast\|_{L^\infty}$ sufficiently small" seemed correct for the Euclidean norm too. The next time we meet, can you show me these results please?
%% %% %\item Please run the following numerical experiment.
%% %% \bit
%% %% \item TEDP with $\Omega$ a square/rectangle.
%% %% \item $\Aso$ being at least Lipschitz (but smooth is fine). To keep things simple, just take scalar- (as opposed to matrix-) valued $\Aso$ and don't worry about making it nontrapping.
%% %% \item Smoothness of $\nso$ doesn't really matter, just take smooth in the first instance for simplicity (and also don't worry about nontrapping).
%% %% \item $\Vhp$ piecewise linear.
%% %% \item Linear system $\Amato \uvec = \Smat_{A} \balpha$ for some arbitrary complex-valued vector $\balpha$ and some arbitrary $A\in L^\infty$. (I claim this corresponds to the problem described in Part (ii) of \cref{cond:2},  but please check this!)
%% %% \item For each $\Aso, \nso, \balpha$, solve linear system for increasing values of $k$, first with $h\sim k^{-2}$, and then with $h\sim k^{-3/2}$.
%% %% \item Goal: see if the bound \cref{eq:bound4} holds, using 
%% %% \beqs
%% %% \N{\sum_j \alpha_j (A\nabla \phi_j)}_{\LtD} \quad \text{ as a proxy for } \quad \N{\LE}_{(\HokD)'}.
%% %% \eeqs
%% %% \eit
%% %% %\item Varying  $\|\Aso-\Ast\|_{L^\infty}$ and $\|\nso-\nst\|_{L^\infty}$ in \emph{weighted} GMRES.
%% %% \een

\section{Review of related techniques in the literature}\label{sec:nbpclitreview}
   
Having proved rigorous results on the effectiveness of nearby preconditioning, and also applied it to a UQ algorithm, we now review similar computational techniques (applied to other problems) which can be found in the literature. Whilst the idea of \emph{nearby} preconditioning introduced here is, as far as we are aware, novel, there has been a body of work on the closely-related idea of \emph{mean-based} preconditioning. In mean-based preconditioning a \emph{single} preconditioner is calculated corresponding to the mean of the random coefficient. This is in contrast to nearby preconditioning, where \emph{multiple} preconditioners are calculated, corresponding to each realisation in a particular subset of all the realisations. Mean-based preconditioning has been most extensively studied for the stationary diffusion equation
    \beqs
\grad \cdot \mleft(\kappa\grad u\mright)  = -f,
\eeqs
with a small number of works analysing other PDEs, including two works on the Helmholtz equation. We will first explain the idea of mean-based preconditioning before we review the literature applying it to the stationary diffusion equation and other PDEs, and finally turning our attention to mean-based preconditioning for the Helmholtz equation. In general, the computational and mathematical results in the literature show that mean-based preconditioning  is effective if the variance of the random parameters is small enough, i.e.,  if most of the samples are sufficiently close to the mean.

Mean-based preconditioning was first developed for the stationary diffusion equation in the context of so-called Stochastic Spectral Finite-Element Methods (SSFEMs). In these methods, the random field $a$ is given by a series expansion, such as a Karhunen--Lo\`eve expansion, and the dependence of $u$ on the random parameters is computed using a Polynomial Chaos expansion (see, e.g., \cite[Section 2.4.2]{GhSp:12}. The resulting problem is then discretised in the whole space $D \times \Omega$, where $D$ is the spatial domain and $\Omega$ the probability space. The resulting discrete problems involve very large matrices of the form
\beq\label{eq:sgmatrix}
\Amat \otimes \Gmat,
\eeq
where $\Amat$ is a standard finite-element matrix, $\Gmat$ is a matrix corresponding to the discretisation in $\Omega,$ and $\otimes$ is the Kronecker product. For SSFEMs (and the closely-related stochastic-Galerkin FEMs, see, e.g. \cite{BaTeZo:04}, which also have discretisations of the form \cref{eq:sgmatrix}) a mean-based preconditioner is a matrix of the form
\beq\label{eq:mbsg}
\Amatmean \otimes \ImatOmega,
\eeq
where $\Amatmean$ is the standard finite-element matrix corresponding to the mean of $\kappa$ and $\ImatOmega$ is the identity matrix associated with the discretisation on $\Omega.$ Using a mean-based preconditioner of the form \cref{eq:mbsg} gives considerable computational savings, as only one preconditioner of a standard finite-element matrix needs to be calculated.

When stochastic Galerkin methods are used with so-called `doubly-orthogonal bases' (see, e.g., \cite[Section 3.2]{ErPoSiUl:09}), then the linear system \cref{eq:sgmatrix} decouples into many distinct standard finite-element matrices; mean-based preconditioning has also been investigated in this context (and in the context of stochastic collocation methods (see, e.g., \cite{BaNoTe:07}), where one similarly obtains many different standard finite-element matrices) as will be discussed below.

The main insight gleaned from studies of mean-based preconditioning is that, as stated above, if the variance of $\kappa$ (or any other stochastic coefficients) is sufficiently small, then mean-based preconditioning is effective.

The initial computational work on mean-based preconditioning for the stationary diffusion equation was carried out by Ghanem and Kruger \cite{GhKr:96}, Pellissetti and Ghanem \cite{PeGh:00}, and Keese \cite{Ke:04}, with theory (proving bounds on the eigenvalues of the preconditioned matrices) following from Powell and Elman \cite{PoEl:09} and Ernst, Powell, Silvester, and Ullmann \cite{ErPoSiUl:09}. These eigenvalue bounds are analagous to results in \cref{sec:main} above, as they allow one to infer convergence properties of the iterative method used. All of the above results were for $\kappa$ given by a (real or artificial) Karhunen--Lo\`eve expansion; that is, in the case where $\kappa$ depends linearly on the random parameter. In the case where $\kappa$ is a lognormal random field (and so the dependence is no longer linear), Powell and Ullmann \cite{PoUl:10} declared mean-based preconditioners to be ineffective, and so developed more advanced preconditioners; in contrast, Ullmann, Elman and Ernst \cite{UlElEr:12} transformed a stationary diffusion problem with lognormal coefficient into a stationary convection-diffusion problem with a random coefficient depending linearly on the noise, before proving eigenvalue bounds as before. With a more computational slant, Tipireddy, Phipps, and Ghanem \cite{TiPhGh:10} and Rosseel and Vandewalle \cite{RoVa:10} compared the computational properties of several mean-based preconditioners and Elman, Miller, Phipps, and Tuminaro \cite{ElMiPhTu:11} compared the computational cost of mean-based preconditioners for stochastic Galerkin and stochastic collocation methods.

Seeking to apply mean-based preconditioning to more challenging problems, Powell and Silvester \cite{PoSi:12} performed computational investigations for mean-based preconditioners applied to stochastic Galerkin discretisations of the steady-state Navier--Stokes equations, and Soused\'ik and Elman \cite{SoEl:16} introduced a Gauss--Seidel-type preconditioner, using mean-based ideas, for the steady-state Navier--Stokes equations. Finally, Khan, Powell, and Silvester \cite{KhPoSi:19} applied mean-based preconditioning to stochastic Galerkin discretisations of the equations for nearly-incompressible elasticity.

The works applying mean-based preconditioning to many individual systems (for the stationary diffusion equation) are those of Eiermann, Ernst and Ullmann \cite{EiErUl:07}; Ernst, Powell, Silvester, and Ullmann \cite{ErPoSiUl:09}; and Gordon and Powell \cite{GoPo:12}. \cite{EiErUl:07} contained computational results in a (decoupled) stochastic Galerkin setting; \cite{ErPoSiUl:09} proved eigenvalue bounds in the same setting, and \cite{GoPo:12} proved rigorous eigenvalue bounds in a stochastic collocation setting. All these works assume linear dependence on the noise, and show that mean-based preconditioning works well when the variance is sufficiently small.

We now turn our attention to mean-based preconditioning for the Helmholtz equation. The first work we discuss is the recent work of Wang and Liao \cite{WaLi:19}. They discretise a stochastic Helmholtz problem with $k=10$ and $n$ given by a truncated Karhunen--Lo\`eve expansion (with either 4 terms or 1 term) and use a generalised polynomial chaos (gPC) expansion (see, e.g., \cite{XiKa:02}) for the solution $u$. Whilst they use mean-based preconditioning (in the `Kronecker product' sense) they are more interested in investigating the effect of the number of terms in the gPC expansion on the accuracy of the discrete solution. Nonetheless, they see convergence using the mean-based preconditioner, although more iterations are needed when the random field is `close to' exciting a resonant frequency (see \cite[Example 4.2]{WaLi:19}).

The work most similar to ours is the work of Jin and Cai \cite{JiCa:09}, who use a stochastic Galerkin discretisation with a doubly-orthogonal basis for a stochastic Helmholtz equation, resulting in around 5000 linear systems. They take $k = 225$ and a Karhunen--Lo\`eve expansion with 4 terms for both (scalar-valued) $A$ and $n$. The random variables in the Karhunen--Lo\`eve expansions are $\Unif(-\sqrt{3},\sqrt{3})$ and $\Unif(-45\sqrt{3},45\sqrt{3})$ for $A$ and $n$ respectively. Their mean-based preconditioner is a 1-level additive Schwarz preconditioner, and they compare resuing the preconditioner with reusing the Krylov subspaces (an idea first introduced by Parks, De Sturler, Mackey, Johnson, and Maiti in \cite{PadeMaJoMa:06}), as well as combining both techniques. Intriguingly, they see no additional benefit from reusing the preconditioner, but considerable benefit from recycling the Krylov subspaces. Based on our results in this \lcnamecref{chap:nbpc}, we conjecture that they see no benefit from a single mean-based preconditioner because $k$ is reasonably large, and therefore for most of the realisations, $k\NLiDRR{\EXP{n}-\nsj}$ and $k\NLiDRRdtd{\EXP{A}-\Asj}$ are not sufficiently small, and so there is little-to-no effect on the number of GMRES iterations from mean-based preconditioning. We conjecture that if they had used multiple preconditioners distributed around the stochastic parameter space, they would have seen computational improvements, as described in this \lcnamecref{chap:nbpc}.
    
\section{Probabilistic nearby preconditioning results}\label{sec:nbpcstochastic}

We now briefly overview how one can prove probabilistic results on the effectiveness of nearby preconditioning. All of the results in \cref{sec:intronbpc,sec:num,sec:3,sec:weaknorm} above have been for deterministic (as opposed to stochastic) coefficients $A$ and $n$ (and we then applied these deterministic results to QMC methods for the Helmholtz equation in \cref{sec:nbpcqmc}). Therefore we now turn our attention to obtaining probabilistic results on the effectiveness of nearby preconditioning for stochastic Helmholtz problems, i.e., \cref{prob:msedp,prob:somsedp,prob:svsedp} from \cref{chap:stochastic}. Firstly, in \cref{cor:stonbpcas} below, we prove an `essentially deterministic' result on the effectiveness of nearby preconditioning, before proving probabilistic results on the effectiveness of nearby preconditioning applied to stochastic problems. However, we will see that our efforts to prove probabilistic results are restricted by the applicability of the Elman estimate (\cref{thm:GMRES1_intro} above).

Throughout this \lcnamecref{sec:nbpcstochastic} we consider \cref{prob:msedp} from \cref{chap:stochastic} but with $A=I$, i.e., for simplicity we only consider the case of random $n$, although everything we say could be easily extended to include random $A$. To maintain consistent notation with the rest of this \lcnamecref{chap:nbpc} we will use a superscript ${}^{(2)}$ to refer to the stochastic problem (e.g., the random coefficient will be $\nst(\omega)$, the solution will be $\ust(\omega)$, the matrices arising from the finite-element discretiation will be $\Amatt(\omega),$ etc.). We let $\nso \in \LiDRR$ define a \emph{deterministic} Helmholtz problem. We will use the discretisation of this deterministic Helmholtz problem to precondition the discretisations of the realisations of the stochastic Helmholtz problem. I.e., we will consider the performance of GMRES applied to
\beq\label{eq:stopc}
\AmatoI\Amatt(\omega)\uvec = \AmatoI \fvec.
\eeq
For simplicity, in all that follows we will measure $\no-\nt$ in the $L^{\infty}$ norm, although one could use any of the weaker norms discussed in \cref{sec:weaknorm} above, and obtain analogous results.

\subsection{Probabilistic theory for nearby preconditioning}
\bde[Number of GMRES iterations required for convergence]\label{def:numGMRESitsconv}

\

\noindent Let $\GMRES{\eps}{\nso}{\nst}$ denote the number of iterations required for GMRES in the unweighted norm $\Nt{\cdot}$ with $\Nt{\rvecz} = 1,$ applied to
\beqs
\AmatoI\Amatt  \uvec = \AmatoI \fvec
\eeqs
to converge to within a tolerance $\eps,$ i.e., to achieve
\beqs
\frac{\Nt{\rvecm}}{\Nt{\fvec}} < \eps.
\eeqs
\ede

Note that $\GMRES{\eps}{\nso}{\nst}$ is a random variable, see \cref{lem:randomvariable} below.

If we apply \cref{cor:1a} to the problem \cref{eq:stopc} we can straightforwardly conclude the following \lcnamecref{cor:stonbpcas}.

\bco[Almost-sure nearby preconditioning]\label{cor:stonbpcas}
Let $0 < \eps < 1,$ $\nst:\Omega \rightarrow \LiDRR$ satisfy the assumptions at the start of \cref{sec:hh-results}, $\no, \,\Dm,$ and $f$ be as in \cref{prob:vgen}, and let the assumptions of \cref{cor:1a} hold. Then $\GMRES{\eps}{\nso}{\nst}$ is bounded independently of $k$ almost surely if
\beq\label{eq:nbpcas}
\NLiDRR{\nso-\nst(\omega)} \leq \frac1{2\Ct k}
\eeq
almost surely.
\eco

%Observe that implicit in \cref{cor:stonbpcas} is the fact that $\GMRES{\eps}{\nso}{\nst}$ is a random variable; we sketch a proof of this fact now.

\ble[$\GMRES{\eps}{\no}{\nt}$ is a random variable]\label{lem:randomvariable}
Under the assumptions of \cref{cor:stonbpcas}, $\GMRES{\eps}{\nso}{\nst}$ is a random variable, i.e., $\GMRES{\eps}{\nso}{\nst}:\Omega\rightarrow \RR$ is measurable.
\ele

\bpf[Sketch Proof of \cref{lem:randomvariable}]
All of the operations used in constructing the vectors $\xvecm$ in the GMRES algorithm are measurable functions of $\xvecmmo$ and $\AmatoI\Amatt$ (see, e.g., \cite[Algorithms 11.4.2 and 5.1.3]{GoVa:13}), therefore $\mleft(\rvecm\mright)_{m=1}^N$ is a sequence of random variables, i.e., a stochastic process (see, e.g., \cite[Definition 2.1.4]{Ok:13}). The stopping criterion $\Nt{\rvecm}/\Nt{\fvec} < \eps$ is an exit time for the stochastic process $\xvecm$ from the set $\CCN \setminus \ball{\CCN}{\xvecs}{\eps\Nt{\fvec}},$ where $\xvecs$ is the true solution. Therefore, because we assume $\OFP$ is a complete probability space, it follows from, e.g.,  \cite[Example 7.2.2]{Ok:13} that $\GMRES{\eps}{\no}{\nt}$ is a stopping time (see \cite[Definition 7.2.1]{Ok:13}). Because $\GMRES{\eps}{\no}{\nt}$ is a stopping time, it is measurable with respect to the associated filtration (see, e.g., \cite[Definition 3.2.2]{Ok:13}), and so is measurable with respect to $\cF$; i.e., $\GMRES{\eps}{\no}{\nt}$ is a random variable.
\epf

The numerical results in \cref{sec:num} above can be seen (in part) as confirming \cref{cor:stonbpcas}. Recall that in \cref{sec:num} we let $\nso-\nst$ be a piecewise-constant random field, and we fixed $\alpha = \NLiDRR{\nso-\nst}$ or $\NLiDRRdtd{\Aso-\Ast}$ almost surely. When we fixed $\alpha = 0.5/k$ almost surely (see \cref{fig:linfinityA2,fig:linfinityn2}) we saw that the number of GMRES iterations was bounded independently of $k.$ This behaviour is precisely that given in \cref{cor:stonbpcas}.

\bre[Drawbacks of \cref{cor:stonbpcas}]\label{rem:notideal}
There are two drawbacks of \cref{cor:stonbpcas}:
\ben
\item\label[itemdrawback]{it:notideal1} The condition \cref{eq:nbpcas} must hold almost surely, and
  \item\label[itemdrawback]{it:notideal2} \Cref{cor:stonbpcas} does not give any explicit information on how the distribution of the number of GMRES iterations depends on the distribution of $\NLiDRR{\nso-\nst}.$
    \een
    \Cref{it:notideal1} is not ideal because in many physically realistic problems $\NLiDRR{\no-\nt(\omega)}$ may be unbounded (e.g., if $\nt$ is a lognormal random field) or even if bounded may not satisfy the condition \cref{eq:nbpcas} almost surely.% \Cref{it:notideal2} is not ideal because it means one cannot infer information about the distribution of the number of GMRES iterations from the distribution of$\NLiDRR{\nso-\nst(\omega)}.$
    \ere

    To correct the deficiencies described in \cref{rem:notideal} one would aim to  prove a bound on the number of GMRES iterations depending explicitly on $\NLiDRR{\nso-\nst(\omega)}$, and then use this bound to prove a probabilistic estimate for the number of GMRES iterations. Such a bound is given in \cref{lem:probgmres1} in \cref{app:probnbpc}. However, such a bound will be highly pessimistic, and will impart little useful information. The reason for this lack of information is that the Elman estimate (\cref{cor:GMRES_intro} above) when applied to the nearby-preconditioned system $\AmatoI\Amatt$) only applies when $k\NLiDop{\Aso-\Ast}$ and $k\NLiDRR{\nso-\nst}$ are sufficiently small (as we saw in \cref{cor:1} above). Therefore one can only obtain detailed information on how the number of GMRES iterations depends on $\NLiDop{\Aso-\Ast}$ and $\NLiDRR{\nso-\nst}$ when these quantities are small (informally, when they are $\lesssim 1/k$). In all other cases (again, informally, when these quantities are $\gtrsim 1/k$) the only statement one can make about the convergence of GMRES is that there will be at most $N$ iterations, where $N$ is the number of degrees of freedom (this result is recalled in \cref{cor:gmresguaranteed} below). In summary, current results on GMRES convergence will only allow us to prove what are likely to be very pessimistic bounds on how the number of GMRES iterations for $\AmatoI\Amatt$ depends on $\NLiDop{\Aso-\Ast}$ and $\NLiDRR{\nso-\nst}$. For completeness, we record these results in \cref{app:probnbpc}.






%%  TO HERE BRO

%%     We first define notation for the number of GMRES iterations required for convergence. Because \cref{lem:probgmres1} below is a \emph{deterministic} result, i.e., it does not require $\nst$ to be a random field. Therefore for this \lcnamecref{lem:probgmres1} only, we assume $\nst$ is as given at the beginning of this \lcnamecref{chap:nbpc}.


    


%% \bre[\Cref{thm:probgmres} is pessimistic]\label{rem:pessimistic}
%% Observe that we expect the bound in \cref{thm:probgmres} to be pessimistic, i.e., we expect that \cref{eq:GMRESprob} is not sharp in its dependence on $\alpha$. In particular, we expect \cref{eq:GMRESprob} is not sharp for large values of $R.$ We now show that in most cases, for large values of $R$ the left-hand side of \cref{eq:GMRESprob} is independent of $R$.

%% One can show via elementary calculus that for $\alpha < 1$ $\Gfnname$ achieves its maximum when $\alpha = 1/3$ (assuming that for $\alpha < 1$ the expression involving $\alpha$ in \cref{eq:gdef} is always  at most $N$). Also observe that $\Gfnname$ over the range $\alpha  \in (0,1)$ only depends on $k$ through the dependence of $\alpha$ on $k.$ Therefore, the maximum of $\Gfnname$ over $\alpha \in (0,1)$ is independent of $k.$ Let $\Gfnmaxlo$ denote the value of this maximum. Then, for any $R \in \mleft(\Gfnmaxlo,N\mright)$, the estimate $\PP\mleft(\Gfn{\nso-\nst} \leq R\mright)$ is equal to $\PP\mleft(\alpha<1\mright) = \PP\mleft(\NLiDRR{\nso-\nst} < 1/\mleft(\Ct k\mright)\mright),$ i.e., the lower-bound in \cref{eq:GMRESprob} is \emph{independent} of $R$, for $R \in \mleft(\Gfnmaxlo,N\mright)$  This is almost certainly not sharp - we would expect $\PP\mleft(\GMRES{\eps}{\nso}{\nst} \leq R\mright)$ to increase with $R$. However, because the only rigorous result we have available if $\alpha \geq 1$ is \cref{cor:gmresguaranteed}, we cannot prove a better bound.
%% \ere

\subsection{Numerical probabalistic results for nearby preconditioning}\label{sec:qualgmres}

Notwithstanding the fact that we are limited in the probabilistic results that we can \emph{prove} about nearby preconditioning, we will now see that we \emph{observe} reasonable probabilistic behaviour when we perform numerical experiments. We again recall that (informally) \cref{cor:stonbpcas} states that we obtain almost-surely bounded GMRES iterations if $\NLiDRR{\nso-\nst} \lesssim 1/k.$ A plausible probabalistic analogue of this result would be that we have bounded \emph{average} number of GMRES iterations if the standard deviation of $\NLiDRR{\nso-\nst}$ is of the order $1/k$. We expect this result because the standard deviation of a random variable is a (probabilistic) measure of its variation. In \cref{cor:stonbpcas} we show that the number of GMRES iterations is bounded almost surely if the variation in $\NLiDRR{\nso-\nst}$ is bounded (of the order $1/k$) almost surely. Therefore, it reasonable to assume that the probabilistic analogue of the number of GMRES iterations (the average) is bounded if the probabilistic analogue of the variation in $\NLiDRR{\nso-\nst}$ (the standard deviation) is bounded (of the order $1/k$). We will see exactly this behaviour in our numerical experiments.

In our numerical experiments we use the computational setup described in \cref{app:compsetup}, with $f=1$ and $\gI=0,$ $\Aso=\Ast=I$, $\nso=1,$ and $\NLiDRR{\nso-\nst}$ given by an exponential random variables with standard deviation $\sigma.$  We consider three cases:
\ben
\item\label[itemcase]{it:sigma1} $\displaystyle \sigma  = 1,$
\item\label[itemcase]{it:sigma2} $\displaystyle \sigma  = \frac{1}k,$ and
  \item\label[itemcase]{it:sigma3} $\displaystyle \sigma  = \frac{1}{k^2}$.
    \een

    For each of these cases we calculate
    \beq\label{eq:gmresprob}
    \PP\mleft(\GMRES{\eps}{\no}{\nt} \leq 12\mright).
    \eeq

    Based on the reasoning above we expect that in \cref{it:sigma2} the probability \cref{eq:gmresprob} \emph{is constant} as $k$ increases, and using similar reasoning, we expect that in \cref{it:sigma1} the probability \cref{eq:gmresprob} \emph{decreases} as $k$ increases and in \cref{it:sigma3} the probability \cref{eq:gmresprob} \emph{increases} as $k$ increases.   This is approximately the behaviour we observe in \cref{fig:prob-plot-0.0,fig:prob-plot-1.0,fig:prob-plot-2.0}. This behaviour demonstrates that whilst the theory developed in the rest of this \lcnamecref{chap:nbpc} does not allow us to easily prove useful results about the probabalistic behaviour of nearby preconditioning, the theory does give us \emph{intuition} as to what the probabilistic behavour will be.

%% \begin{figure}[p]
%%   \centering
%%   \begin{subfigure}{\textwidth}
%%     \centering
%% \input{prob-gmres-theory-0.0.pgf}
%% \caption{The lower bound in \cref{eq:GMRESprob} with $\NLiDR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1.$\label{fig:prob-theory-plot-0.0}}
%% \end{subfigure}

%% \begin{subfigure}{\textwidth}
%%     \centering
%% \input{prob-gmres-theory-1.0.pgf}
%% \caption{The lower bound in \cref{eq:GMRESprob} with $\NLiDR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1/k.$\label{fig:prob-theory-plot-1.0}}
%% \end{subfigure}

%% \begin{subfigure}{\textwidth}
%%     \centering
%%     \input{prob-gmres-theory-2.0.pgf}
%%     \caption{The lower bound in \cref{eq:GMRESprob} with $\NLiDR{\no-\nt} \sim \Exp{\sigma}$ with $\sigma = 1/k^2.$\label{fig:prob-theory-plot-2.0}}
%% \end{subfigure}
%% \caption{The lower bound in \cref{eq:GMRESprob} for $R=12$, $\eps = 10^{-5}$, $N = \ceil{k^{3}}$, and $\Ct=0.1,$ for different functional forms of $\NLiDR{\no-\nt}$.}
%% \end{figure}
%% \ednote{Euan---the y axis on \cref{fig:prob-theory-plot-1.0} isn't correct. But basically, the values are all around 0.29, to within $10^{-11}$.}

\begin{figure}[p]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
\input{prob-plot-rate-0.0.pgf}
\caption{The empirical probability that $\GMRES{\eps}{\no}{\nt}\leq 12$ for $\sigma = 1.$\label{fig:prob-plot-0.0}}
\end{subfigure}

\begin{subfigure}{\textwidth}
    \centering
\input{prob-plot-rate-1.0.pgf}
\caption{The empirical probability that $\GMRES{\eps}{\no}{\nt}\leq 12$ for $\sigma = 1/k$\label{fig:prob-plot-1.0}}
\end{subfigure}

\begin{subfigure}{\textwidth}
    \centering
\input{prob-plot-rate-2.0.pgf}
\caption{The empirical probability that $\GMRES{\eps}{\no}{\nt}\leq 12$ for $\sigma = 1/k^2.$\label{fig:prob-plot-2.0}}
\end{subfigure}
\caption[The empirical probability that GMRES applied to a nearby-preconditioned linear system converges in at most 12 iterations.]{The empirical probability (calculated from 1000 realisations) that $\GMRES{\eps}{\no}{\nt}\leq 12$ for $k = 10, 20, 30, 40,$ where $R=12$, $\eps = 10^{-5}$, $\nso=1,$ and $\NLiDR{\no-\nt} \sim \Exp{\sigma}$ for different functional forms of $\sigma.$}
\end{figure}
